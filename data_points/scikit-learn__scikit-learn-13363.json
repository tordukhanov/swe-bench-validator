{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-13363",
  "base_commit": "eda99f3cec70ba90303de0ef3ab7f988657fadb9",
  "patch": "diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py\n--- a/sklearn/linear_model/ridge.py\n+++ b/sklearn/linear_model/ridge.py\n@@ -368,12 +368,25 @@ def _ridge_regression(X, y, alpha, sample_weight=None, solver='auto',\n                       return_n_iter=False, return_intercept=False,\n                       X_scale=None, X_offset=None):\n \n-    if return_intercept and sparse.issparse(X) and solver != 'sag':\n-        if solver != 'auto':\n-            warnings.warn(\"In Ridge, only 'sag' solver can currently fit the \"\n-                          \"intercept when X is sparse. Solver has been \"\n-                          \"automatically changed into 'sag'.\")\n-        solver = 'sag'\n+    has_sw = sample_weight is not None\n+\n+    if solver == 'auto':\n+        if return_intercept:\n+            # only sag supports fitting intercept directly\n+            solver = \"sag\"\n+        elif not sparse.issparse(X):\n+            solver = \"cholesky\"\n+        else:\n+            solver = \"sparse_cg\"\n+\n+    if solver not in ('sparse_cg', 'cholesky', 'svd', 'lsqr', 'sag', 'saga'):\n+        raise ValueError(\"Known solvers are 'sparse_cg', 'cholesky', 'svd'\"\n+                         \" 'lsqr', 'sag' or 'saga'. Got %s.\" % solver)\n+\n+    if return_intercept and solver != 'sag':\n+        raise ValueError(\"In Ridge, only 'sag' solver can directly fit the \"\n+                         \"intercept. Please change solver to 'sag' or set \"\n+                         \"return_intercept=False.\")\n \n     _dtype = [np.float64, np.float32]\n \n@@ -404,14 +417,7 @@ def _ridge_regression(X, y, alpha, sample_weight=None, solver='auto',\n         raise ValueError(\"Number of samples in X and y does not correspond:\"\n                          \" %d != %d\" % (n_samples, n_samples_))\n \n-    has_sw = sample_weight is not None\n \n-    if solver == 'auto':\n-        # cholesky if it's a dense array and cg in any other case\n-        if not sparse.issparse(X) or has_sw:\n-            solver = 'cholesky'\n-        else:\n-            solver = 'sparse_cg'\n \n     if has_sw:\n         if np.atleast_1d(sample_weight).ndim > 1:\n@@ -432,8 +438,6 @@ def _ridge_regression(X, y, alpha, sample_weight=None, solver='auto',\n     if alpha.size == 1 and n_targets > 1:\n         alpha = np.repeat(alpha, n_targets)\n \n-    if solver not in ('sparse_cg', 'cholesky', 'svd', 'lsqr', 'sag', 'saga'):\n-        raise ValueError('Solver %s not understood' % solver)\n \n     n_iter = None\n     if solver == 'sparse_cg':\n@@ -555,7 +559,7 @@ def fit(self, X, y, sample_weight=None):\n             # add the offset which was subtracted by _preprocess_data\n             self.intercept_ += y_offset\n         else:\n-            if sparse.issparse(X):\n+            if sparse.issparse(X) and self.solver == 'sparse_cg':\n                 # required to fit intercept with sparse_cg solver\n                 params = {'X_offset': X_offset, 'X_scale': X_scale}\n             else:\n",
  "test_patch": "diff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py\n--- a/sklearn/linear_model/tests/test_ridge.py\n+++ b/sklearn/linear_model/tests/test_ridge.py\n@@ -7,6 +7,7 @@\n \n from sklearn.utils.testing import assert_almost_equal\n from sklearn.utils.testing import assert_array_almost_equal\n+from sklearn.utils.testing import assert_allclose\n from sklearn.utils.testing import assert_equal\n from sklearn.utils.testing import assert_array_equal\n from sklearn.utils.testing import assert_greater\n@@ -778,7 +779,8 @@ def test_raises_value_error_if_solver_not_supported():\n     wrong_solver = \"This is not a solver (MagritteSolveCV QuantumBitcoin)\"\n \n     exception = ValueError\n-    message = \"Solver %s not understood\" % wrong_solver\n+    message = (\"Known solvers are 'sparse_cg', 'cholesky', 'svd'\"\n+               \" 'lsqr', 'sag' or 'saga'. Got %s.\" % wrong_solver)\n \n     def func():\n         X = np.eye(3)\n@@ -832,9 +834,57 @@ def test_ridge_fit_intercept_sparse():\n     # test the solver switch and the corresponding warning\n     for solver in ['saga', 'lsqr']:\n         sparse = Ridge(alpha=1., tol=1.e-15, solver=solver, fit_intercept=True)\n-        assert_warns(UserWarning, sparse.fit, X_csr, y)\n-        assert_almost_equal(dense.intercept_, sparse.intercept_)\n-        assert_array_almost_equal(dense.coef_, sparse.coef_)\n+        assert_raises_regex(ValueError, \"In Ridge,\", sparse.fit, X_csr, y)\n+\n+\n+@pytest.mark.parametrize('return_intercept', [False, True])\n+@pytest.mark.parametrize('sample_weight', [None, np.ones(1000)])\n+@pytest.mark.parametrize('arr_type', [np.array, sp.csr_matrix])\n+@pytest.mark.parametrize('solver', ['auto', 'sparse_cg', 'cholesky', 'lsqr',\n+                                    'sag', 'saga'])\n+def test_ridge_regression_check_arguments_validity(return_intercept,\n+                                                   sample_weight, arr_type,\n+                                                   solver):\n+    \"\"\"check if all combinations of arguments give valid estimations\"\"\"\n+\n+    # test excludes 'svd' solver because it raises exception for sparse inputs\n+\n+    rng = check_random_state(42)\n+    X = rng.rand(1000, 3)\n+    true_coefs = [1, 2, 0.1]\n+    y = np.dot(X, true_coefs)\n+    true_intercept = 0.\n+    if return_intercept:\n+        true_intercept = 10000.\n+    y += true_intercept\n+    X_testing = arr_type(X)\n+\n+    alpha, atol, tol = 1e-3, 1e-4, 1e-6\n+\n+    if solver not in ['sag', 'auto'] and return_intercept:\n+        assert_raises_regex(ValueError,\n+                            \"In Ridge, only 'sag' solver\",\n+                            ridge_regression, X_testing, y,\n+                            alpha=alpha,\n+                            solver=solver,\n+                            sample_weight=sample_weight,\n+                            return_intercept=return_intercept,\n+                            tol=tol)\n+        return\n+\n+    out = ridge_regression(X_testing, y, alpha=alpha,\n+                           solver=solver,\n+                           sample_weight=sample_weight,\n+                           return_intercept=return_intercept,\n+                           tol=tol,\n+                           )\n+\n+    if return_intercept:\n+        coef, intercept = out\n+        assert_allclose(coef, true_coefs, rtol=0, atol=atol)\n+        assert_allclose(intercept, true_intercept, rtol=0, atol=atol)\n+    else:\n+        assert_allclose(out, true_coefs, rtol=0, atol=atol)\n \n \n def test_errors_and_values_helper():\n",
  "problem_statement": "return_intercept==True in ridge_regression raises an exception\n<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn\r\n- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn\r\nFor more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->\r\n\r\n#### Steps/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.linear_model import ridge_regression\r\nridge_regression([[0], [1], [3]], [0, 1, 3], 1, solver='auto', return_intercept=True)\r\n```\r\n\r\n#### Expected Results\r\n<!-- Example: No error is thrown. Please paste or describe the expected results.-->\r\n\r\n`(array([1]), 0)` (the values can differ, but at least no exception should be raised)\r\n\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback. -->\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nUnboundLocalError                         Traceback (most recent call last)\r\n<ipython-input-5-84df44249e86> in <module>\r\n----> 1 ridge_regression([[0], [1], [3]], [1, 2, 3], 1, solver='auto', return_intercept=True)\r\n\r\n~/.pyenv/versions/3.7.2/envs/kaggle-3.7.2/lib/python3.7/site-packages/sklearn/linear_model/ridge.py in ridge_regression(X, y, alpha, sample_weight, solver, max_iter, tol, verbose, random_state, return_n_iter, return_intercept)\r\n    450         return coef, n_iter, intercept\r\n    451     elif return_intercept:\r\n--> 452         return coef, intercept\r\n    453     elif return_n_iter:\r\n    454         return coef, n_iter\r\n\r\nUnboundLocalError: local variable 'intercept' referenced before assignment\r\n```\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nFor scikit-learn >= 0.20:\r\nimport sklearn; sklearn.show_versions()\r\nFor scikit-learn < 0.20:\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport numpy; print(\"NumPy\", numpy.__version__)\r\nimport scipy; print(\"SciPy\", scipy.__version__)\r\nimport sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\n-->\r\n\r\n```\r\nLinux-4.20.8-arch1-1-ARCH-x86_64-with-arch\r\nPython 3.7.2 (default, Feb 22 2019, 18:13:04) \r\n[GCC 8.2.1 20181127]\r\nNumPy 1.16.1\r\nSciPy 1.2.1\r\nScikit-Learn 0.21.dev0\r\n```\r\n\r\n\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
  "hints_text": "",
  "created_at": "2019-03-01T16:25:10Z",
  "version": "0.21",
  "FAIL_TO_PASS": "[\"sklearn/linear_model/tests/test_ridge.py::test_raises_value_error_if_solver_not_supported\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_fit_intercept_sparse\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[auto-array-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[auto-array-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sparse_cg-array-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sparse_cg-array-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sparse_cg-csr_matrix-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sparse_cg-csr_matrix-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[cholesky-array-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[cholesky-array-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[cholesky-csr_matrix-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[cholesky-csr_matrix-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[lsqr-array-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[lsqr-array-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[lsqr-csr_matrix-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[lsqr-csr_matrix-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[saga-array-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[saga-array-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[saga-csr_matrix-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[saga-csr_matrix-sample_weight1-True]\"]",
  "PASS_TO_PASS": "[\"sklearn/linear_model/tests/test_ridge.py::test_ridge[svd]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge[sparse_cg]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge[cholesky]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge[lsqr]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge[sag]\", \"sklearn/linear_model/tests/test_ridge.py::test_primal_dual_relationship\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_singular\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_sample_weights\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_convergence_fail\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_sample_weights\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_shapes\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_intercept\", \"sklearn/linear_model/tests/test_ridge.py::test_toy_ridge_object\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_vs_lstsq\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_individual_penalties\", \"sklearn/linear_model/tests/test_ridge.py::test_dense_sparse[_test_ridge_loo]\", \"sklearn/linear_model/tests/test_ridge.py::test_dense_sparse[_test_ridge_cv]\", \"sklearn/linear_model/tests/test_ridge.py::test_dense_sparse[_test_ridge_cv_normalize]\", \"sklearn/linear_model/tests/test_ridge.py::test_dense_sparse[_test_ridge_diabetes]\", \"sklearn/linear_model/tests/test_ridge.py::test_dense_sparse[_test_multi_ridge_diabetes]\", \"sklearn/linear_model/tests/test_ridge.py::test_dense_sparse[_test_ridge_classifiers]\", \"sklearn/linear_model/tests/test_ridge.py::test_dense_sparse[_test_tolerance]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_cv_sparse_svd\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_sparse_svd\", \"sklearn/linear_model/tests/test_ridge.py::test_class_weights\", \"sklearn/linear_model/tests/test_ridge.py::test_class_weight_vs_sample_weight[RidgeClassifier]\", \"sklearn/linear_model/tests/test_ridge.py::test_class_weight_vs_sample_weight[RidgeClassifierCV]\", \"sklearn/linear_model/tests/test_ridge.py::test_class_weights_cv\", \"sklearn/linear_model/tests/test_ridge.py::test_ridgecv_store_cv_values\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_classifier_cv_store_cv_values\", \"sklearn/linear_model/tests/test_ridge.py::test_ridgecv_sample_weight\", \"sklearn/linear_model/tests/test_ridge.py::test_raises_value_error_if_sample_weights_greater_than_1d\", \"sklearn/linear_model/tests/test_ridge.py::test_sparse_design_with_sample_weights\", \"sklearn/linear_model/tests/test_ridge.py::test_ridgecv_int_alphas\", \"sklearn/linear_model/tests/test_ridge.py::test_ridgecv_negative_alphas\", \"sklearn/linear_model/tests/test_ridge.py::test_sparse_cg_max_iter\", \"sklearn/linear_model/tests/test_ridge.py::test_n_iter\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[auto-array-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[auto-array-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[auto-csr_matrix-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[auto-csr_matrix-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[auto-csr_matrix-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[auto-csr_matrix-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sparse_cg-array-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sparse_cg-array-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sparse_cg-csr_matrix-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sparse_cg-csr_matrix-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[cholesky-array-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[cholesky-array-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[cholesky-csr_matrix-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[cholesky-csr_matrix-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[lsqr-array-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[lsqr-array-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[lsqr-csr_matrix-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[lsqr-csr_matrix-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sag-array-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sag-array-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sag-array-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sag-array-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sag-csr_matrix-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sag-csr_matrix-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sag-csr_matrix-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sag-csr_matrix-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[saga-array-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[saga-array-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[saga-csr_matrix-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[saga-csr_matrix-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_errors_and_values_helper\", \"sklearn/linear_model/tests/test_ridge.py::test_errors_and_values_svd_helper\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_classifier_no_support_multilabel\", \"sklearn/linear_model/tests/test_ridge.py::test_dtype_match\", \"sklearn/linear_model/tests/test_ridge.py::test_dtype_match_cholesky\"]",
  "environment_setup_commit": "7813f7efb5b2012412888b69e73d76f2df2b50b6",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.993061",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}