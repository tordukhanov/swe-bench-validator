{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-12784",
  "base_commit": "55bf5d93e5674f13a1134d93a11fd0cd11aabcd1",
  "patch": "diff --git a/build_tools/generate_authors_table.py b/build_tools/generate_authors_table.py\n--- a/build_tools/generate_authors_table.py\n+++ b/build_tools/generate_authors_table.py\n@@ -97,7 +97,7 @@ def key(profile):\n contributors = get_contributors()\n \n print(\".. raw :: html\\n\")\n-print(\"    <!-- Generated by gen_authors.py -->\")\n+print(\"    <!-- Generated by generate_authors_table.py -->\")\n print(\"    <table>\")\n print(\"    <col style='width:%d%%' span='%d'>\"\n       % (int(100 / ROW_SIZE), ROW_SIZE))\ndiff --git a/examples/applications/plot_stock_market.py b/examples/applications/plot_stock_market.py\n--- a/examples/applications/plot_stock_market.py\n+++ b/examples/applications/plot_stock_market.py\n@@ -65,7 +65,6 @@\n # License: BSD 3 clause\n \n import sys\n-from datetime import datetime\n \n import numpy as np\n import matplotlib.pyplot as plt\n@@ -85,8 +84,6 @@\n # that we get high-tech firms, and before the 2008 crash). This kind of\n # historical data can be obtained for from APIs like the quandl.com and\n # alphavantage.co ones.\n-start_date = datetime(2003, 1, 1).date()\n-end_date = datetime(2008, 1, 1).date()\n \n symbol_dict = {\n     'TOT': 'Total',\ndiff --git a/examples/compose/plot_transformed_target.py b/examples/compose/plot_transformed_target.py\n--- a/examples/compose/plot_transformed_target.py\n+++ b/examples/compose/plot_transformed_target.py\n@@ -20,7 +20,9 @@\n from __future__ import print_function, division\n \n import numpy as np\n+import matplotlib\n import matplotlib.pyplot as plt\n+from distutils.version import LooseVersion\n \n print(__doc__)\n \n@@ -34,6 +36,13 @@\n from sklearn.compose import TransformedTargetRegressor\n from sklearn.metrics import median_absolute_error, r2_score\n \n+\n+# `normed` is being deprecated in favor of `density` in histograms\n+if LooseVersion(matplotlib.__version__) >= '2.1':\n+    density_param = {'density': True}\n+else:\n+    density_param = {'normed': True}\n+\n ###############################################################################\n # A synthetic random regression problem is generated. The targets ``y`` are\n # modified by: (i) translating all targets such that all entries are\n@@ -54,13 +63,13 @@\n \n f, (ax0, ax1) = plt.subplots(1, 2)\n \n-ax0.hist(y, bins=100, normed=True)\n+ax0.hist(y, bins=100, **density_param)\n ax0.set_xlim([0, 2000])\n ax0.set_ylabel('Probability')\n ax0.set_xlabel('Target')\n ax0.set_title('Target distribution')\n \n-ax1.hist(y_trans, bins=100, normed=True)\n+ax1.hist(y_trans, bins=100, **density_param)\n ax1.set_ylabel('Probability')\n ax1.set_xlabel('Target')\n ax1.set_title('Transformed target distribution')\n@@ -139,12 +148,12 @@\n \n f, (ax0, ax1) = plt.subplots(1, 2)\n \n-ax0.hist(y, bins=100, normed=True)\n+ax0.hist(y, bins=100, **density_param)\n ax0.set_ylabel('Probability')\n ax0.set_xlabel('Target')\n ax0.set_title('Target distribution')\n \n-ax1.hist(y_trans, bins=100, normed=True)\n+ax1.hist(y_trans, bins=100, **density_param)\n ax1.set_ylabel('Probability')\n ax1.set_xlabel('Target')\n ax1.set_title('Transformed target distribution')\ndiff --git a/examples/mixture/plot_gmm_covariances.py b/examples/mixture/plot_gmm_covariances.py\n--- a/examples/mixture/plot_gmm_covariances.py\n+++ b/examples/mixture/plot_gmm_covariances.py\n@@ -64,6 +64,7 @@ def make_ellipses(gmm, ax):\n         ell.set_clip_box(ax.bbox)\n         ell.set_alpha(0.5)\n         ax.add_artist(ell)\n+        ax.set_aspect('equal', 'datalim')\n \n iris = datasets.load_iris()\n \ndiff --git a/examples/neighbors/plot_kde_1d.py b/examples/neighbors/plot_kde_1d.py\n--- a/examples/neighbors/plot_kde_1d.py\n+++ b/examples/neighbors/plot_kde_1d.py\n@@ -29,10 +29,17 @@\n # Author: Jake Vanderplas <jakevdp@cs.washington.edu>\n #\n import numpy as np\n+import matplotlib\n import matplotlib.pyplot as plt\n+from distutils.version import LooseVersion\n from scipy.stats import norm\n from sklearn.neighbors import KernelDensity\n \n+# `normed` is being deprecated in favor of `density` in histograms\n+if LooseVersion(matplotlib.__version__) >= '2.1':\n+    density_param = {'density': True}\n+else:\n+    density_param = {'normed': True}\n \n #----------------------------------------------------------------------\n # Plot the progression of histograms to kernels\n@@ -47,11 +54,11 @@\n fig.subplots_adjust(hspace=0.05, wspace=0.05)\n \n # histogram 1\n-ax[0, 0].hist(X[:, 0], bins=bins, fc='#AAAAFF', normed=True)\n+ax[0, 0].hist(X[:, 0], bins=bins, fc='#AAAAFF', **density_param)\n ax[0, 0].text(-3.5, 0.31, \"Histogram\")\n \n # histogram 2\n-ax[0, 1].hist(X[:, 0], bins=bins + 0.75, fc='#AAAAFF', normed=True)\n+ax[0, 1].hist(X[:, 0], bins=bins + 0.75, fc='#AAAAFF', **density_param)\n ax[0, 1].text(-3.5, 0.31, \"Histogram, bins shifted\")\n \n # tophat KDE\ndiff --git a/examples/plot_anomaly_comparison.py b/examples/plot_anomaly_comparison.py\n--- a/examples/plot_anomaly_comparison.py\n+++ b/examples/plot_anomaly_comparison.py\n@@ -14,32 +14,34 @@\n except for Local Outlier Factor (LOF) as it has no predict method to be applied\n on new data when it is used for outlier detection.\n \n-The :class:`svm.OneClassSVM` is known to be sensitive to outliers and thus does\n-not perform very well for outlier detection. This estimator is best suited for\n-novelty detection when the training set is not contaminated by outliers.\n-That said, outlier detection in high-dimension, or without any assumptions on\n-the distribution of the inlying data is very challenging, and a One-class SVM\n-might give useful results in these situations depending on the value of its\n-hyperparameters.\n-\n-:class:`covariance.EllipticEnvelope` assumes the data is Gaussian and learns\n-an ellipse. It thus degrades when the data is not unimodal. Notice however\n-that this estimator is robust to outliers.\n-\n-:class:`ensemble.IsolationForest` and :class:`neighbors.LocalOutlierFactor`\n-seem to perform reasonably well for multi-modal data sets. The advantage of\n-:class:`neighbors.LocalOutlierFactor` over the other estimators is shown for\n-the third data set, where the two modes have different densities. This\n-advantage is explained by the local aspect of LOF, meaning that it only\n+The :class:`sklearn.svm.OneClassSVM` is known to be sensitive to outliers and\n+thus does not perform very well for outlier detection. This estimator is best\n+suited for novelty detection when the training set is not contaminated by\n+outliers. That said, outlier detection in high-dimension, or without any\n+assumptions on the distribution of the inlying data is very challenging, and a\n+One-class SVM might give useful results in these situations depending on the\n+value of its hyperparameters.\n+\n+:class:`sklearn.covariance.EllipticEnvelope` assumes the data is Gaussian and\n+learns an ellipse. It thus degrades when the data is not unimodal. Notice\n+however that this estimator is robust to outliers.\n+\n+:class:`sklearn.ensemble.IsolationForest` and\n+:class:`sklearn.neighbors.LocalOutlierFactor` seem to perform reasonably well\n+for multi-modal data sets. The advantage of\n+:class:`sklearn.neighbors.LocalOutlierFactor` over the other estimators is\n+shown for the third data set, where the two modes have different densities.\n+This advantage is explained by the local aspect of LOF, meaning that it only\n compares the score of abnormality of one sample with the scores of its\n neighbors.\n \n Finally, for the last data set, it is hard to say that one sample is more\n abnormal than another sample as they are uniformly distributed in a\n-hypercube. Except for the :class:`svm.OneClassSVM` which overfits a little, all\n-estimators present decent solutions for this situation. In such a case, it\n-would be wise to look more closely at the scores of abnormality of the samples\n-as a good estimator should assign similar scores to all the samples.\n+hypercube. Except for the :class:`sklearn.svm.OneClassSVM` which overfits a\n+little, all estimators present decent solutions for this situation. In such a\n+case, it would be wise to look more closely at the scores of abnormality of\n+the samples as a good estimator should assign similar scores to all the\n+samples.\n \n While these examples give some intuition about the algorithms, this\n intuition might not apply to very high dimensional data.\ndiff --git a/examples/plot_johnson_lindenstrauss_bound.py b/examples/plot_johnson_lindenstrauss_bound.py\n--- a/examples/plot_johnson_lindenstrauss_bound.py\n+++ b/examples/plot_johnson_lindenstrauss_bound.py\n@@ -94,13 +94,21 @@\n import sys\n from time import time\n import numpy as np\n+import matplotlib\n import matplotlib.pyplot as plt\n+from distutils.version import LooseVersion\n from sklearn.random_projection import johnson_lindenstrauss_min_dim\n from sklearn.random_projection import SparseRandomProjection\n from sklearn.datasets import fetch_20newsgroups_vectorized\n from sklearn.datasets import load_digits\n from sklearn.metrics.pairwise import euclidean_distances\n \n+# `normed` is being deprecated in favor of `density` in histograms\n+if LooseVersion(matplotlib.__version__) >= '2.1':\n+    density_param = {'density': True}\n+else:\n+    density_param = {'normed': True}\n+\n # Part 1: plot the theoretical dependency between n_components_min and\n # n_samples\n \n@@ -187,7 +195,7 @@\n           % (np.mean(rates), np.std(rates)))\n \n     plt.figure()\n-    plt.hist(rates, bins=50, normed=True, range=(0., 2.), edgecolor='k')\n+    plt.hist(rates, bins=50, range=(0., 2.), edgecolor='k', **density_param)\n     plt.xlabel(\"Squared distances rate: projected / original\")\n     plt.ylabel(\"Distribution of samples pairs\")\n     plt.title(\"Histogram of pairwise distance rates for n_components=%d\" %\ndiff --git a/examples/text/plot_document_classification_20newsgroups.py b/examples/text/plot_document_classification_20newsgroups.py\n--- a/examples/text/plot_document_classification_20newsgroups.py\n+++ b/examples/text/plot_document_classification_20newsgroups.py\n@@ -145,7 +145,7 @@ def size_mb(docs):\n     len(data_train.data), data_train_size_mb))\n print(\"%d documents - %0.3fMB (test set)\" % (\n     len(data_test.data), data_test_size_mb))\n-print(\"%d categories\" % len(categories))\n+print(\"%d categories\" % len(target_names))\n print()\n \n # split a training set and a test set\ndiff --git a/sklearn/compose/_column_transformer.py b/sklearn/compose/_column_transformer.py\n--- a/sklearn/compose/_column_transformer.py\n+++ b/sklearn/compose/_column_transformer.py\n@@ -692,7 +692,7 @@ def _validate_transformers(transformers):\n         return True\n \n     for t in transformers:\n-        if t in ('drop', 'passthrough'):\n+        if isinstance(t, six.string_types) and t in ('drop', 'passthrough'):\n             continue\n         if (not (hasattr(t, \"fit\") or hasattr(t, \"fit_transform\")) or not\n                 hasattr(t, \"transform\")):\ndiff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py\n--- a/sklearn/feature_extraction/text.py\n+++ b/sklearn/feature_extraction/text.py\n@@ -1356,8 +1356,10 @@ class TfidfVectorizer(CountVectorizer):\n         preprocessing and n-grams generation steps.\n         Only applies if ``analyzer == 'word'``.\n \n-    analyzer : string, {'word', 'char'} or callable\n+    analyzer : string, {'word', 'char', 'char_wb'} or callable\n         Whether the feature should be made of word or character n-grams.\n+        Option 'char_wb' creates character n-grams only from text inside\n+        word boundaries; n-grams at the edges of words are padded with space.\n \n         If a callable is passed it is used to extract the sequence of features\n         out of the raw, unprocessed input.\ndiff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py\n--- a/sklearn/linear_model/logistic.py\n+++ b/sklearn/linear_model/logistic.py\n@@ -1142,6 +1142,9 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,\n     Attributes\n     ----------\n \n+    classes_ : array, shape (n_classes, )\n+        A list of class labels known to the classifier.\n+\n     coef_ : array, shape (1, n_features) or (n_classes, n_features)\n         Coefficient of the features in the decision function.\n \n@@ -1594,6 +1597,9 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,\n \n     Attributes\n     ----------\n+    classes_ : array, shape (n_classes, )\n+        A list of class labels known to the classifier.\n+\n     coef_ : array, shape (1, n_features) or (n_classes, n_features)\n         Coefficient of the features in the decision function.\n \ndiff --git a/sklearn/metrics/pairwise.py b/sklearn/metrics/pairwise.py\n--- a/sklearn/metrics/pairwise.py\n+++ b/sklearn/metrics/pairwise.py\n@@ -1136,6 +1136,24 @@ def _check_chunk_size(reduced, chunk_size):\n                           chunk_size))\n \n \n+def _precompute_metric_params(X, Y, metric=None, **kwds):\n+    \"\"\"Precompute data-derived metric parameters if not provided\n+    \"\"\"\n+    if metric == \"seuclidean\" and 'V' not in kwds:\n+        if X is Y:\n+            V = np.var(X, axis=0, ddof=1)\n+        else:\n+            V = np.var(np.vstack([X, Y]), axis=0, ddof=1)\n+        return {'V': V}\n+    if metric == \"mahalanobis\" and 'VI' not in kwds:\n+        if X is Y:\n+            VI = np.linalg.inv(np.cov(X.T)).T\n+        else:\n+            VI = np.linalg.inv(np.cov(np.vstack([X, Y]).T)).T\n+        return {'VI': VI}\n+    return {}\n+\n+\n def pairwise_distances_chunked(X, Y=None, reduce_func=None,\n                                metric='euclidean', n_jobs=None,\n                                working_memory=None, **kwds):\n@@ -1271,6 +1289,10 @@ def pairwise_distances_chunked(X, Y=None, reduce_func=None,\n                                         working_memory=working_memory)\n         slices = gen_batches(n_samples_X, chunk_n_rows)\n \n+    # precompute data-derived metric params\n+    params = _precompute_metric_params(X, Y, metric=metric, **kwds)\n+    kwds.update(**params)\n+\n     for sl in slices:\n         if sl.start == 0 and sl.stop == n_samples_X:\n             X_chunk = X  # enable optimised paths for X is Y\n@@ -1398,6 +1420,10 @@ def pairwise_distances(X, Y=None, metric=\"euclidean\", n_jobs=None, **kwds):\n         dtype = bool if metric in PAIRWISE_BOOLEAN_FUNCTIONS else None\n         X, Y = check_pairwise_arrays(X, Y, dtype=dtype)\n \n+        # precompute data-derived metric params\n+        params = _precompute_metric_params(X, Y, metric=metric, **kwds)\n+        kwds.update(**params)\n+\n         if effective_n_jobs(n_jobs) == 1 and X is Y:\n             return distance.squareform(distance.pdist(X, metric=metric,\n                                                       **kwds))\ndiff --git a/sklearn/metrics/regression.py b/sklearn/metrics/regression.py\n--- a/sklearn/metrics/regression.py\n+++ b/sklearn/metrics/regression.py\n@@ -514,19 +514,19 @@ def r2_score(y_true, y_pred, sample_weight=None,\n     0.948...\n     >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n     >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n-    >>> r2_score(y_true, y_pred, multioutput='variance_weighted')\n-    ... # doctest: +ELLIPSIS\n+    >>> r2_score(y_true, y_pred,\n+    ...          multioutput='variance_weighted') # doctest: +ELLIPSIS\n     0.938...\n-    >>> y_true = [1,2,3]\n-    >>> y_pred = [1,2,3]\n+    >>> y_true = [1, 2, 3]\n+    >>> y_pred = [1, 2, 3]\n     >>> r2_score(y_true, y_pred)\n     1.0\n-    >>> y_true = [1,2,3]\n-    >>> y_pred = [2,2,2]\n+    >>> y_true = [1, 2, 3]\n+    >>> y_pred = [2, 2, 2]\n     >>> r2_score(y_true, y_pred)\n     0.0\n-    >>> y_true = [1,2,3]\n-    >>> y_pred = [3,2,1]\n+    >>> y_true = [1, 2, 3]\n+    >>> y_pred = [3, 2, 1]\n     >>> r2_score(y_true, y_pred)\n     -3.0\n     \"\"\"\ndiff --git a/sklearn/neural_network/multilayer_perceptron.py b/sklearn/neural_network/multilayer_perceptron.py\n--- a/sklearn/neural_network/multilayer_perceptron.py\n+++ b/sklearn/neural_network/multilayer_perceptron.py\n@@ -619,7 +619,7 @@ def fit(self, X, y):\n \n     @property\n     def partial_fit(self):\n-        \"\"\"Fit the model to data matrix X and target y.\n+        \"\"\"Update the model with a single iteration over the given data.\n \n         Parameters\n         ----------\n@@ -978,7 +978,7 @@ def fit(self, X, y):\n \n     @property\n     def partial_fit(self):\n-        \"\"\"Fit the model to data matrix X and target y.\n+        \"\"\"Update the model with a single iteration over the given data.\n \n         Parameters\n         ----------\ndiff --git a/sklearn/preprocessing/data.py b/sklearn/preprocessing/data.py\n--- a/sklearn/preprocessing/data.py\n+++ b/sklearn/preprocessing/data.py\n@@ -1988,7 +1988,7 @@ class QuantileTransformer(BaseEstimator, TransformerMixin):\n     (marginal) outliers: this is therefore a robust preprocessing scheme.\n \n     The transformation is applied on each feature independently.\n-    The cumulative density function of a feature is used to project the\n+    The cumulative distribution function of a feature is used to project the\n     original values. Features values of new/unseen data that fall below\n     or above the fitted range will be mapped to the bounds of the output\n     distribution. Note that this transform is non-linear. It may distort linear\n@@ -2001,7 +2001,7 @@ class QuantileTransformer(BaseEstimator, TransformerMixin):\n     ----------\n     n_quantiles : int, optional (default=1000)\n         Number of quantiles to be computed. It corresponds to the number\n-        of landmarks used to discretize the cumulative density function.\n+        of landmarks used to discretize the cumulative distribution function.\n \n     output_distribution : str, optional (default='uniform')\n         Marginal distribution for the transformed data. The choices are\n@@ -2378,7 +2378,7 @@ def quantile_transform(X, axis=0, n_quantiles=1000,\n     (marginal) outliers: this is therefore a robust preprocessing scheme.\n \n     The transformation is applied on each feature independently.\n-    The cumulative density function of a feature is used to project the\n+    The cumulative distribution function of a feature is used to project the\n     original values. Features values of new/unseen data that fall below\n     or above the fitted range will be mapped to the bounds of the output\n     distribution. Note that this transform is non-linear. It may distort linear\n@@ -2398,7 +2398,7 @@ def quantile_transform(X, axis=0, n_quantiles=1000,\n \n     n_quantiles : int, optional (default=1000)\n         Number of quantiles to be computed. It corresponds to the number\n-        of landmarks used to discretize the cumulative density function.\n+        of landmarks used to discretize the cumulative distribution function.\n \n     output_distribution : str, optional (default='uniform')\n         Marginal distribution for the transformed data. The choices are\ndiff --git a/sklearn/svm/base.py b/sklearn/svm/base.py\n--- a/sklearn/svm/base.py\n+++ b/sklearn/svm/base.py\n@@ -384,7 +384,7 @@ def _compute_kernel(self, X):\n         return X\n \n     def _decision_function(self, X):\n-        \"\"\"Distance of the samples X to the separating hyperplane.\n+        \"\"\"Evaluates the decision function for the samples in X.\n \n         Parameters\n         ----------\n@@ -529,7 +529,7 @@ def _validate_targets(self, y):\n         return np.asarray(y, dtype=np.float64, order='C')\n \n     def decision_function(self, X):\n-        \"\"\"Distance of the samples X to the separating hyperplane.\n+        \"\"\"Evaluates the decision function for the samples in X.\n \n         Parameters\n         ----------\n@@ -541,7 +541,16 @@ def decision_function(self, X):\n             Returns the decision function of the sample for each class\n             in the model.\n             If decision_function_shape='ovr', the shape is (n_samples,\n-            n_classes)\n+            n_classes).\n+\n+        Notes\n+        ------\n+        If decision_function_shape='ovo', the function values are proportional\n+        to the distance of the samples X to the separating hyperplane. If the\n+        exact distances are required, divide the function values by the norm of\n+        the weight vector (``coef_``). See also `this question\n+        <https://stats.stackexchange.com/questions/14876/\n+        interpreting-distance-from-hyperplane-in-svm>`_ for further details.\n         \"\"\"\n         dec = self._decision_function(X)\n         if self.decision_function_shape == 'ovr' and len(self.classes_) > 2:\ndiff --git a/sklearn/utils/testing.py b/sklearn/utils/testing.py\n--- a/sklearn/utils/testing.py\n+++ b/sklearn/utils/testing.py\n@@ -42,12 +42,21 @@\n except NameError:\n     WindowsError = None\n \n+from numpy.testing import assert_allclose\n+from numpy.testing import assert_almost_equal\n+from numpy.testing import assert_approx_equal\n+from numpy.testing import assert_array_equal\n+from numpy.testing import assert_array_almost_equal\n+from numpy.testing import assert_array_less\n+import numpy as np\n+\n import sklearn\n-from sklearn.base import BaseEstimator\n+from sklearn.base import (BaseEstimator, ClassifierMixin, ClusterMixin,\n+                          RegressorMixin, TransformerMixin)\n+from sklearn.utils import deprecated, IS_PYPY, _IS_32BIT\n from sklearn.utils._joblib import joblib\n+from sklearn.utils._unittest_backport import TestCase\n from sklearn.utils.fixes import signature\n-from sklearn.utils import deprecated, IS_PYPY, _IS_32BIT\n-\n \n additional_names_in_all = []\n try:\n@@ -73,24 +82,13 @@\n except ImportError:\n     pass\n \n-from numpy.testing import assert_almost_equal\n-from numpy.testing import assert_array_equal\n-from numpy.testing import assert_array_almost_equal\n-from numpy.testing import assert_array_less\n-from numpy.testing import assert_approx_equal\n-import numpy as np\n-\n-from sklearn.base import (ClassifierMixin, RegressorMixin, TransformerMixin,\n-                          ClusterMixin)\n-from sklearn.utils._unittest_backport import TestCase\n-\n __all__ = [\"assert_equal\", \"assert_not_equal\", \"assert_raises\",\n            \"assert_raises_regexp\", \"assert_true\",\n            \"assert_false\", \"assert_almost_equal\", \"assert_array_equal\",\n            \"assert_array_almost_equal\", \"assert_array_less\",\n            \"assert_less\", \"assert_less_equal\",\n            \"assert_greater\", \"assert_greater_equal\",\n-           \"assert_approx_equal\", \"SkipTest\"]\n+           \"assert_approx_equal\", \"assert_allclose\", \"SkipTest\"]\n __all__.extend(additional_names_in_all)\n \n _dummy = TestCase('__init__')\n@@ -379,11 +377,6 @@ def __exit__(self, *exc_info):\n         clean_warning_registry()\n \n \n-assert_less = _dummy.assertLess\n-assert_greater = _dummy.assertGreater\n-\n-assert_allclose = np.testing.assert_allclose\n-\n def assert_raise_message(exceptions, message, function, *args, **kwargs):\n     \"\"\"Helper function to test the message raised in an exception.\n \ndiff --git a/sklearn/utils/validation.py b/sklearn/utils/validation.py\n--- a/sklearn/utils/validation.py\n+++ b/sklearn/utils/validation.py\n@@ -477,7 +477,7 @@ def check_array(array, accept_sparse=False, accept_large_sparse=True,\n     # check if the object contains several dtypes (typically a pandas\n     # DataFrame), and store them. If not, store None.\n     dtypes_orig = None\n-    if hasattr(array, \"dtypes\") and len(array.dtypes):\n+    if hasattr(array, \"dtypes\") and hasattr(array.dtypes, '__array__'):\n         dtypes_orig = np.array(array.dtypes)\n \n     if dtype_numeric:\n",
  "test_patch": "diff --git a/sklearn/compose/tests/test_column_transformer.py b/sklearn/compose/tests/test_column_transformer.py\n--- a/sklearn/compose/tests/test_column_transformer.py\n+++ b/sklearn/compose/tests/test_column_transformer.py\n@@ -542,6 +542,20 @@ def test_make_column_transformer():\n                                 ('first', 'drop'))\n \n \n+def test_make_column_transformer_pandas():\n+    pd = pytest.importorskip('pandas')\n+    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n+    X_df = pd.DataFrame(X_array, columns=['first', 'second'])\n+    norm = Normalizer()\n+    # XXX remove in v0.22\n+    with pytest.warns(DeprecationWarning,\n+                      match='`make_column_transformer` now expects'):\n+        ct1 = make_column_transformer((X_df.columns, norm))\n+    ct2 = make_column_transformer((norm, X_df.columns))\n+    assert_almost_equal(ct1.fit_transform(X_df),\n+                        ct2.fit_transform(X_df))\n+\n+\n def test_make_column_transformer_kwargs():\n     scaler = StandardScaler()\n     norm = Normalizer()\ndiff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py\n--- a/sklearn/linear_model/tests/test_logistic.py\n+++ b/sklearn/linear_model/tests/test_logistic.py\n@@ -1150,14 +1150,31 @@ def test_logreg_l1_sparse_data():\n \n @pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22\n @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22\n-def test_logreg_cv_penalty():\n-    # Test that the correct penalty is passed to the final fit.\n-    X, y = make_classification(n_samples=50, n_features=20, random_state=0)\n-    lr_cv = LogisticRegressionCV(penalty=\"l1\", Cs=[1.0], solver='saga')\n+@pytest.mark.parametrize(\"random_seed\", [42])\n+@pytest.mark.parametrize(\"penalty\", [\"l1\", \"l2\"])\n+def test_logistic_regression_cv_refit(random_seed, penalty):\n+    # Test that when refit=True, logistic regression cv with the saga solver\n+    # converges to the same solution as logistic regression with a fixed\n+    # regularization parameter.\n+    # Internally the LogisticRegressionCV model uses a warm start to refit on\n+    # the full data model with the optimal C found by CV. As the penalized\n+    # logistic regression loss is convex, we should still recover exactly\n+    # the same solution as long as the stopping criterion is strict enough (and\n+    # that there are no exactly duplicated features when penalty='l1').\n+    X, y = make_classification(n_samples=50, n_features=20,\n+                               random_state=random_seed)\n+    common_params = dict(\n+        solver='saga',\n+        penalty=penalty,\n+        random_state=random_seed,\n+        max_iter=10000,\n+        tol=1e-12,\n+    )\n+    lr_cv = LogisticRegressionCV(Cs=[1.0], refit=True, **common_params)\n     lr_cv.fit(X, y)\n-    lr = LogisticRegression(penalty=\"l1\", C=1.0, solver='saga')\n+    lr = LogisticRegression(C=1.0, **common_params)\n     lr.fit(X, y)\n-    assert_equal(np.count_nonzero(lr_cv.coef_), np.count_nonzero(lr.coef_))\n+    assert_array_almost_equal(lr_cv.coef_, lr.coef_)\n \n \n def test_logreg_predict_proba_multinomial():\ndiff --git a/sklearn/metrics/tests/test_pairwise.py b/sklearn/metrics/tests/test_pairwise.py\n--- a/sklearn/metrics/tests/test_pairwise.py\n+++ b/sklearn/metrics/tests/test_pairwise.py\n@@ -5,9 +5,12 @@\n \n from scipy.sparse import dok_matrix, csr_matrix, issparse\n from scipy.spatial.distance import cosine, cityblock, minkowski, wminkowski\n+from scipy.spatial.distance import cdist, pdist, squareform\n \n import pytest\n \n+from sklearn import config_context\n+\n from sklearn.utils.testing import assert_greater\n from sklearn.utils.testing import assert_array_almost_equal\n from sklearn.utils.testing import assert_allclose\n@@ -893,3 +896,39 @@ def test_check_preserve_type():\n                                                    XB.astype(np.float))\n     assert_equal(XA_checked.dtype, np.float)\n     assert_equal(XB_checked.dtype, np.float)\n+\n+\n+@pytest.mark.parametrize(\"n_jobs\", [1, 2])\n+@pytest.mark.parametrize(\"metric\", [\"seuclidean\", \"mahalanobis\"])\n+@pytest.mark.parametrize(\"dist_function\",\n+                         [pairwise_distances, pairwise_distances_chunked])\n+@pytest.mark.parametrize(\"y_is_x\", [True, False], ids=[\"Y is X\", \"Y is not X\"])\n+def test_pairwise_distances_data_derived_params(n_jobs, metric, dist_function,\n+                                                y_is_x):\n+    # check that pairwise_distances give the same result in sequential and\n+    # parallel, when metric has data-derived parameters.\n+    with config_context(working_memory=0.1):  # to have more than 1 chunk\n+        rng = np.random.RandomState(0)\n+        X = rng.random_sample((1000, 10))\n+\n+        if y_is_x:\n+            Y = X\n+            expected_dist_default_params = squareform(pdist(X, metric=metric))\n+            if metric == \"seuclidean\":\n+                params = {'V': np.var(X, axis=0, ddof=1)}\n+            else:\n+                params = {'VI': np.linalg.inv(np.cov(X.T)).T}\n+        else:\n+            Y = rng.random_sample((1000, 10))\n+            expected_dist_default_params = cdist(X, Y, metric=metric)\n+            if metric == \"seuclidean\":\n+                params = {'V': np.var(np.vstack([X, Y]), axis=0, ddof=1)}\n+            else:\n+                params = {'VI': np.linalg.inv(np.cov(np.vstack([X, Y]).T)).T}\n+\n+        expected_dist_explicit_params = cdist(X, Y, metric=metric, **params)\n+        dist = np.vstack(tuple(dist_function(X, Y,\n+                                             metric=metric, n_jobs=n_jobs)))\n+\n+        assert_allclose(dist, expected_dist_explicit_params)\n+        assert_allclose(dist, expected_dist_default_params)\ndiff --git a/sklearn/model_selection/tests/test_split.py b/sklearn/model_selection/tests/test_split.py\n--- a/sklearn/model_selection/tests/test_split.py\n+++ b/sklearn/model_selection/tests/test_split.py\n@@ -458,13 +458,13 @@ def test_shuffle_kfold():\n \n \n def test_shuffle_kfold_stratifiedkfold_reproducibility():\n-    # Check that when the shuffle is True multiple split calls produce the\n-    # same split when random_state is set\n     X = np.ones(15)  # Divisible by 3\n     y = [0] * 7 + [1] * 8\n     X2 = np.ones(16)  # Not divisible by 3\n     y2 = [0] * 8 + [1] * 8\n \n+    # Check that when the shuffle is True, multiple split calls produce the\n+    # same split when random_state is int\n     kf = KFold(3, shuffle=True, random_state=0)\n     skf = StratifiedKFold(3, shuffle=True, random_state=0)\n \n@@ -472,8 +472,12 @@ def test_shuffle_kfold_stratifiedkfold_reproducibility():\n         np.testing.assert_equal(list(cv.split(X, y)), list(cv.split(X, y)))\n         np.testing.assert_equal(list(cv.split(X2, y2)), list(cv.split(X2, y2)))\n \n-    kf = KFold(3, shuffle=True)\n-    skf = StratifiedKFold(3, shuffle=True)\n+    # Check that when the shuffle is True, multiple split calls often\n+    # (not always) produce different splits when random_state is\n+    # RandomState instance or None\n+    kf = KFold(3, shuffle=True, random_state=np.random.RandomState(0))\n+    skf = StratifiedKFold(3, shuffle=True,\n+                          random_state=np.random.RandomState(0))\n \n     for cv in (kf, skf):\n         for data in zip((X, X2), (y, y2)):\ndiff --git a/sklearn/neighbors/tests/test_dist_metrics.py b/sklearn/neighbors/tests/test_dist_metrics.py\n--- a/sklearn/neighbors/tests/test_dist_metrics.py\n+++ b/sklearn/neighbors/tests/test_dist_metrics.py\n@@ -6,6 +6,8 @@\n \n import pytest\n \n+from distutils.version import LooseVersion\n+from scipy import __version__ as scipy_version\n from scipy.spatial.distance import cdist\n from sklearn.neighbors.dist_metrics import DistanceMetric\n from sklearn.neighbors import BallTree\n@@ -101,6 +103,11 @@ def check_pdist(metric, kwargs, D_true):\n def check_pdist_bool(metric, D_true):\n     dm = DistanceMetric.get_metric(metric)\n     D12 = dm.pairwise(X1_bool)\n+    # Based on https://github.com/scipy/scipy/pull/7373\n+    # When comparing two all-zero vectors, scipy>=1.2.0 jaccard metric\n+    # was changed to return 0, instead of nan.\n+    if metric == 'jaccard' and LooseVersion(scipy_version) < '1.2.0':\n+        D_true[np.isnan(D_true)] = 0\n     assert_array_almost_equal(D12, D_true)\n \n \ndiff --git a/sklearn/utils/tests/test_validation.py b/sklearn/utils/tests/test_validation.py\n--- a/sklearn/utils/tests/test_validation.py\n+++ b/sklearn/utils/tests/test_validation.py\n@@ -700,6 +700,11 @@ def test_check_array_series():\n                       warn_on_dtype=True)\n     assert_array_equal(res, np.array([1, 2, 3]))\n \n+    # with categorical dtype (not a numpy dtype) (GH12699)\n+    s = pd.Series(['a', 'b', 'c']).astype('category')\n+    res = check_array(s, dtype=None, ensure_2d=False)\n+    assert_array_equal(res, np.array(['a', 'b', 'c'], dtype=object))\n+\n \n def test_check_dataframe_warns_on_dtype():\n     # Check that warn_on_dtype also works for DataFrames.\n",
  "problem_statement": "KNeighborsRegressor gives different results for different n_jobs values\n<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn\r\n- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn\r\nFor more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->\r\nWhen using 'seuclidean' distance metric, the algorithm produces different predictions for different values of the n_jobs parameter if no V is passed as additional metric_params. This implies that if configured with n_jobs=-1 two different machines show different results depending on the number of cores. The same happens for 'mahalanobis' distance metric if no V and VI are passed as metric_params.\r\n\r\n#### Steps/Code to Reproduce\r\n<!--\r\nExample:\r\n```python\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nfrom sklearn.decomposition import LatentDirichletAllocation\r\n\r\ndocs = [\"Help I have a bug\" for i in range(1000)]\r\n\r\nvectorizer = CountVectorizer(input=docs, analyzer='word')\r\nlda_features = vectorizer.fit_transform(docs)\r\n\r\nlda_model = LatentDirichletAllocation(\r\n    n_topics=10,\r\n    learning_method='online',\r\n    evaluate_every=10,\r\n    n_jobs=4,\r\n)\r\nmodel = lda_model.fit(lda_features)\r\n```\r\nIf the code is too long, feel free to put it in a public gist and link\r\nit in the issue: https://gist.github.com\r\n-->\r\n```python\r\n# Import required packages\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom sklearn.datasets import load_boston\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.neighbors import KNeighborsRegressor\r\n\r\n# Prepare the dataset\r\ndataset = load_boston()\r\ntarget = dataset.target\r\ndata = pd.DataFrame(dataset.data, columns=dataset.feature_names)\r\n\r\n# Split the dataset\r\nnp.random.seed(42)\r\nX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\r\n\r\n# Create a regressor with seuclidean distance and passing V as additional argument\r\nmodel_n_jobs_1 = KNeighborsRegressor(n_jobs=1, algorithm='brute', metric='seuclidean')\r\nmodel_n_jobs_1.fit(X_train, y_train)\r\nnp.sum(model_n_jobs_1.predict(X_test)) # --> 2127.99999\r\n\r\n# Create a regressor with seuclidean distance and passing V as additional argument\r\nmodel_n_jobs_3 = KNeighborsRegressor(n_jobs=3, algorithm='brute', metric='seuclidean')\r\nmodel_n_jobs_3.fit(X_train, y_train)\r\nnp.sum(model_n_jobs_3.predict(X_test)) # --> 2129.38\r\n\r\n# Create a regressor with seuclidean distance and passing V as additional argument\r\nmodel_n_jobs_all = KNeighborsRegressor(n_jobs=-1, algorithm='brute', metric='seuclidean')\r\nmodel_n_jobs_all.fit(X_train, y_train)\r\nnp.sum(model_n_jobs_all.predict(X_test)) # --> 2125.29999\r\n```\r\n\r\n#### Expected Results\r\n<!-- Example: No error is thrown. Please paste or describe the expected results.-->\r\nThe prediction should be always the same and not depend on the value passed to the n_jobs parameter.\r\n\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback. -->\r\nThe prediction value changes depending on the value passed to n_jobs which, in case of n_jobs=-1, makes the prediction depend on the number of cores of the machine running the code.\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nFor scikit-learn >= 0.20:\r\nimport sklearn; sklearn.show_versions()\r\nFor scikit-learn < 0.20:\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport numpy; print(\"NumPy\", numpy.__version__)\r\nimport scipy; print(\"SciPy\", scipy.__version__)\r\nimport sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\n-->\r\nSystem\r\n------\r\n    python: 3.6.6 (default, Jun 28 2018, 04:42:43)  [GCC 5.4.0 20160609]\r\n    executable: /home/mcorella/.local/share/virtualenvs/outlier_detection-8L4UL10d/bin/python3.6\r\n    machine: Linux-4.15.0-39-generic-x86_64-with-Ubuntu-16.04-xenial\r\n\r\nBLAS\r\n----\r\n    macros: NO_ATLAS_INFO=1, HAVE_CBLAS=None\r\n    lib_dirs: /usr/lib\r\n    cblas_libs: cblas\r\n\r\nPython deps\r\n-----------\r\n    pip: 18.1\r\n    setuptools: 40.5.0\r\n    sklearn: 0.20.0\r\n    numpy: 1.15.4\r\n    scipy: 1.1.0\r\n    Cython: None\r\n    pandas: 0.23.4\r\n\r\n<!-- Thanks for contributing! -->\r\n\nutils.validation.check_array throws bad TypeError pandas series is passed in\n#### Description\r\nvalidation.check_array throws bad TypeError pandas series is passed in. It cropped up when using the RandomizedSearchCV class.  Caused when line 480 is executed\r\n\r\n480 - if hasattr(array, \"dtypes\") and len(array.dtypes):\r\n\r\n#### Steps/Code to Reproduce\r\n\r\nvalidation.check_array(y, ensure_2d=False, dtype=None) where y is a pandas series\r\n\r\n#### Expected Results\r\nNo error (I'm not familiar with this code so not sure on the details)\r\n\r\n#### Actual Results\r\nTypeError: object of type 'DTYPE NAME OF THE SERIES' has no len()\r\n\r\n#### Versions\r\n0.20.1\n",
  "hints_text": "\n",
  "created_at": "2018-12-14T16:48:26Z",
  "version": "0.20",
  "FAIL_TO_PASS": "[\"sklearn/compose/tests/test_column_transformer.py::test_make_column_transformer_pandas\", \"sklearn/utils/tests/test_validation.py::test_check_array_series\"]",
  "PASS_TO_PASS": "[\"sklearn/compose/tests/test_column_transformer.py::test_column_transformer\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_dataframe\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_empty_columns[list-pandas]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_empty_columns[list-numpy]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_empty_columns[bool-pandas]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_empty_columns[bool-numpy]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_sparse_array\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_list\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_sparse_stacking\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_mixed_cols_sparse\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_sparse_threshold\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_error_msg_1D\", \"sklearn/compose/tests/test_column_transformer.py::test_2D_transformer_output\", \"sklearn/compose/tests/test_column_transformer.py::test_2D_transformer_output_pandas\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_invalid_columns[drop]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_invalid_columns[passthrough]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_invalid_transformer\", \"sklearn/compose/tests/test_column_transformer.py::test_make_column_transformer\", \"sklearn/compose/tests/test_column_transformer.py::test_make_column_transformer_kwargs\", \"sklearn/compose/tests/test_column_transformer.py::test_make_column_transformer_remainder_transformer\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_get_set_params\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_named_estimators\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_cloning\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_get_feature_names\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_special_strings\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_remainder\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_remainder_numpy[key0]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_remainder_numpy[key1]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_remainder_numpy[key2]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_remainder_numpy[key3]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_remainder_pandas[key0]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_remainder_pandas[key1]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_remainder_pandas[key2]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_remainder_pandas[key3]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_remainder_pandas[pd-index]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_remainder_pandas[key5]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_remainder_pandas[key6]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_remainder_pandas[key7]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_remainder_pandas[key8]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_remainder_transformer[key0]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_remainder_transformer[key1]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_remainder_transformer[key2]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_remainder_transformer[key3]\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_no_remaining_remainder_transformer\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_drops_all_remainder_transformer\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_sparse_remainder_transformer\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_drop_all_sparse_remainder_transformer\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_get_set_params_with_remainder\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_no_estimators\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_no_estimators_set_params\", \"sklearn/compose/tests/test_column_transformer.py::test_column_transformer_callable_specifier\", \"sklearn/linear_model/tests/test_logistic.py::test_predict_2_classes\", \"sklearn/linear_model/tests/test_logistic.py::test_error\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv_mock_scorer\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv_score_does_not_warn_by_default\", \"sklearn/linear_model/tests/test_logistic.py::test_lr_liblinear_warning\", \"sklearn/linear_model/tests/test_logistic.py::test_predict_3_classes\", \"sklearn/linear_model/tests/test_logistic.py::test_predict_iris\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_validation[lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_validation[newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_validation[sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_validation[saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_check_solver_option[LogisticRegression]\", \"sklearn/linear_model/tests/test_logistic.py::test_check_solver_option[LogisticRegressionCV]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_warnings[LogisticRegression-params0-True]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_warnings[LogisticRegressionCV-params1-False]\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_binary[lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_binary[newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_binary[sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_binary[saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_binary_probabilities\", \"sklearn/linear_model/tests/test_logistic.py::test_sparsify\", \"sklearn/linear_model/tests/test_logistic.py::test_inconsistent_input\", \"sklearn/linear_model/tests/test_logistic.py::test_write_parameters\", \"sklearn/linear_model/tests/test_logistic.py::test_nan\", \"sklearn/linear_model/tests/test_logistic.py::test_consistency_path\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_path_convergence_fail\", \"sklearn/linear_model/tests/test_logistic.py::test_liblinear_dual_random_state\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_loss_and_grad\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_grad_hess\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv_multinomial_score[accuracy-multiclass_agg_list0]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv_multinomial_score[precision-multiclass_agg_list1]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv_multinomial_score[f1-multiclass_agg_list2]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv_multinomial_score[neg_log_loss-multiclass_agg_list3]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv_multinomial_score[recall-multiclass_agg_list4]\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_logistic_regression_string_inputs\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv_sparse\", \"sklearn/linear_model/tests/test_logistic.py::test_intercept_logistic_helper\", \"sklearn/linear_model/tests/test_logistic.py::test_ovr_multinomial_iris\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_solvers\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_solvers_multiclass\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regressioncv_class_weights\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_sample_weights\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_class_weights\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multinomial\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_grad_hess\", \"sklearn/linear_model/tests/test_logistic.py::test_liblinear_decision_function_zero\", \"sklearn/linear_model/tests/test_logistic.py::test_liblinear_logregcv_sparse\", \"sklearn/linear_model/tests/test_logistic.py::test_saga_sparse\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_intercept_scaling\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_intercept_scaling_zero\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_l1\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_l1_sparse_data\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_cv_refit[l1-42]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_cv_refit[l2-42]\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_predict_proba_multinomial\", \"sklearn/linear_model/tests/test_logistic.py::test_max_iter\", \"sklearn/linear_model/tests/test_logistic.py::test_n_iter[newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_n_iter[liblinear]\", \"sklearn/linear_model/tests/test_logistic.py::test_n_iter[sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_n_iter[saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_n_iter[lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-True-True-newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-True-True-sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-True-True-saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-True-True-lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-True-False-newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-True-False-sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-True-False-saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-True-False-lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-False-True-newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-False-True-sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-False-True-saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-False-True-lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-False-False-newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-False-False-sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-False-False-saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-False-False-lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-True-True-newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-True-True-sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-True-True-saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-True-True-lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-True-False-newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-True-False-sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-True-False-saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-True-False-lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-False-True-newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-False-True-sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-False-True-saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-False-True-lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-False-False-newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-False-False-sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-False-False-saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-False-False-lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_saga_vs_liblinear\", \"sklearn/linear_model/tests/test_logistic.py::test_dtype_match[ovr]\", \"sklearn/linear_model/tests/test_logistic.py::test_dtype_match[multinomial]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start_converge_LR\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_path_coefs_multinomial\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[liblinear-est0]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[liblinear-est1]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[lbfgs-est0]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[lbfgs-est1]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[newton-cg-est0]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[newton-cg-est1]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[sag-est0]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[sag-est1]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[saga-est0]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[saga-est1]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_distances\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_boolean_distance[dice]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_boolean_distance[jaccard]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_boolean_distance[kulsinski]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_boolean_distance[matching]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_boolean_distance[rogerstanimoto]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_boolean_distance[russellrao]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_boolean_distance[sokalmichener]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_boolean_distance[sokalsneath]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_boolean_distance[yule]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_precomputed[pairwise_distances]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_precomputed[pairwise_kernels]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_parallel[pairwise_distances-euclidean-kwds0]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_parallel[pairwise_distances-wminkowski-kwds1]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_parallel[pairwise_distances-wminkowski-kwds2]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_parallel[pairwise_kernels-polynomial-kwds3]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_parallel[pairwise_kernels-callable_rbf_kernel-kwds4]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_callable_nonstrict_metric\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_kernels[rbf]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_kernels[laplacian]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_kernels[sigmoid]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_kernels[polynomial]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_kernels[linear]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_kernels[chi2]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_kernels[additive_chi2]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_kernels_callable\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_kernels_filter_param\", \"sklearn/metrics/tests/test_pairwise.py::test_paired_distances[cosine-paired_cosine_distances]\", \"sklearn/metrics/tests/test_pairwise.py::test_paired_distances[euclidean-paired_euclidean_distances]\", \"sklearn/metrics/tests/test_pairwise.py::test_paired_distances[l2-paired_euclidean_distances]\", \"sklearn/metrics/tests/test_pairwise.py::test_paired_distances[l1-paired_manhattan_distances]\", \"sklearn/metrics/tests/test_pairwise.py::test_paired_distances[manhattan-paired_manhattan_distances]\", \"sklearn/metrics/tests/test_pairwise.py::test_paired_distances[cityblock-paired_manhattan_distances]\", \"sklearn/metrics/tests/test_pairwise.py::test_paired_distances_callable\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_distances_argmin_min\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_distances_chunked_reduce\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_distances_chunked_reduce_valid[<lambda>0]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_distances_chunked_reduce_valid[<lambda>1]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_distances_chunked_reduce_valid[<lambda>2]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_distances_chunked_reduce_valid[<lambda>3]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_distances_chunked_reduce_valid[<lambda>4]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_distances_chunked_reduce_invalid[<lambda>-ValueError-length\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_distances_chunked_reduce_invalid[<lambda>-TypeError-returned\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_distances_chunked_reduce_invalid[<lambda>-TypeError-,\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_distances_chunked_diagonal[euclidean]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_distances_chunked_diagonal[l2]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_distances_chunked_diagonal[sqeuclidean]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_distances_chunked\", \"sklearn/metrics/tests/test_pairwise.py::test_euclidean_distances\", \"sklearn/metrics/tests/test_pairwise.py::test_cosine_distances\", \"sklearn/metrics/tests/test_pairwise.py::test_paired_euclidean_distances\", \"sklearn/metrics/tests/test_pairwise.py::test_paired_manhattan_distances\", \"sklearn/metrics/tests/test_pairwise.py::test_chi_square_kernel\", \"sklearn/metrics/tests/test_pairwise.py::test_kernel_symmetry[linear_kernel]\", \"sklearn/metrics/tests/test_pairwise.py::test_kernel_symmetry[polynomial_kernel]\", \"sklearn/metrics/tests/test_pairwise.py::test_kernel_symmetry[rbf_kernel]\", \"sklearn/metrics/tests/test_pairwise.py::test_kernel_symmetry[laplacian_kernel]\", \"sklearn/metrics/tests/test_pairwise.py::test_kernel_symmetry[sigmoid_kernel]\", \"sklearn/metrics/tests/test_pairwise.py::test_kernel_symmetry[cosine_similarity]\", \"sklearn/metrics/tests/test_pairwise.py::test_kernel_sparse[linear_kernel]\", \"sklearn/metrics/tests/test_pairwise.py::test_kernel_sparse[polynomial_kernel]\", \"sklearn/metrics/tests/test_pairwise.py::test_kernel_sparse[rbf_kernel]\", \"sklearn/metrics/tests/test_pairwise.py::test_kernel_sparse[laplacian_kernel]\", \"sklearn/metrics/tests/test_pairwise.py::test_kernel_sparse[sigmoid_kernel]\", \"sklearn/metrics/tests/test_pairwise.py::test_kernel_sparse[cosine_similarity]\", \"sklearn/metrics/tests/test_pairwise.py::test_linear_kernel\", \"sklearn/metrics/tests/test_pairwise.py::test_rbf_kernel\", \"sklearn/metrics/tests/test_pairwise.py::test_laplacian_kernel\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_similarity_sparse_output[linear-linear_kernel]\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_similarity_sparse_output[cosine-cosine_similarity]\", \"sklearn/metrics/tests/test_pairwise.py::test_cosine_similarity\", \"sklearn/metrics/tests/test_pairwise.py::test_check_dense_matrices\", \"sklearn/metrics/tests/test_pairwise.py::test_check_XB_returned\", \"sklearn/metrics/tests/test_pairwise.py::test_check_different_dimensions\", \"sklearn/metrics/tests/test_pairwise.py::test_check_invalid_dimensions\", \"sklearn/metrics/tests/test_pairwise.py::test_check_sparse_arrays\", \"sklearn/metrics/tests/test_pairwise.py::test_check_tuple_input\", \"sklearn/metrics/tests/test_pairwise.py::test_check_preserve_type\", \"sklearn/metrics/tests/test_pairwise.py::test_pairwise_distances_data_derived_params[Y\", \"sklearn/model_selection/tests/test_split.py::test_cross_validator_with_default_params\", \"sklearn/model_selection/tests/test_split.py::test_2d_y\", \"sklearn/model_selection/tests/test_split.py::test_kfold_valueerrors\", \"sklearn/model_selection/tests/test_split.py::test_kfold_indices\", \"sklearn/model_selection/tests/test_split.py::test_kfold_no_shuffle\", \"sklearn/model_selection/tests/test_split.py::test_stratified_kfold_no_shuffle\", \"sklearn/model_selection/tests/test_split.py::test_stratified_kfold_ratios\", \"sklearn/model_selection/tests/test_split.py::test_kfold_balance\", \"sklearn/model_selection/tests/test_split.py::test_stratifiedkfold_balance\", \"sklearn/model_selection/tests/test_split.py::test_shuffle_kfold\", \"sklearn/model_selection/tests/test_split.py::test_shuffle_stratifiedkfold\", \"sklearn/model_selection/tests/test_split.py::test_kfold_can_detect_dependent_samples_on_digits\", \"sklearn/model_selection/tests/test_split.py::test_shuffle_split\", \"sklearn/model_selection/tests/test_split.py::test_stratified_shuffle_split_init\", \"sklearn/model_selection/tests/test_split.py::test_stratified_shuffle_split_respects_test_size\", \"sklearn/model_selection/tests/test_split.py::test_stratified_shuffle_split_iter\", \"sklearn/model_selection/tests/test_split.py::test_stratified_shuffle_split_even\", \"sklearn/model_selection/tests/test_split.py::test_stratified_shuffle_split_overlap_train_test_bug\", \"sklearn/model_selection/tests/test_split.py::test_stratified_shuffle_split_multilabel\", \"sklearn/model_selection/tests/test_split.py::test_stratified_shuffle_split_multilabel_many_labels\", \"sklearn/model_selection/tests/test_split.py::test_predefinedsplit_with_kfold_split\", \"sklearn/model_selection/tests/test_split.py::test_group_shuffle_split\", \"sklearn/model_selection/tests/test_split.py::test_leave_one_p_group_out\", \"sklearn/model_selection/tests/test_split.py::test_leave_group_out_changing_groups\", \"sklearn/model_selection/tests/test_split.py::test_leave_one_p_group_out_error_on_fewer_number_of_groups\", \"sklearn/model_selection/tests/test_split.py::test_repeated_cv_value_errors\", \"sklearn/model_selection/tests/test_split.py::test_repeated_kfold_determinstic_split\", \"sklearn/model_selection/tests/test_split.py::test_get_n_splits_for_repeated_kfold\", \"sklearn/model_selection/tests/test_split.py::test_get_n_splits_for_repeated_stratified_kfold\", \"sklearn/model_selection/tests/test_split.py::test_repeated_stratified_kfold_determinstic_split\", \"sklearn/model_selection/tests/test_split.py::test_train_test_split_errors\", \"sklearn/model_selection/tests/test_split.py::test_train_test_split\", \"sklearn/model_selection/tests/test_split.py::test_shufflesplit_errors\", \"sklearn/model_selection/tests/test_split.py::test_shufflesplit_reproducible\", \"sklearn/model_selection/tests/test_split.py::test_stratifiedshufflesplit_list_input\", \"sklearn/model_selection/tests/test_split.py::test_train_test_split_allow_nans\", \"sklearn/model_selection/tests/test_split.py::test_check_cv\", \"sklearn/model_selection/tests/test_split.py::test_cv_iterable_wrapper\", \"sklearn/model_selection/tests/test_split.py::test_group_kfold\", \"sklearn/model_selection/tests/test_split.py::test_time_series_cv\", \"sklearn/model_selection/tests/test_split.py::test_time_series_max_train_size\", \"sklearn/model_selection/tests/test_split.py::test_nested_cv\", \"sklearn/model_selection/tests/test_split.py::test_train_test_default_warning\", \"sklearn/model_selection/tests/test_split.py::test_nsplit_default_warn\", \"sklearn/model_selection/tests/test_split.py::test_check_cv_default_warn\", \"sklearn/model_selection/tests/test_split.py::test_build_repr\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_cdist[euclidean]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_cdist[cityblock]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_cdist[minkowski]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_cdist[chebyshev]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_cdist[seuclidean]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_cdist[wminkowski]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_cdist[mahalanobis]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_cdist[hamming]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_cdist[canberra]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_cdist[braycurtis]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_cdist_bool_metric[matching]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_cdist_bool_metric[jaccard]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_cdist_bool_metric[dice]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_cdist_bool_metric[kulsinski]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_cdist_bool_metric[rogerstanimoto]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_cdist_bool_metric[russellrao]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_cdist_bool_metric[sokalmichener]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_cdist_bool_metric[sokalsneath]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pdist[euclidean]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pdist[cityblock]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pdist[minkowski]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pdist[chebyshev]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pdist[seuclidean]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pdist[wminkowski]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pdist[mahalanobis]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pdist[hamming]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pdist[canberra]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pdist[braycurtis]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pdist_bool_metrics[matching]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pdist_bool_metrics[dice]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pdist_bool_metrics[kulsinski]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pdist_bool_metrics[rogerstanimoto]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pdist_bool_metrics[russellrao]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pdist_bool_metrics[sokalmichener]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pdist_bool_metrics[sokalsneath]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pickle[euclidean]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pickle[cityblock]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pickle[minkowski]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pickle[chebyshev]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pickle[seuclidean]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pickle[wminkowski]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pickle[mahalanobis]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pickle[hamming]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pickle[canberra]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pickle[braycurtis]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pickle_bool_metrics[matching]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pickle_bool_metrics[jaccard]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pickle_bool_metrics[dice]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pickle_bool_metrics[kulsinski]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pickle_bool_metrics[rogerstanimoto]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pickle_bool_metrics[russellrao]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pickle_bool_metrics[sokalmichener]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pickle_bool_metrics[sokalsneath]\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_haversine_metric\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_pyfunc_metric\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_bad_pyfunc_metric\", \"sklearn/neighbors/tests/test_dist_metrics.py::test_input_data_size\", \"sklearn/utils/tests/test_validation.py::test_as_float_array\", \"sklearn/utils/tests/test_validation.py::test_as_float_array_nan[X0]\", \"sklearn/utils/tests/test_validation.py::test_as_float_array_nan[X1]\", \"sklearn/utils/tests/test_validation.py::test_np_matrix\", \"sklearn/utils/tests/test_validation.py::test_memmap\", \"sklearn/utils/tests/test_validation.py::test_ordering\", \"sklearn/utils/tests/test_validation.py::test_check_array_force_all_finite_valid[asarray-inf-False]\", \"sklearn/utils/tests/test_validation.py::test_check_array_force_all_finite_valid[asarray-nan-allow-nan]\", \"sklearn/utils/tests/test_validation.py::test_check_array_force_all_finite_valid[asarray-nan-False]\", \"sklearn/utils/tests/test_validation.py::test_check_array_force_all_finite_valid[csr_matrix-inf-False]\", \"sklearn/utils/tests/test_validation.py::test_check_array_force_all_finite_valid[csr_matrix-nan-allow-nan]\", \"sklearn/utils/tests/test_validation.py::test_check_array_force_all_finite_valid[csr_matrix-nan-False]\", \"sklearn/utils/tests/test_validation.py::test_check_array_force_all_finiteinvalid[asarray-inf-True-Input\", \"sklearn/utils/tests/test_validation.py::test_check_array_force_all_finiteinvalid[asarray-inf-allow-nan-Input\", \"sklearn/utils/tests/test_validation.py::test_check_array_force_all_finiteinvalid[asarray-nan-True-Input\", \"sklearn/utils/tests/test_validation.py::test_check_array_force_all_finiteinvalid[asarray-nan-allow-inf-force_all_finite\", \"sklearn/utils/tests/test_validation.py::test_check_array_force_all_finiteinvalid[asarray-nan-1-Input\", \"sklearn/utils/tests/test_validation.py::test_check_array_force_all_finiteinvalid[csr_matrix-inf-True-Input\", \"sklearn/utils/tests/test_validation.py::test_check_array_force_all_finiteinvalid[csr_matrix-inf-allow-nan-Input\", \"sklearn/utils/tests/test_validation.py::test_check_array_force_all_finiteinvalid[csr_matrix-nan-True-Input\", \"sklearn/utils/tests/test_validation.py::test_check_array_force_all_finiteinvalid[csr_matrix-nan-allow-inf-force_all_finite\", \"sklearn/utils/tests/test_validation.py::test_check_array_force_all_finiteinvalid[csr_matrix-nan-1-Input\", \"sklearn/utils/tests/test_validation.py::test_check_array\", \"sklearn/utils/tests/test_validation.py::test_check_array_pandas_dtype_object_conversion\", \"sklearn/utils/tests/test_validation.py::test_check_array_on_mock_dataframe\", \"sklearn/utils/tests/test_validation.py::test_check_array_dtype_stability\", \"sklearn/utils/tests/test_validation.py::test_check_array_dtype_warning\", \"sklearn/utils/tests/test_validation.py::test_check_array_accept_sparse_type_exception\", \"sklearn/utils/tests/test_validation.py::test_check_array_accept_sparse_no_exception\", \"sklearn/utils/tests/test_validation.py::test_check_array_accept_large_sparse_no_exception[csr]\", \"sklearn/utils/tests/test_validation.py::test_check_array_accept_large_sparse_no_exception[csc]\", \"sklearn/utils/tests/test_validation.py::test_check_array_accept_large_sparse_no_exception[coo]\", \"sklearn/utils/tests/test_validation.py::test_check_array_accept_large_sparse_no_exception[bsr]\", \"sklearn/utils/tests/test_validation.py::test_check_array_accept_large_sparse_raise_exception[csr]\", \"sklearn/utils/tests/test_validation.py::test_check_array_accept_large_sparse_raise_exception[csc]\", \"sklearn/utils/tests/test_validation.py::test_check_array_accept_large_sparse_raise_exception[coo]\", \"sklearn/utils/tests/test_validation.py::test_check_array_accept_large_sparse_raise_exception[bsr]\", \"sklearn/utils/tests/test_validation.py::test_check_array_large_indices_non_supported_scipy_version[csr]\", \"sklearn/utils/tests/test_validation.py::test_check_array_large_indices_non_supported_scipy_version[csc]\", \"sklearn/utils/tests/test_validation.py::test_check_array_large_indices_non_supported_scipy_version[coo]\", \"sklearn/utils/tests/test_validation.py::test_check_array_large_indices_non_supported_scipy_version[bsr]\", \"sklearn/utils/tests/test_validation.py::test_check_array_min_samples_and_features_messages\", \"sklearn/utils/tests/test_validation.py::test_check_array_complex_data_error\", \"sklearn/utils/tests/test_validation.py::test_has_fit_parameter\", \"sklearn/utils/tests/test_validation.py::test_check_symmetric\", \"sklearn/utils/tests/test_validation.py::test_check_is_fitted\", \"sklearn/utils/tests/test_validation.py::test_check_consistent_length\", \"sklearn/utils/tests/test_validation.py::test_check_dataframe_fit_attribute\", \"sklearn/utils/tests/test_validation.py::test_suppress_validation\", \"sklearn/utils/tests/test_validation.py::test_check_dataframe_warns_on_dtype\", \"sklearn/utils/tests/test_validation.py::test_check_memory\", \"sklearn/utils/tests/test_validation.py::test_check_array_memmap[True]\", \"sklearn/utils/tests/test_validation.py::test_check_array_memmap[False]\", \"sklearn/utils/tests/test_validation.py::test_check_X_y_informative_error\", \"sklearn/utils/tests/test_validation.py::test_retrieve_samples_from_non_standard_shape\"]",
  "environment_setup_commit": "55bf5d93e5674f13a1134d93a11fd0cd11aabcd1",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.967933",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}