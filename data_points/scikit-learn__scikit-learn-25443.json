{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-25443",
  "base_commit": "677a4cfef679313cd437c6af9e0398a22df73ab6",
  "patch": "diff --git a/sklearn/neural_network/_multilayer_perceptron.py b/sklearn/neural_network/_multilayer_perceptron.py\n--- a/sklearn/neural_network/_multilayer_perceptron.py\n+++ b/sklearn/neural_network/_multilayer_perceptron.py\n@@ -607,6 +607,7 @@ def _fit_stochastic(\n             batch_size = np.clip(self.batch_size, 1, n_samples)\n \n         try:\n+            self.n_iter_ = 0\n             for it in range(self.max_iter):\n                 if self.shuffle:\n                     # Only shuffle the sample indices instead of X and y to\n",
  "test_patch": "diff --git a/sklearn/neural_network/tests/test_mlp.py b/sklearn/neural_network/tests/test_mlp.py\n--- a/sklearn/neural_network/tests/test_mlp.py\n+++ b/sklearn/neural_network/tests/test_mlp.py\n@@ -752,7 +752,7 @@ def test_warm_start_full_iteration(MLPEstimator):\n     clf.fit(X, y)\n     assert max_iter == clf.n_iter_\n     clf.fit(X, y)\n-    assert 2 * max_iter == clf.n_iter_\n+    assert max_iter == clf.n_iter_\n \n \n def test_n_iter_no_change():\n@@ -926,3 +926,25 @@ def test_mlp_warm_start_with_early_stopping(MLPEstimator):\n     mlp.set_params(max_iter=20)\n     mlp.fit(X_iris, y_iris)\n     assert len(mlp.validation_scores_) > n_validation_scores\n+\n+\n+@pytest.mark.parametrize(\"MLPEstimator\", [MLPClassifier, MLPRegressor])\n+@pytest.mark.parametrize(\"solver\", [\"sgd\", \"adam\", \"lbfgs\"])\n+def test_mlp_warm_start_no_convergence(MLPEstimator, solver):\n+    \"\"\"Check that we stop the number of iteration at `max_iter` when warm starting.\n+\n+    Non-regression test for:\n+    https://github.com/scikit-learn/scikit-learn/issues/24764\n+    \"\"\"\n+    model = MLPEstimator(\n+        solver=solver, warm_start=True, early_stopping=False, max_iter=10\n+    )\n+\n+    with pytest.warns(ConvergenceWarning):\n+        model.fit(X_iris, y_iris)\n+    assert model.n_iter_ == 10\n+\n+    model.set_params(max_iter=20)\n+    with pytest.warns(ConvergenceWarning):\n+        model.fit(X_iris, y_iris)\n+    assert model.n_iter_ == 20\n",
  "problem_statement": "With MLPClassifer, when warm_start is True or coeffs_ are provided, fit doesn’t respect max_iters\n#### Description\r\nWith MLPClassifer, when warm_start is True or coeffs_ are provided, fit doesn’t respect max_iters. The reason for this is, when fitting, max iteration check is equality (==) against self.n_iter_. When warm_start is true or coeffs_ are provided, initialize is not called; this method resets n_iter_ to 0. Based on this implementation, there is doubt as to the meaning of max_iter. Consider, if max_iter is 1 and fit terminates due to reaching maximum iterations, subsequent fittings with warm_start true will never terminate due to reaching maximum iterations. This is bug. An alternate interpretation is max_iter represents the maximum iterations per fit call. In this case, the implementation is also wrong. The later interpretation seems more reasonable.\r\n\r\n#### Steps/Code to Reproduce\r\n```\r\nimport numpy as np\r\nfrom sklearn.neural_network import MLPClassifier\r\n\r\nX = np.random.rand(100,10)\r\ny = np.random.random_integers(0, 1, (100,))\r\n\r\nclf = MLPClassifier(max_iter=1, warm_start=True, verbose=True)\r\nfor k in range(3):\r\n    clf.fit(X, y)\r\n```\r\n#### Expected Results\r\nIteration 1, loss = 0.72311215\r\nConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\r\nIteration 2, loss = 0.71843526\r\nConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\r\nIteration 3, loss = 0.71418678\r\nConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\r\n\r\n#### Actual Results\r\nIteration 1, loss = 0.72311215\r\nConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\r\nIteration 2, loss = 0.71843526\r\nIteration 3, loss = 0.71418678\r\n\r\n#### Versions\r\nWindows-7-6.1.7601-SP1\r\nPython 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)]\r\nNumPy 1.12.0\r\nSciPy 0.18.1\r\nScikit-Learn 0.18.1\r\n\r\n\n",
  "hints_text": "I would like to investigate this.\nJust change the **random_state** parameter to **0** i.e. **random_state=_0_**. This will give you the same result\n@Julisam sorry I don't follow.\nI think ``max_iter`` should probably be the total number of calls for consistency with ``RandomForest`` (and gradient boosting?). That means if max_iter is reached and you call fit it shouldn't do anything (and maybe give an error?).\r\n\r\nNot 100% this is the least unexpected behavior, though.",
  "created_at": "2023-01-20T14:46:21Z",
  "version": "1.3",
  "FAIL_TO_PASS": "[\"sklearn/neural_network/tests/test_mlp.py::test_warm_start_full_iteration[MLPClassifier]\", \"sklearn/neural_network/tests/test_mlp.py::test_warm_start_full_iteration[MLPRegressor]\", \"sklearn/neural_network/tests/test_mlp.py::test_mlp_warm_start_no_convergence[sgd-MLPClassifier]\", \"sklearn/neural_network/tests/test_mlp.py::test_mlp_warm_start_no_convergence[sgd-MLPRegressor]\", \"sklearn/neural_network/tests/test_mlp.py::test_mlp_warm_start_no_convergence[adam-MLPClassifier]\", \"sklearn/neural_network/tests/test_mlp.py::test_mlp_warm_start_no_convergence[adam-MLPRegressor]\"]",
  "PASS_TO_PASS": "[\"sklearn/neural_network/tests/test_mlp.py::test_alpha\", \"sklearn/neural_network/tests/test_mlp.py::test_fit\", \"sklearn/neural_network/tests/test_mlp.py::test_gradient\", \"sklearn/neural_network/tests/test_mlp.py::test_lbfgs_classification[X0-y0]\", \"sklearn/neural_network/tests/test_mlp.py::test_lbfgs_classification[X1-y1]\", \"sklearn/neural_network/tests/test_mlp.py::test_lbfgs_regression[X0-y0]\", \"sklearn/neural_network/tests/test_mlp.py::test_lbfgs_classification_maxfun[X0-y0]\", \"sklearn/neural_network/tests/test_mlp.py::test_lbfgs_classification_maxfun[X1-y1]\", \"sklearn/neural_network/tests/test_mlp.py::test_lbfgs_regression_maxfun[X0-y0]\", \"sklearn/neural_network/tests/test_mlp.py::test_learning_rate_warmstart\", \"sklearn/neural_network/tests/test_mlp.py::test_multilabel_classification\", \"sklearn/neural_network/tests/test_mlp.py::test_multioutput_regression\", \"sklearn/neural_network/tests/test_mlp.py::test_partial_fit_classes_error\", \"sklearn/neural_network/tests/test_mlp.py::test_partial_fit_classification\", \"sklearn/neural_network/tests/test_mlp.py::test_partial_fit_unseen_classes\", \"sklearn/neural_network/tests/test_mlp.py::test_partial_fit_regression\", \"sklearn/neural_network/tests/test_mlp.py::test_partial_fit_errors\", \"sklearn/neural_network/tests/test_mlp.py::test_nonfinite_params\", \"sklearn/neural_network/tests/test_mlp.py::test_predict_proba_binary\", \"sklearn/neural_network/tests/test_mlp.py::test_predict_proba_multiclass\", \"sklearn/neural_network/tests/test_mlp.py::test_predict_proba_multilabel\", \"sklearn/neural_network/tests/test_mlp.py::test_shuffle\", \"sklearn/neural_network/tests/test_mlp.py::test_sparse_matrices\", \"sklearn/neural_network/tests/test_mlp.py::test_tolerance\", \"sklearn/neural_network/tests/test_mlp.py::test_verbose_sgd\", \"sklearn/neural_network/tests/test_mlp.py::test_early_stopping[MLPClassifier]\", \"sklearn/neural_network/tests/test_mlp.py::test_early_stopping[MLPRegressor]\", \"sklearn/neural_network/tests/test_mlp.py::test_adaptive_learning_rate\", \"sklearn/neural_network/tests/test_mlp.py::test_warm_start\", \"sklearn/neural_network/tests/test_mlp.py::test_n_iter_no_change\", \"sklearn/neural_network/tests/test_mlp.py::test_n_iter_no_change_inf\", \"sklearn/neural_network/tests/test_mlp.py::test_early_stopping_stratified\", \"sklearn/neural_network/tests/test_mlp.py::test_mlp_classifier_dtypes_casting\", \"sklearn/neural_network/tests/test_mlp.py::test_mlp_regressor_dtypes_casting\", \"sklearn/neural_network/tests/test_mlp.py::test_mlp_param_dtypes[MLPClassifier-float32]\", \"sklearn/neural_network/tests/test_mlp.py::test_mlp_param_dtypes[MLPClassifier-float64]\", \"sklearn/neural_network/tests/test_mlp.py::test_mlp_param_dtypes[MLPRegressor-float32]\", \"sklearn/neural_network/tests/test_mlp.py::test_mlp_param_dtypes[MLPRegressor-float64]\", \"sklearn/neural_network/tests/test_mlp.py::test_mlp_loading_from_joblib_partial_fit\", \"sklearn/neural_network/tests/test_mlp.py::test_preserve_feature_names[MLPClassifier]\", \"sklearn/neural_network/tests/test_mlp.py::test_preserve_feature_names[MLPRegressor]\", \"sklearn/neural_network/tests/test_mlp.py::test_mlp_warm_start_with_early_stopping[MLPClassifier]\", \"sklearn/neural_network/tests/test_mlp.py::test_mlp_warm_start_with_early_stopping[MLPRegressor]\", \"sklearn/neural_network/tests/test_mlp.py::test_mlp_warm_start_no_convergence[lbfgs-MLPClassifier]\", \"sklearn/neural_network/tests/test_mlp.py::test_mlp_warm_start_no_convergence[lbfgs-MLPRegressor]\"]",
  "environment_setup_commit": "1e8a5b833d1b58f3ab84099c4582239af854b23a",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:31.020195",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}