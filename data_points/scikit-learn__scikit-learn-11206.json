{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-11206",
  "base_commit": "4143356c3c51831300789e4fdf795d83716dbab6",
  "patch": "diff --git a/sklearn/decomposition/incremental_pca.py b/sklearn/decomposition/incremental_pca.py\n--- a/sklearn/decomposition/incremental_pca.py\n+++ b/sklearn/decomposition/incremental_pca.py\n@@ -243,9 +243,10 @@ def partial_fit(self, X, y=None, check_input=True):\n \n         # Update stats - they are 0 if this is the fisrt step\n         col_mean, col_var, n_total_samples = \\\n-            _incremental_mean_and_var(X, last_mean=self.mean_,\n-                                      last_variance=self.var_,\n-                                      last_sample_count=self.n_samples_seen_)\n+            _incremental_mean_and_var(\n+                X, last_mean=self.mean_, last_variance=self.var_,\n+                last_sample_count=np.repeat(self.n_samples_seen_, X.shape[1]))\n+        n_total_samples = n_total_samples[0]\n \n         # Whitening\n         if self.n_samples_seen_ == 0:\ndiff --git a/sklearn/preprocessing/data.py b/sklearn/preprocessing/data.py\n--- a/sklearn/preprocessing/data.py\n+++ b/sklearn/preprocessing/data.py\n@@ -126,6 +126,9 @@ def scale(X, axis=0, with_mean=True, with_std=True, copy=True):\n \n     To avoid memory copy the caller should pass a CSC matrix.\n \n+    NaNs are treated as missing values: disregarded to compute the statistics,\n+    and maintained during the data transformation.\n+\n     For a comparison of the different scalers, transformers, and normalizers,\n     see :ref:`examples/preprocessing/plot_all_scaling.py\n     <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n@@ -138,7 +141,7 @@ def scale(X, axis=0, with_mean=True, with_std=True, copy=True):\n     \"\"\"  # noqa\n     X = check_array(X, accept_sparse='csc', copy=copy, ensure_2d=False,\n                     warn_on_dtype=True, estimator='the scale function',\n-                    dtype=FLOAT_DTYPES)\n+                    dtype=FLOAT_DTYPES, force_all_finite='allow-nan')\n     if sparse.issparse(X):\n         if with_mean:\n             raise ValueError(\n@@ -154,15 +157,15 @@ def scale(X, axis=0, with_mean=True, with_std=True, copy=True):\n     else:\n         X = np.asarray(X)\n         if with_mean:\n-            mean_ = np.mean(X, axis)\n+            mean_ = np.nanmean(X, axis)\n         if with_std:\n-            scale_ = np.std(X, axis)\n+            scale_ = np.nanstd(X, axis)\n         # Xr is a view on the original array that enables easy use of\n         # broadcasting on the axis in which we are interested in\n         Xr = np.rollaxis(X, axis)\n         if with_mean:\n             Xr -= mean_\n-            mean_1 = Xr.mean(axis=0)\n+            mean_1 = np.nanmean(Xr, axis=0)\n             # Verify that mean_1 is 'close to zero'. If X contains very\n             # large values, mean_1 can also be very large, due to a lack of\n             # precision of mean_. In this case, a pre-scaling of the\n@@ -179,7 +182,7 @@ def scale(X, axis=0, with_mean=True, with_std=True, copy=True):\n             scale_ = _handle_zeros_in_scale(scale_, copy=False)\n             Xr /= scale_\n             if with_mean:\n-                mean_2 = Xr.mean(axis=0)\n+                mean_2 = np.nanmean(Xr, axis=0)\n                 # If mean_2 is not 'close to zero', it comes from the fact that\n                 # scale_ is very small so that mean_2 = mean_1/scale_ > 0, even\n                 # if mean_1 was close to zero. The problem is thus essentially\n@@ -520,27 +523,31 @@ class StandardScaler(BaseEstimator, TransformerMixin):\n \n     Attributes\n     ----------\n-    scale_ : ndarray, shape (n_features,)\n-        Per feature relative scaling of the data.\n+    scale_ : ndarray or None, shape (n_features,)\n+        Per feature relative scaling of the data. Equal to ``None`` when\n+        ``with_std=False``.\n \n         .. versionadded:: 0.17\n            *scale_*\n \n-    mean_ : array of floats with shape [n_features]\n+    mean_ : ndarray or None, shape (n_features,)\n         The mean value for each feature in the training set.\n+        Equal to ``None`` when ``with_mean=False``.\n \n-    var_ : array of floats with shape [n_features]\n+    var_ : ndarray or None, shape (n_features,)\n         The variance for each feature in the training set. Used to compute\n-        `scale_`\n+        `scale_`. Equal to ``None`` when ``with_std=False``.\n \n-    n_samples_seen_ : int\n-        The number of samples processed by the estimator. Will be reset on\n-        new calls to fit, but increments across ``partial_fit`` calls.\n+    n_samples_seen_ : int or array, shape (n_features,)\n+        The number of samples processed by the estimator for each feature.\n+        If there are not missing samples, the ``n_samples_seen`` will be an\n+        integer, otherwise it will be an array.\n+        Will be reset on new calls to fit, but increments across\n+        ``partial_fit`` calls.\n \n     Examples\n     --------\n     >>> from sklearn.preprocessing import StandardScaler\n-    >>>\n     >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n     >>> scaler = StandardScaler()\n     >>> print(scaler.fit(data))\n@@ -564,6 +571,9 @@ class StandardScaler(BaseEstimator, TransformerMixin):\n \n     Notes\n     -----\n+    NaNs are treated as missing values: disregarded in fit, and maintained in\n+    transform.\n+\n     For a comparison of the different scalers, transformers, and normalizers,\n     see :ref:`examples/preprocessing/plot_all_scaling.py\n     <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n@@ -626,22 +636,41 @@ def partial_fit(self, X, y=None):\n             Ignored\n         \"\"\"\n         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n-                        warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)\n+                        warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES,\n+                        force_all_finite='allow-nan')\n \n         # Even in the case of `with_mean=False`, we update the mean anyway\n         # This is needed for the incremental computation of the var\n         # See incr_mean_variance_axis and _incremental_mean_variance_axis\n \n+        # if n_samples_seen_ is an integer (i.e. no missing values), we need to\n+        # transform it to a NumPy array of shape (n_features,) required by\n+        # incr_mean_variance_axis and _incremental_variance_axis\n+        if (hasattr(self, 'n_samples_seen_') and\n+                isinstance(self.n_samples_seen_, (int, np.integer))):\n+            self.n_samples_seen_ = np.repeat(self.n_samples_seen_,\n+                                             X.shape[1]).astype(np.int64)\n+\n         if sparse.issparse(X):\n             if self.with_mean:\n                 raise ValueError(\n                     \"Cannot center sparse matrices: pass `with_mean=False` \"\n                     \"instead. See docstring for motivation and alternatives.\")\n+\n+            sparse_constructor = (sparse.csr_matrix\n+                                  if X.format == 'csr' else sparse.csc_matrix)\n+            counts_nan = sparse_constructor(\n+                        (np.isnan(X.data), X.indices, X.indptr),\n+                        shape=X.shape).sum(axis=0).A.ravel()\n+\n+            if not hasattr(self, 'n_samples_seen_'):\n+                self.n_samples_seen_ = (X.shape[0] -\n+                                        counts_nan).astype(np.int64)\n+\n             if self.with_std:\n                 # First pass\n-                if not hasattr(self, 'n_samples_seen_'):\n+                if not hasattr(self, 'scale_'):\n                     self.mean_, self.var_ = mean_variance_axis(X, axis=0)\n-                    self.n_samples_seen_ = X.shape[0]\n                 # Next passes\n                 else:\n                     self.mean_, self.var_, self.n_samples_seen_ = \\\n@@ -652,15 +681,15 @@ def partial_fit(self, X, y=None):\n             else:\n                 self.mean_ = None\n                 self.var_ = None\n-                if not hasattr(self, 'n_samples_seen_'):\n-                    self.n_samples_seen_ = X.shape[0]\n-                else:\n-                    self.n_samples_seen_ += X.shape[0]\n+                if hasattr(self, 'scale_'):\n+                    self.n_samples_seen_ += X.shape[0] - counts_nan\n         else:\n-            # First pass\n             if not hasattr(self, 'n_samples_seen_'):\n+                self.n_samples_seen_ = np.zeros(X.shape[1], dtype=np.int64)\n+\n+            # First pass\n+            if not hasattr(self, 'scale_'):\n                 self.mean_ = .0\n-                self.n_samples_seen_ = 0\n                 if self.with_std:\n                     self.var_ = .0\n                 else:\n@@ -669,12 +698,18 @@ def partial_fit(self, X, y=None):\n             if not self.with_mean and not self.with_std:\n                 self.mean_ = None\n                 self.var_ = None\n-                self.n_samples_seen_ += X.shape[0]\n+                self.n_samples_seen_ += X.shape[0] - np.isnan(X).sum(axis=0)\n             else:\n                 self.mean_, self.var_, self.n_samples_seen_ = \\\n                     _incremental_mean_and_var(X, self.mean_, self.var_,\n                                               self.n_samples_seen_)\n \n+        # for backward-compatibility, reduce n_samples_seen_ to an integer\n+        # if the number of samples is the same for each feature (i.e. no\n+        # missing values)\n+        if np.ptp(self.n_samples_seen_) == 0:\n+            self.n_samples_seen_ = self.n_samples_seen_[0]\n+\n         if self.with_std:\n             self.scale_ = _handle_zeros_in_scale(np.sqrt(self.var_))\n         else:\n@@ -704,7 +739,8 @@ def transform(self, X, y='deprecated', copy=None):\n \n         copy = copy if copy is not None else self.copy\n         X = check_array(X, accept_sparse='csr', copy=copy, warn_on_dtype=True,\n-                        estimator=self, dtype=FLOAT_DTYPES)\n+                        estimator=self, dtype=FLOAT_DTYPES,\n+                        force_all_finite='allow-nan')\n \n         if sparse.issparse(X):\n             if self.with_mean:\ndiff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py\n--- a/sklearn/utils/estimator_checks.py\n+++ b/sklearn/utils/estimator_checks.py\n@@ -77,7 +77,7 @@\n                 'RandomForestRegressor', 'Ridge', 'RidgeCV']\n \n ALLOW_NAN = ['Imputer', 'SimpleImputer', 'MICEImputer',\n-             'MinMaxScaler', 'QuantileTransformer']\n+             'MinMaxScaler', 'StandardScaler', 'QuantileTransformer']\n \n \n def _yield_non_meta_checks(name, estimator):\ndiff --git a/sklearn/utils/extmath.py b/sklearn/utils/extmath.py\n--- a/sklearn/utils/extmath.py\n+++ b/sklearn/utils/extmath.py\n@@ -642,8 +642,7 @@ def make_nonnegative(X, min_value=0):\n     return X\n \n \n-def _incremental_mean_and_var(X, last_mean=.0, last_variance=None,\n-                              last_sample_count=0):\n+def _incremental_mean_and_var(X, last_mean, last_variance, last_sample_count):\n     \"\"\"Calculate mean update and a Youngs and Cramer variance update.\n \n     last_mean and last_variance are statistics computed at the last step by the\n@@ -664,7 +663,7 @@ def _incremental_mean_and_var(X, last_mean=.0, last_variance=None,\n \n     last_variance : array-like, shape: (n_features,)\n \n-    last_sample_count : int\n+    last_sample_count : array-like, shape (n_features,)\n \n     Returns\n     -------\n@@ -673,7 +672,11 @@ def _incremental_mean_and_var(X, last_mean=.0, last_variance=None,\n     updated_variance : array, shape (n_features,)\n         If None, only mean is computed\n \n-    updated_sample_count : int\n+    updated_sample_count : array, shape (n_features,)\n+\n+    Notes\n+    -----\n+    NaNs are ignored during the algorithm.\n \n     References\n     ----------\n@@ -689,9 +692,9 @@ def _incremental_mean_and_var(X, last_mean=.0, last_variance=None,\n     # new = the current increment\n     # updated = the aggregated stats\n     last_sum = last_mean * last_sample_count\n-    new_sum = X.sum(axis=0)\n+    new_sum = np.nansum(X, axis=0)\n \n-    new_sample_count = X.shape[0]\n+    new_sample_count = np.sum(~np.isnan(X), axis=0)\n     updated_sample_count = last_sample_count + new_sample_count\n \n     updated_mean = (last_sum + new_sum) / updated_sample_count\n@@ -699,17 +702,18 @@ def _incremental_mean_and_var(X, last_mean=.0, last_variance=None,\n     if last_variance is None:\n         updated_variance = None\n     else:\n-        new_unnormalized_variance = X.var(axis=0) * new_sample_count\n-        if last_sample_count == 0:  # Avoid division by 0\n-            updated_unnormalized_variance = new_unnormalized_variance\n-        else:\n+        new_unnormalized_variance = np.nanvar(X, axis=0) * new_sample_count\n+        last_unnormalized_variance = last_variance * last_sample_count\n+\n+        with np.errstate(divide='ignore'):\n             last_over_new_count = last_sample_count / new_sample_count\n-            last_unnormalized_variance = last_variance * last_sample_count\n             updated_unnormalized_variance = (\n-                last_unnormalized_variance +\n-                new_unnormalized_variance +\n+                last_unnormalized_variance + new_unnormalized_variance +\n                 last_over_new_count / updated_sample_count *\n                 (last_sum / last_over_new_count - new_sum) ** 2)\n+\n+        zeros = last_sample_count == 0\n+        updated_unnormalized_variance[zeros] = new_unnormalized_variance[zeros]\n         updated_variance = updated_unnormalized_variance / updated_sample_count\n \n     return updated_mean, updated_variance, updated_sample_count\ndiff --git a/sklearn/utils/sparsefuncs.py b/sklearn/utils/sparsefuncs.py\n--- a/sklearn/utils/sparsefuncs.py\n+++ b/sklearn/utils/sparsefuncs.py\n@@ -121,7 +121,7 @@ def incr_mean_variance_axis(X, axis, last_mean, last_var, last_n):\n     last_var : float array with shape (n_features,)\n         Array of feature-wise var to update with the new data X.\n \n-    last_n : int\n+    last_n : int with shape (n_features,)\n         Number of samples seen so far, excluded X.\n \n     Returns\n@@ -133,9 +133,13 @@ def incr_mean_variance_axis(X, axis, last_mean, last_var, last_n):\n     variances : float array with shape (n_features,)\n         Updated feature-wise variances.\n \n-    n : int\n+    n : int with shape (n_features,)\n         Updated number of seen samples.\n \n+    Notes\n+    -----\n+    NaNs are ignored in the algorithm.\n+\n     \"\"\"\n     _raise_error_wrong_axis(axis)\n \n",
  "test_patch": "diff --git a/sklearn/preprocessing/tests/test_common.py b/sklearn/preprocessing/tests/test_common.py\n--- a/sklearn/preprocessing/tests/test_common.py\n+++ b/sklearn/preprocessing/tests/test_common.py\n@@ -9,9 +9,11 @@\n from sklearn.base import clone\n \n from sklearn.preprocessing import minmax_scale\n+from sklearn.preprocessing import scale\n from sklearn.preprocessing import quantile_transform\n \n from sklearn.preprocessing import MinMaxScaler\n+from sklearn.preprocessing import StandardScaler\n from sklearn.preprocessing import QuantileTransformer\n \n from sklearn.utils.testing import assert_array_equal\n@@ -28,6 +30,8 @@ def _get_valid_samples_by_column(X, col):\n @pytest.mark.parametrize(\n     \"est, func, support_sparse\",\n     [(MinMaxScaler(), minmax_scale, False),\n+     (StandardScaler(), scale, False),\n+     (StandardScaler(with_mean=False), scale, True),\n      (QuantileTransformer(n_quantiles=10), quantile_transform, True)]\n )\n def test_missing_value_handling(est, func, support_sparse):\n@@ -66,7 +70,7 @@ def test_missing_value_handling(est, func, support_sparse):\n         est.fit(_get_valid_samples_by_column(X_train, i))\n         # check transforming with NaN works even when training without NaN\n         Xt_col = est.transform(X_test[:, [i]])\n-        assert_array_equal(Xt_col, Xt[:, [i]])\n+        assert_allclose(Xt_col, Xt[:, [i]])\n         # check non-NaN is handled as before - the 1st column is all nan\n         if not np.isnan(X_test[:, i]).all():\n             Xt_col_nonan = est.transform(\ndiff --git a/sklearn/preprocessing/tests/test_data.py b/sklearn/preprocessing/tests/test_data.py\n--- a/sklearn/preprocessing/tests/test_data.py\n+++ b/sklearn/preprocessing/tests/test_data.py\n@@ -33,6 +33,7 @@\n from sklearn.utils.testing import assert_warns_message\n from sklearn.utils.testing import assert_no_warnings\n from sklearn.utils.testing import assert_allclose\n+from sklearn.utils.testing import assert_allclose_dense_sparse\n from sklearn.utils.testing import skip_if_32bit\n from sklearn.utils.testing import SkipTest\n \n@@ -703,6 +704,28 @@ def test_scaler_without_centering():\n     assert_array_almost_equal(X_csc_scaled_back.toarray(), X)\n \n \n+@pytest.mark.parametrize(\"with_mean\", [True, False])\n+@pytest.mark.parametrize(\"with_std\", [True, False])\n+@pytest.mark.parametrize(\"array_constructor\",\n+                         [np.asarray, sparse.csc_matrix, sparse.csr_matrix])\n+def test_scaler_n_samples_seen_with_nan(with_mean, with_std,\n+                                        array_constructor):\n+    X = np.array([[0, 1, 3],\n+                  [np.nan, 6, 10],\n+                  [5, 4, np.nan],\n+                  [8, 0, np.nan]],\n+                 dtype=np.float64)\n+    X = array_constructor(X)\n+\n+    if sparse.issparse(X) and with_mean:\n+        pytest.skip(\"'with_mean=True' cannot be used with sparse matrix.\")\n+\n+    transformer = StandardScaler(with_mean=with_mean, with_std=with_std)\n+    transformer.fit(X)\n+\n+    assert_array_equal(transformer.n_samples_seen_, np.array([3, 4, 2]))\n+\n+\n def _check_identity_scalers_attributes(scaler_1, scaler_2):\n     assert scaler_1.mean_ is scaler_2.mean_ is None\n     assert scaler_1.var_ is scaler_2.var_ is None\n@@ -729,8 +752,8 @@ def test_scaler_return_identity():\n     transformer_csc = clone(transformer_dense)\n     X_trans_csc = transformer_csc.fit_transform(X_csc)\n \n-    assert_allclose(X_trans_csr.toarray(), X_csr.toarray())\n-    assert_allclose(X_trans_csc.toarray(), X_csc.toarray())\n+    assert_allclose_dense_sparse(X_trans_csr, X_csr)\n+    assert_allclose_dense_sparse(X_trans_csc, X_csc)\n     assert_allclose(X_trans_dense, X_dense)\n \n     for trans_1, trans_2 in itertools.combinations([transformer_dense,\n@@ -881,14 +904,9 @@ def test_scale_sparse_with_mean_raise_exception():\n \n def test_scale_input_finiteness_validation():\n     # Check if non finite inputs raise ValueError\n-    X = [[np.nan, 5, 6, 7, 8]]\n-    assert_raises_regex(ValueError,\n-                        \"Input contains NaN, infinity or a value too large\",\n-                        scale, X)\n-\n     X = [[np.inf, 5, 6, 7, 8]]\n     assert_raises_regex(ValueError,\n-                        \"Input contains NaN, infinity or a value too large\",\n+                        \"Input contains infinity or a value too large\",\n                         scale, X)\n \n \ndiff --git a/sklearn/utils/tests/test_extmath.py b/sklearn/utils/tests/test_extmath.py\n--- a/sklearn/utils/tests/test_extmath.py\n+++ b/sklearn/utils/tests/test_extmath.py\n@@ -13,6 +13,7 @@\n \n from sklearn.utils.testing import assert_equal\n from sklearn.utils.testing import assert_almost_equal\n+from sklearn.utils.testing import assert_allclose\n from sklearn.utils.testing import assert_array_equal\n from sklearn.utils.testing import assert_array_almost_equal\n from sklearn.utils.testing import assert_true\n@@ -484,7 +485,7 @@ def test_incremental_variance_update_formulas():\n \n     old_means = X1.mean(axis=0)\n     old_variances = X1.var(axis=0)\n-    old_sample_count = X1.shape[0]\n+    old_sample_count = np.ones(X1.shape[1], dtype=np.int32) * X1.shape[0]\n     final_means, final_variances, final_count = \\\n         _incremental_mean_and_var(X2, old_means, old_variances,\n                                   old_sample_count)\n@@ -493,6 +494,30 @@ def test_incremental_variance_update_formulas():\n     assert_almost_equal(final_count, A.shape[0])\n \n \n+def test_incremental_mean_and_variance_ignore_nan():\n+    old_means = np.array([535., 535., 535., 535.])\n+    old_variances = np.array([4225., 4225., 4225., 4225.])\n+    old_sample_count = np.array([2, 2, 2, 2], dtype=np.int32)\n+\n+    X = np.array([[170, 170, 170, 170],\n+                  [430, 430, 430, 430],\n+                  [300, 300, 300, 300]])\n+\n+    X_nan = np.array([[170, np.nan, 170, 170],\n+                      [np.nan, 170, 430, 430],\n+                      [430, 430, np.nan, 300],\n+                      [300, 300, 300, np.nan]])\n+\n+    X_means, X_variances, X_count = _incremental_mean_and_var(\n+        X, old_means, old_variances, old_sample_count)\n+    X_nan_means, X_nan_variances, X_nan_count = _incremental_mean_and_var(\n+        X_nan, old_means, old_variances, old_sample_count)\n+\n+    assert_allclose(X_nan_means, X_means)\n+    assert_allclose(X_nan_variances, X_variances)\n+    assert_allclose(X_nan_count, X_count)\n+\n+\n @skip_if_32bit\n def test_incremental_variance_numerical_stability():\n     # Test Youngs and Cramer incremental variance formulas.\n@@ -562,12 +587,13 @@ def naive_mean_variance_update(x, last_mean, last_variance,\n     assert_greater(np.abs(stable_var(A) - var).max(), tol)\n \n     # Robust implementation: <tol (177)\n-    mean, var, n = A0[0, :], np.zeros(n_features), n_samples // 2\n+    mean, var = A0[0, :], np.zeros(n_features)\n+    n = np.ones(n_features, dtype=np.int32) * (n_samples // 2)\n     for i in range(A1.shape[0]):\n         mean, var, n = \\\n             _incremental_mean_and_var(A1[i, :].reshape((1, A1.shape[1])),\n                                       mean, var, n)\n-    assert_equal(n, A.shape[0])\n+    assert_array_equal(n, A.shape[0])\n     assert_array_almost_equal(A.mean(axis=0), mean)\n     assert_greater(tol, np.abs(stable_var(A) - var).max())\n \n@@ -589,7 +615,8 @@ def test_incremental_variance_ddof():\n                 incremental_variances = batch.var(axis=0)\n                 # Assign this twice so that the test logic is consistent\n                 incremental_count = batch.shape[0]\n-                sample_count = batch.shape[0]\n+                sample_count = (np.ones(batch.shape[1], dtype=np.int32) *\n+                                batch.shape[0])\n             else:\n                 result = _incremental_mean_and_var(\n                     batch, incremental_means, incremental_variances,\n@@ -603,7 +630,7 @@ def test_incremental_variance_ddof():\n             assert_almost_equal(incremental_means, calculated_means, 6)\n             assert_almost_equal(incremental_variances,\n                                 calculated_variances, 6)\n-            assert_equal(incremental_count, sample_count)\n+            assert_array_equal(incremental_count, sample_count)\n \n \n def test_vector_sign_flip():\ndiff --git a/sklearn/utils/tests/test_sparsefuncs.py b/sklearn/utils/tests/test_sparsefuncs.py\n--- a/sklearn/utils/tests/test_sparsefuncs.py\n+++ b/sklearn/utils/tests/test_sparsefuncs.py\n@@ -20,6 +20,7 @@\n                                             inplace_csr_row_normalize_l1,\n                                             inplace_csr_row_normalize_l2)\n from sklearn.utils.testing import assert_raises\n+from sklearn.utils.testing import assert_allclose\n \n \n def test_mean_variance_axis0():\n@@ -95,7 +96,7 @@ def test_incr_mean_variance_axis():\n         # default params for incr_mean_variance\n         last_mean = np.zeros(n_features)\n         last_var = np.zeros_like(last_mean)\n-        last_n = 0\n+        last_n = np.zeros_like(last_mean, dtype=np.int64)\n \n         # Test errors\n         X = np.array(data_chunks[0])\n@@ -137,6 +138,8 @@ def test_incr_mean_variance_axis():\n         for input_dtype, output_dtype in expected_dtypes:\n             for X_sparse in (X_csr, X_csc):\n                 X_sparse = X_sparse.astype(input_dtype)\n+                last_mean = last_mean.astype(output_dtype)\n+                last_var = last_var.astype(output_dtype)\n                 X_means, X_vars = mean_variance_axis(X_sparse, axis)\n                 X_means_incr, X_vars_incr, n_incr = \\\n                     incr_mean_variance_axis(X_sparse, axis, last_mean,\n@@ -148,6 +151,43 @@ def test_incr_mean_variance_axis():\n                 assert_equal(X.shape[axis], n_incr)\n \n \n+@pytest.mark.parametrize(\"axis\", [0, 1])\n+@pytest.mark.parametrize(\"sparse_constructor\", [sp.csc_matrix, sp.csr_matrix])\n+def test_incr_mean_variance_axis_ignore_nan(axis, sparse_constructor):\n+    old_means = np.array([535., 535., 535., 535.])\n+    old_variances = np.array([4225., 4225., 4225., 4225.])\n+    old_sample_count = np.array([2, 2, 2, 2], dtype=np.int64)\n+\n+    X = sparse_constructor(\n+        np.array([[170, 170, 170, 170],\n+                  [430, 430, 430, 430],\n+                  [300, 300, 300, 300]]))\n+\n+    X_nan = sparse_constructor(\n+        np.array([[170, np.nan, 170, 170],\n+                  [np.nan, 170, 430, 430],\n+                  [430, 430, np.nan, 300],\n+                  [300, 300, 300, np.nan]]))\n+\n+    # we avoid creating specific data for axis 0 and 1: translating the data is\n+    # enough.\n+    if axis:\n+        X = X.T\n+        X_nan = X_nan.T\n+\n+    # take a copy of the old statistics since they are modified in place.\n+    X_means, X_vars, X_sample_count = incr_mean_variance_axis(\n+        X, axis, old_means.copy(), old_variances.copy(),\n+        old_sample_count.copy())\n+    X_nan_means, X_nan_vars, X_nan_sample_count = incr_mean_variance_axis(\n+        X_nan, axis, old_means.copy(), old_variances.copy(),\n+        old_sample_count.copy())\n+\n+    assert_allclose(X_nan_means, X_means)\n+    assert_allclose(X_nan_vars, X_vars)\n+    assert_allclose(X_nan_sample_count, X_sample_count)\n+\n+\n def test_mean_variance_illegal_axis():\n     X, _ = make_classification(5, 4, random_state=0)\n     # Sparsify the array a little bit\n",
  "problem_statement": "increment_mean_and_var can now handle NaN values\n<!--\r\nThanks for contributing a pull request! Please ensure you have taken a look at\r\nthe contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist\r\n-->\r\n\r\n#### Reference Issues/PRs\r\n<!--\r\nExample: Fixes #1234. See also #3456.\r\nPlease use keywords (e.g., Fixes) to create link to the issues or pull requests\r\nyou resolved, so that they will automatically be closed when your pull request\r\nis merged. See https://github.com/blog/1506-closing-issues-via-pull-requests\r\n-->\r\n#10457 check if incremental_mean_and_var gives a green tick without failing in numerical_stability\r\n\r\n#### What does this implement/fix? Explain your changes.\r\n\r\n\r\n#### Any other comments?\r\n\r\n\r\n<!--\r\nPlease be aware that we are a loose team of volunteers so patience is\r\nnecessary; assistance handling other issues is very welcome. We value\r\nall user contributions, no matter how minor they are. If we are slow to\r\nreview, either the pull request needs some benchmarking, tinkering,\r\nconvincing, etc. or more likely the reviewers are simply busy. In either\r\ncase, we ask for your understanding during the review process.\r\nFor more information, see our FAQ on this topic:\r\nhttp://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.\r\n\r\nThanks for contributing!\r\n-->\r\n\n",
  "hints_text": "@jnothman i am constantly getting these mismatch of the values in an array calculated. since i am getting no error in my local system, it looks the only way to figure out which line of the code is creating this error is to undo all new codes and implement 1 step at a time and see if it gets a green tick.\r\n\r\nSo in short:\r\n* revert all changes\r\n* implement each step like np.nansum() and np.nanvar and then see if it keeps on getting a green tick\r\n\r\nPlease let me know your views before i start doing that (as this kind of approach is going to take up a lot of waiting time as travis and appveyor is extremely slow)\nI must admit that it appears quite perplexing for something like `test_incremental_variance_ddof` to succeed on one platform and fail drastically on others. Could I suggest you try installing an old version of cython (0.25.2 is failing) to see if this affects local test runs...?\r\n\r\nIf you really need to keep pushing to test your changes, you can limit the tests to relevant modules by modifying `appveyor.yml` and `build_tools/travis/test_script.sh`.\n(Then again, it seems appveyor is failing with the most recent cython)\nNah, it looks like cython should have nothing to do with it. Perhaps numpy version. Not sure... :\\\nBehaviour could have changed across numpy versions that pertains to numerical stability. Are you sure that when you do `np.ones(...)` you want them to be floats, not ints?\nThough that's not going to be the problem for numpy 1.10 :|\n@jnothman i did np.ones() float because I doubted that maybe the division was somehow becoming an integer division i.e. 9/2=4 and not 4.5 . Maybe because of that (though it was quite illogical) but later even after making dtype=float the same errors are repeating, hence that is not the source of problem.\r\n\r\nProbably i should move 1 step at a time. That would easily locate the source of error.\nas long as `__future__.division` is imported, that should not be an issue.\nI'll see if I have a moment to try replicate the test failures.\n\nYes, downgrading numpy to 1.10.4 is sufficient to trigger the errors. I've\nnot investigated the cause.\n\n@jnothman can you please review this?\n                                                                                      Yes but I considered those changes as minimal since that this is one estimator and mainly tests.\n@jnothman @glemaitre can you please review the code now. i have made all the changes according to your previous review.\n>  I'll make them generalised i.e. if all n_samples_seen are equal, it will return a scalar instead of a vector.\r\n\r\n@pinakinathc ok. ping me when you addressed all the points to be reviewed.",
  "created_at": "2018-06-05T16:24:00Z",
  "version": "0.20",
  "FAIL_TO_PASS": "[\"sklearn/preprocessing/tests/test_common.py::test_missing_value_handling[est1-scale-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[asarray-True-True]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[asarray-True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[asarray-False-True]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[asarray-False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[csc_matrix-True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[csc_matrix-False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[csr_matrix-True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[csr_matrix-False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scale_input_finiteness_validation\", \"sklearn/utils/tests/test_extmath.py::test_incremental_variance_update_formulas\", \"sklearn/utils/tests/test_extmath.py::test_incremental_mean_and_variance_ignore_nan\", \"sklearn/utils/tests/test_extmath.py::test_incremental_variance_numerical_stability\", \"sklearn/utils/tests/test_extmath.py::test_incremental_variance_ddof\"]",
  "PASS_TO_PASS": "[\"sklearn/preprocessing/tests/test_common.py::test_missing_value_handling[est0-minmax_scale-False]\", \"sklearn/preprocessing/tests/test_common.py::test_missing_value_handling[est3-quantile_transform-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_feature_names\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_sparse_X[1-True-False-int]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_sparse_X[2-True-False-int]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_sparse_X[2-True-False-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_sparse_X[2-True-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_sparse_X[3-False-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_sparse_X[3-False-True-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_1d\", \"sklearn/preprocessing/tests/test_data.py::test_scale_1d\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_numerical_stability\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_2d_arrays\", \"sklearn/preprocessing/tests/test_data.py::test_handle_zeros_in_scale\", \"sklearn/preprocessing/tests/test_data.py::test_minmax_scaler_partial_fit\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_partial_fit\", \"sklearn/preprocessing/tests/test_data.py::test_partial_fit_sparse_input\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_trasform_with_partial_fit\", \"sklearn/preprocessing/tests/test_data.py::test_min_max_scaler_iris\", \"sklearn/preprocessing/tests/test_data.py::test_min_max_scaler_zero_variance_features\", \"sklearn/preprocessing/tests/test_data.py::test_minmax_scale_axis1\", \"sklearn/preprocessing/tests/test_data.py::test_min_max_scaler_1d\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_without_centering\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_return_identity\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_int\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_without_copy\", \"sklearn/preprocessing/tests/test_data.py::test_scale_sparse_with_mean_raise_exception\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_2d_arrays\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_transform_one_row_csr\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_iris\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_iris_quantiles\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_iris\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_check_error\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_sparse_ignore_zeros\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_dense_toy\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_subsampling\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_sparse_toy\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_axis1\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_bounds\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_and_inverse\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_nan\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_invalid_range\", \"sklearn/preprocessing/tests/test_data.py::test_scale_function_without_centering\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scale_axis1\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scale_1d_array\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_zero_variance_features\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_zero_variance_features\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_large_negative_value\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_transform_one_row_csr\", \"sklearn/preprocessing/tests/test_data.py::test_warning_scaling_integers\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_1d\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_partial_fit\", \"sklearn/preprocessing/tests/test_data.py::test_normalizer_l1\", \"sklearn/preprocessing/tests/test_data.py::test_normalizer_l2\", \"sklearn/preprocessing/tests/test_data.py::test_normalizer_max\", \"sklearn/preprocessing/tests/test_data.py::test_normalize\", \"sklearn/preprocessing/tests/test_data.py::test_binarizer\", \"sklearn/preprocessing/tests/test_data.py::test_center_kernel\", \"sklearn/preprocessing/tests/test_data.py::test_cv_pipeline_precomputed\", \"sklearn/preprocessing/tests/test_data.py::test_fit_transform\", \"sklearn/preprocessing/tests/test_data.py::test_add_dummy_feature\", \"sklearn/preprocessing/tests/test_data.py::test_add_dummy_feature_coo\", \"sklearn/preprocessing/tests/test_data.py::test_add_dummy_feature_csc\", \"sklearn/preprocessing/tests/test_data.py::test_add_dummy_feature_csr\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_sparse\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_dense\", \"sklearn/preprocessing/tests/test_data.py::test_transform_selected[int32-int32]\", \"sklearn/preprocessing/tests/test_data.py::test_transform_selected[int32-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_transform_selected[int32-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_transform_selected[float32-int32]\", \"sklearn/preprocessing/tests/test_data.py::test_transform_selected[float32-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_transform_selected[float32-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_transform_selected[float64-int32]\", \"sklearn/preprocessing/tests/test_data.py::test_transform_selected[float64-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_transform_selected[float64-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_transform_selected_copy_arg[int32-int32]\", \"sklearn/preprocessing/tests/test_data.py::test_transform_selected_copy_arg[int32-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_transform_selected_copy_arg[int32-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_transform_selected_copy_arg[float32-int32]\", \"sklearn/preprocessing/tests/test_data.py::test_transform_selected_copy_arg[float32-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_transform_selected_copy_arg[float32-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_transform_selected_copy_arg[float64-int32]\", \"sklearn/preprocessing/tests/test_data.py::test_transform_selected_copy_arg[float64-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_transform_selected_copy_arg[float64-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_categorical_features\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_preserve_type[True-int32-int32]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_preserve_type[True-int32-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_preserve_type[True-int32-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_preserve_type[True-float32-int32]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_preserve_type[True-float32-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_preserve_type[True-float32-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_preserve_type[True-float64-int32]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_preserve_type[True-float64-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_preserve_type[True-float64-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_preserve_type[False-int32-int32]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_preserve_type[False-int32-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_preserve_type[False-int32-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_preserve_type[False-float32-int32]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_preserve_type[False-float32-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_preserve_type[False-float32-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_preserve_type[False-float64-int32]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_preserve_type[False-float64-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_preserve_type[False-float64-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_one_hot_encoder_unknown_transform\", \"sklearn/preprocessing/tests/test_data.py::test_categorical_encoder_onehot\", \"sklearn/preprocessing/tests/test_data.py::test_categorical_encoder_onehot_inverse\", \"sklearn/preprocessing/tests/test_data.py::test_categorical_encoder_handle_unknown\", \"sklearn/preprocessing/tests/test_data.py::test_categorical_encoder_categories\", \"sklearn/preprocessing/tests/test_data.py::test_categorical_encoder_specified_categories\", \"sklearn/preprocessing/tests/test_data.py::test_categorical_encoder_pandas\", \"sklearn/preprocessing/tests/test_data.py::test_categorical_encoder_ordinal\", \"sklearn/preprocessing/tests/test_data.py::test_categorical_encoder_ordinal_inverse\", \"sklearn/preprocessing/tests/test_data.py::test_categorical_encoder_dtypes\", \"sklearn/preprocessing/tests/test_data.py::test_categorical_encoder_dtypes_pandas\", \"sklearn/preprocessing/tests/test_data.py::test_categorical_encoder_warning\", \"sklearn/preprocessing/tests/test_data.py::test_fit_cold_start\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_valid_axis\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_notfitted\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_1d\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_2d\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_strictly_positive_exception\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_shape_exception\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_method_exception\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_lambda_zero\", \"sklearn/utils/tests/test_extmath.py::test_density\", \"sklearn/utils/tests/test_extmath.py::test_uniform_weights\", \"sklearn/utils/tests/test_extmath.py::test_random_weights\", \"sklearn/utils/tests/test_extmath.py::test_logsumexp\", \"sklearn/utils/tests/test_extmath.py::test_randomized_svd_low_rank_all_dtypes[int32]\", \"sklearn/utils/tests/test_extmath.py::test_randomized_svd_low_rank_all_dtypes[int64]\", \"sklearn/utils/tests/test_extmath.py::test_randomized_svd_low_rank_all_dtypes[float32]\", \"sklearn/utils/tests/test_extmath.py::test_randomized_svd_low_rank_all_dtypes[float64]\", \"sklearn/utils/tests/test_extmath.py::test_norm_squared_norm\", \"sklearn/utils/tests/test_extmath.py::test_row_norms[float32]\", \"sklearn/utils/tests/test_extmath.py::test_row_norms[float64]\", \"sklearn/utils/tests/test_extmath.py::test_randomized_svd_low_rank_with_noise\", \"sklearn/utils/tests/test_extmath.py::test_randomized_svd_infinite_rank\", \"sklearn/utils/tests/test_extmath.py::test_randomized_svd_transpose_consistency\", \"sklearn/utils/tests/test_extmath.py::test_randomized_svd_power_iteration_normalizer\", \"sklearn/utils/tests/test_extmath.py::test_svd_flip\", \"sklearn/utils/tests/test_extmath.py::test_randomized_svd_sign_flip\", \"sklearn/utils/tests/test_extmath.py::test_randomized_svd_sign_flip_with_transpose\", \"sklearn/utils/tests/test_extmath.py::test_cartesian\", \"sklearn/utils/tests/test_extmath.py::test_logistic_sigmoid\", \"sklearn/utils/tests/test_extmath.py::test_vector_sign_flip\", \"sklearn/utils/tests/test_extmath.py::test_softmax\", \"sklearn/utils/tests/test_extmath.py::test_stable_cumsum\", \"sklearn/utils/tests/test_sparsefuncs.py::test_mean_variance_axis0\", \"sklearn/utils/tests/test_sparsefuncs.py::test_mean_variance_axis1\", \"sklearn/utils/tests/test_sparsefuncs.py::test_mean_variance_illegal_axis\", \"sklearn/utils/tests/test_sparsefuncs.py::test_densify_rows\", \"sklearn/utils/tests/test_sparsefuncs.py::test_inplace_column_scale\", \"sklearn/utils/tests/test_sparsefuncs.py::test_inplace_row_scale\", \"sklearn/utils/tests/test_sparsefuncs.py::test_inplace_swap_row\", \"sklearn/utils/tests/test_sparsefuncs.py::test_inplace_swap_column\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[0-amin-amax-False-csr_matrix-0-float32]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[0-amin-amax-False-csr_matrix-0-float64]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[0-amin-amax-False-csr_matrix-1-float32]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[0-amin-amax-False-csr_matrix-1-float64]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[0-amin-amax-False-csr_matrix-None-float32]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[0-amin-amax-False-csr_matrix-None-float64]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[0-amin-amax-False-csc_matrix-0-float32]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[0-amin-amax-False-csc_matrix-0-float64]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[0-amin-amax-False-csc_matrix-1-float32]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[0-amin-amax-False-csc_matrix-1-float64]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[0-amin-amax-False-csc_matrix-None-float32]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[0-amin-amax-False-csc_matrix-None-float64]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[nan-nanmin-nanmax-True-csr_matrix-0-float32]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[nan-nanmin-nanmax-True-csr_matrix-0-float64]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[nan-nanmin-nanmax-True-csr_matrix-1-float32]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[nan-nanmin-nanmax-True-csr_matrix-1-float64]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[nan-nanmin-nanmax-True-csr_matrix-None-float32]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[nan-nanmin-nanmax-True-csr_matrix-None-float64]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[nan-nanmin-nanmax-True-csc_matrix-0-float32]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[nan-nanmin-nanmax-True-csc_matrix-0-float64]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[nan-nanmin-nanmax-True-csc_matrix-1-float32]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[nan-nanmin-nanmax-True-csc_matrix-1-float64]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[nan-nanmin-nanmax-True-csc_matrix-None-float32]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max[nan-nanmin-nanmax-True-csc_matrix-None-float64]\", \"sklearn/utils/tests/test_sparsefuncs.py::test_min_max_axis_errors\", \"sklearn/utils/tests/test_sparsefuncs.py::test_count_nonzero\", \"sklearn/utils/tests/test_sparsefuncs.py::test_csc_row_median\", \"sklearn/utils/tests/test_sparsefuncs.py::test_inplace_normalize\"]",
  "environment_setup_commit": "55bf5d93e5674f13a1134d93a11fd0cd11aabcd1",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.959434",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}