{
  "repo": "sympy/sympy",
  "instance_id": "sympy__sympy-14085",
  "base_commit": "b95ffadae8cfad3acf15fada28140373c896b766",
  "patch": "diff --git a/sympy/parsing/sympy_parser.py b/sympy/parsing/sympy_parser.py\n--- a/sympy/parsing/sympy_parser.py\n+++ b/sympy/parsing/sympy_parser.py\n@@ -2,22 +2,18 @@\n \n from __future__ import print_function, division\n \n-from .sympy_tokenize import \\\n-    generate_tokens, untokenize, TokenError, \\\n-    NUMBER, STRING, NAME, OP, ENDMARKER\n+from tokenize import (generate_tokens, untokenize, TokenError,\n+    NUMBER, STRING, NAME, OP, ENDMARKER, ERRORTOKEN)\n \n from keyword import iskeyword\n \n import ast\n-import re\n import unicodedata\n \n import sympy\n from sympy.core.compatibility import exec_, StringIO\n from sympy.core.basic import Basic\n \n-_re_repeated = re.compile(r\"^(\\d*)\\.(\\d*)\\[(\\d+)\\]$\")\n-\n def _token_splittable(token):\n     \"\"\"\n     Predicate for whether a token name can be split into multiple tokens.\n@@ -589,26 +585,24 @@ def lambda_notation(tokens, local_dict, global_dict):\n def factorial_notation(tokens, local_dict, global_dict):\n     \"\"\"Allows standard notation for factorial.\"\"\"\n     result = []\n-    prevtoken = ''\n+    nfactorial = 0\n     for toknum, tokval in tokens:\n-        if toknum == OP:\n+        if toknum == ERRORTOKEN:\n             op = tokval\n-\n-            if op == '!!':\n-                if prevtoken == '!' or prevtoken == '!!':\n-                    raise TokenError\n-                result = _add_factorial_tokens('factorial2', result)\n-            elif op == '!':\n-                if prevtoken == '!' or prevtoken == '!!':\n-                    raise TokenError\n-                result = _add_factorial_tokens('factorial', result)\n+            if op == '!':\n+                nfactorial += 1\n             else:\n+                nfactorial = 0\n                 result.append((OP, op))\n         else:\n+            if nfactorial == 1:\n+                result = _add_factorial_tokens('factorial', result)\n+            elif nfactorial == 2:\n+                result = _add_factorial_tokens('factorial2', result)\n+            elif nfactorial > 2:\n+                raise TokenError\n+            nfactorial = 0\n             result.append((toknum, tokval))\n-\n-        prevtoken = tokval\n-\n     return result\n \n \n@@ -626,16 +620,105 @@ def convert_xor(tokens, local_dict, global_dict):\n \n     return result\n \n+def repeated_decimals(tokens, local_dict, global_dict):\n+    \"\"\"\n+    Allows 0.2[1] notation to represent the repeated decimal 0.2111... (19/90)\n+\n+    Run this before auto_number.\n+\n+    \"\"\"\n+    result = []\n+\n+    def is_digit(s):\n+        return all(i in '0123456789_' for i in s)\n+\n+    # num will running match any DECIMAL [ INTEGER ]\n+    num = []\n+    for toknum, tokval in tokens:\n+        if toknum == NUMBER:\n+            if (not num and '.' in tokval and 'e' not in tokval.lower() and\n+                'j' not in tokval.lower()):\n+                num.append((toknum, tokval))\n+            elif is_digit(tokval)and  len(num) == 2:\n+                num.append((toknum, tokval))\n+            elif is_digit(tokval) and len(num) == 3 and is_digit(num[-1][1]):\n+                # Python 2 tokenizes 00123 as '00', '123'\n+                # Python 3 tokenizes 01289 as '012', '89'\n+                num.append((toknum, tokval))\n+            else:\n+                num = []\n+        elif toknum == OP:\n+            if tokval == '[' and len(num) == 1:\n+                num.append((OP, tokval))\n+            elif tokval == ']' and len(num) >= 3:\n+                num.append((OP, tokval))\n+            elif tokval == '.' and not num:\n+                # handle .[1]\n+                num.append((NUMBER, '0.'))\n+            else:\n+                num = []\n+        else:\n+            num = []\n+\n+        result.append((toknum, tokval))\n+\n+        if num and num[-1][1] == ']':\n+            # pre.post[repetend] = a + b/c + d/e where a = pre, b/c = post,\n+            # and d/e = repetend\n+            result = result[:-len(num)]\n+            pre, post = num[0][1].split('.')\n+            repetend = num[2][1]\n+            if len(num) == 5:\n+                repetend += num[3][1]\n+\n+            pre = pre.replace('_', '')\n+            post = post.replace('_', '')\n+            repetend = repetend.replace('_', '')\n+\n+            zeros = '0'*len(post)\n+            post, repetends = [w.lstrip('0') for w in [post, repetend]]\n+                                        # or else interpreted as octal\n+\n+            a = pre or '0'\n+            b, c = post or '0', '1' + zeros\n+            d, e = repetends, ('9'*len(repetend)) + zeros\n+\n+            seq = [\n+                (OP, '('),\n+                    (NAME, 'Integer'),\n+                    (OP, '('),\n+                        (NUMBER, a),\n+                    (OP, ')'),\n+                    (OP, '+'),\n+                    (NAME, 'Rational'),\n+                    (OP, '('),\n+                        (NUMBER, b),\n+                        (OP, ','),\n+                        (NUMBER, c),\n+                    (OP, ')'),\n+                    (OP, '+'),\n+                    (NAME, 'Rational'),\n+                    (OP, '('),\n+                        (NUMBER, d),\n+                        (OP, ','),\n+                        (NUMBER, e),\n+                    (OP, ')'),\n+                (OP, ')'),\n+            ]\n+            result.extend(seq)\n+            num = []\n+\n+    return result\n \n def auto_number(tokens, local_dict, global_dict):\n-    \"\"\"Converts numeric literals to use SymPy equivalents.\n+    \"\"\"\n+    Converts numeric literals to use SymPy equivalents.\n \n-    Complex numbers use ``I``; integer literals use ``Integer``, float\n-    literals use ``Float``, and repeating decimals use ``Rational``.\n+    Complex numbers use ``I``, integer literals use ``Integer``, and float\n+    literals use ``Float``.\n \n     \"\"\"\n     result = []\n-    prevtoken = ''\n \n     for toknum, tokval in tokens:\n         if toknum == NUMBER:\n@@ -648,35 +731,8 @@ def auto_number(tokens, local_dict, global_dict):\n \n             if '.' in number or (('e' in number or 'E' in number) and\n                     not (number.startswith('0x') or number.startswith('0X'))):\n-                match = _re_repeated.match(number)\n-\n-                if match is not None:\n-                    # Clear repeating decimals, e.g. 3.4[31] -> (3 + 4/10 + 31/990)\n-                    pre, post, repetend = match.groups()\n-\n-                    zeros = '0'*len(post)\n-                    post, repetends = [w.lstrip('0') for w in [post, repetend]]\n-                                                # or else interpreted as octal\n-\n-                    a = pre or '0'\n-                    b, c = post or '0', '1' + zeros\n-                    d, e = repetends, ('9'*len(repetend)) + zeros\n-\n-                    seq = [\n-                        (OP, '('),\n-                        (NAME,\n-                         'Integer'), (OP, '('), (NUMBER, a), (OP, ')'),\n-                        (OP, '+'),\n-                        (NAME, 'Rational'), (OP, '('), (\n-                            NUMBER, b), (OP, ','), (NUMBER, c), (OP, ')'),\n-                        (OP, '+'),\n-                        (NAME, 'Rational'), (OP, '('), (\n-                            NUMBER, d), (OP, ','), (NUMBER, e), (OP, ')'),\n-                        (OP, ')'),\n-                    ]\n-                else:\n-                    seq = [(NAME, 'Float'), (OP, '('),\n-                           (NUMBER, repr(str(number))), (OP, ')')]\n+                seq = [(NAME, 'Float'), (OP, '('),\n+                    (NUMBER, repr(str(number))), (OP, ')')]\n             else:\n                 seq = [(NAME, 'Integer'), (OP, '('), (\n                     NUMBER, number), (OP, ')')]\n@@ -687,7 +743,6 @@ def auto_number(tokens, local_dict, global_dict):\n \n     return result\n \n-\n def rationalize(tokens, local_dict, global_dict):\n     \"\"\"Converts floats into ``Rational``. Run AFTER ``auto_number``.\"\"\"\n     result = []\n@@ -776,7 +831,8 @@ def convert_equals_signs(result, local_dict, global_dict):\n #: Standard transformations for :func:`parse_expr`.\n #: Inserts calls to :class:`Symbol`, :class:`Integer`, and other SymPy\n #: datatypes and allows the use of standard factorial notation (e.g. ``x!``).\n-standard_transformations = (lambda_notation, auto_symbol, auto_number, factorial_notation)\n+standard_transformations = (lambda_notation, auto_symbol, repeated_decimals, auto_number,\n+    factorial_notation)\n \n \n def stringify_expr(s, local_dict, global_dict, transformations):\ndiff --git a/sympy/parsing/sympy_tokenize.py b/sympy/parsing/sympy_tokenize.py\ndeleted file mode 100644\n--- a/sympy/parsing/sympy_tokenize.py\n+++ /dev/null\n@@ -1,451 +0,0 @@\n-\"\"\"Tokenization help for Python programs.\n-\n-generate_tokens(readline) is a generator that breaks a stream of\n-text into Python tokens.  It accepts a readline-like method which is called\n-repeatedly to get the next line of input (or \"\" for EOF).  It generates\n-5-tuples with these members:\n-\n-    the token type (see token.py)\n-    the token (a string)\n-    the starting (row, column) indices of the token (a 2-tuple of ints)\n-    the ending (row, column) indices of the token (a 2-tuple of ints)\n-    the original line (string)\n-\n-It is designed to match the working of the Python tokenizer exactly, except\n-that it produces COMMENT tokens for comments and gives type OP for all\n-operators\n-\n-Older entry points\n-    tokenize_loop(readline, tokeneater)\n-    tokenize(readline, tokeneater=printtoken)\n-are the same, except instead of generating tokens, tokeneater is a callback\n-function to which the 5 fields described above are passed as 5 arguments,\n-each time a new token is found.\"\"\"\n-\n-from __future__ import print_function, division\n-\n-__author__ = 'Ka-Ping Yee <ping@lfw.org>'\n-__credits__ = \\\n-    'GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro, Raymond Hettinger'\n-\n-import string\n-import re\n-from token import *\n-\n-import token\n-__all__ = [x for x in dir(token) if x[0] != '_'] + [\"COMMENT\", \"tokenize\",\n-           \"generate_tokens\", \"NL\", \"untokenize\"]\n-del token\n-\n-COMMENT = N_TOKENS\n-tok_name[COMMENT] = 'COMMENT'\n-NL = N_TOKENS + 1\n-tok_name[NL] = 'NL'\n-N_TOKENS += 2\n-\n-\n-def group(*choices):\n-    return '(' + '|'.join(choices) + ')'\n-\n-\n-def any(*choices):\n-    return group(*choices) + '*'\n-\n-\n-def maybe(*choices):\n-    return group(*choices) + '?'\n-\n-Whitespace = r'[ \\f\\t]*'\n-Comment = r'#[^\\r\\n]*'\n-Ignore = Whitespace + any(r'\\\\\\r?\\n' + Whitespace) + maybe(Comment)\n-Name = r'[a-zA-Z_]\\w*'\n-\n-Hexnumber = r'0[xX][\\da-fA-F]+[lL]?'\n-Octnumber = r'(0[oO][0-7]+)|(0[0-7]*)[lL]?'\n-Binnumber = r'0[bB][01]+[lL]?'\n-Decnumber = r'[1-9]\\d*[lL]?'\n-Intnumber = group(Hexnumber, Binnumber, Octnumber, Decnumber)\n-Exponent = r'[eE][-+]?\\d+'\n-Pointfloat = group(r'\\d+\\.\\d*', r'\\.\\d+') + maybe(Exponent)\n-Repeatedfloat = r'\\d*\\.\\d*\\[\\d+\\]'\n-Expfloat = r'\\d+' + Exponent\n-Floatnumber = group(Repeatedfloat, Pointfloat, Expfloat)\n-Imagnumber = group(r'\\d+[jJ]', Floatnumber + r'[jJ]')\n-Number = group(Imagnumber, Floatnumber, Intnumber)\n-\n-# Tail end of ' string.\n-Single = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\n-# Tail end of \" string.\n-Double = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\n-# Tail end of ''' string.\n-Single3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\n-# Tail end of \"\"\" string.\n-Double3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\n-Triple = group(\"[uU]?[rR]?'''\", '[uU]?[rR]?\"\"\"')\n-# Single-line ' or \" string.\n-String = group(r\"[uU]?[rR]?'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n-               r'[uU]?[rR]?\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"')\n-\n-# Because of leftmost-then-longest match semantics, be sure to put the\n-# longest operators first (e.g., if = came before ==, == would get\n-# recognized as two instances of =).\n-Operator = group(r\"\\*\\*=?\", r\">>=?\", r\"<<=?\", r\"<>\", r\"!=\",\n-                 r\"//=?\",\n-                 r\"[+\\-*/%&|^=<>]=?\",\n-                 r\"~\")\n-\n-Bracket = '[][(){}]'\n-Special = group(r'\\r?\\n', r'[:;.,`@]', r'\\!\\!', r'\\!')\n-Funny = group(Operator, Bracket, Special)\n-\n-PlainToken = group(Number, Funny, String, Name)\n-Token = Ignore + PlainToken\n-\n-# First (or only) line of ' or \" string.\n-ContStr = group(r\"[uU]?[rR]?'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" +\n-                group(\"'\", r'\\\\\\r?\\n'),\n-                r'[uU]?[rR]?\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' +\n-                group('\"', r'\\\\\\r?\\n'))\n-PseudoExtras = group(r'\\\\\\r?\\n', Comment, Triple)\n-PseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n-\n-tokenprog, pseudoprog, single3prog, double3prog = map(\n-    re.compile, (Token, PseudoToken, Single3, Double3))\n-endprogs = {\"'\": re.compile(Single), '\"': re.compile(Double),\n-            \"'''\": single3prog, '\"\"\"': double3prog,\n-            \"r'''\": single3prog, 'r\"\"\"': double3prog,\n-            \"u'''\": single3prog, 'u\"\"\"': double3prog,\n-            \"ur'''\": single3prog, 'ur\"\"\"': double3prog,\n-            \"R'''\": single3prog, 'R\"\"\"': double3prog,\n-            \"U'''\": single3prog, 'U\"\"\"': double3prog,\n-            \"uR'''\": single3prog, 'uR\"\"\"': double3prog,\n-            \"Ur'''\": single3prog, 'Ur\"\"\"': double3prog,\n-            \"UR'''\": single3prog, 'UR\"\"\"': double3prog,\n-            \"b'''\": single3prog, 'b\"\"\"': double3prog,\n-            \"br'''\": single3prog, 'br\"\"\"': double3prog,\n-            \"B'''\": single3prog, 'B\"\"\"': double3prog,\n-            \"bR'''\": single3prog, 'bR\"\"\"': double3prog,\n-            \"Br'''\": single3prog, 'Br\"\"\"': double3prog,\n-            \"BR'''\": single3prog, 'BR\"\"\"': double3prog,\n-            'r': None, 'R': None, 'u': None, 'U': None,\n-            'b': None, 'B': None}\n-\n-triple_quoted = {}\n-for t in (\"'''\", '\"\"\"',\n-          \"r'''\", 'r\"\"\"', \"R'''\", 'R\"\"\"',\n-          \"u'''\", 'u\"\"\"', \"U'''\", 'U\"\"\"',\n-          \"ur'''\", 'ur\"\"\"', \"Ur'''\", 'Ur\"\"\"',\n-          \"uR'''\", 'uR\"\"\"', \"UR'''\", 'UR\"\"\"',\n-          \"b'''\", 'b\"\"\"', \"B'''\", 'B\"\"\"',\n-          \"br'''\", 'br\"\"\"', \"Br'''\", 'Br\"\"\"',\n-          \"bR'''\", 'bR\"\"\"', \"BR'''\", 'BR\"\"\"'):\n-    triple_quoted[t] = t\n-single_quoted = {}\n-for t in (\"'\", '\"',\n-          \"r'\", 'r\"', \"R'\", 'R\"',\n-          \"u'\", 'u\"', \"U'\", 'U\"',\n-          \"ur'\", 'ur\"', \"Ur'\", 'Ur\"',\n-          \"uR'\", 'uR\"', \"UR'\", 'UR\"',\n-          \"b'\", 'b\"', \"B'\", 'B\"',\n-          \"br'\", 'br\"', \"Br'\", 'Br\"',\n-          \"bR'\", 'bR\"', \"BR'\", 'BR\"' ):\n-    single_quoted[t] = t\n-\n-tabsize = 8\n-\n-\n-class TokenError(Exception):\n-    pass\n-\n-\n-class StopTokenizing(Exception):\n-    pass\n-\n-\n-def printtoken(type, token, srow_scol, erow_ecol, line):  # for testing\n-    srow, scol = srow_scol\n-    erow, ecol = erow_ecol\n-    print(\"%d,%d-%d,%d:\\t%s\\t%s\" % \\\n-        (srow, scol, erow, ecol, tok_name[type], repr(token)))\n-\n-\n-def tokenize(readline, tokeneater=printtoken):\n-    \"\"\"\n-    The tokenize() function accepts two parameters: one representing the\n-    input stream, and one providing an output mechanism for tokenize().\n-\n-    The first parameter, readline, must be a callable object which provides\n-    the same interface as the readline() method of built-in file objects.\n-    Each call to the function should return one line of input as a string.\n-\n-    The second parameter, tokeneater, must also be a callable object. It is\n-    called once for each token, with five arguments, corresponding to the\n-    tuples generated by generate_tokens().\n-    \"\"\"\n-    try:\n-        tokenize_loop(readline, tokeneater)\n-    except StopTokenizing:\n-        pass\n-\n-# backwards compatible interface\n-\n-\n-def tokenize_loop(readline, tokeneater):\n-    for token_info in generate_tokens(readline):\n-        tokeneater(*token_info)\n-\n-\n-class Untokenizer:\n-\n-    def __init__(self):\n-        self.tokens = []\n-        self.prev_row = 1\n-        self.prev_col = 0\n-\n-    def add_whitespace(self, start):\n-        row, col = start\n-        if row > self.prev_row:\n-            raise ValueError(\"row should not be greater than prev_row\")\n-        col_offset = col - self.prev_col\n-        if col_offset:\n-            self.tokens.append(\" \" * col_offset)\n-\n-    def untokenize(self, iterable):\n-        for t in iterable:\n-            if len(t) == 2:\n-                self.compat(t, iterable)\n-                break\n-            tok_type, token, start, end, line = t\n-            self.add_whitespace(start)\n-            self.tokens.append(token)\n-            self.prev_row, self.prev_col = end\n-            if tok_type in (NEWLINE, NL):\n-                self.prev_row += 1\n-                self.prev_col = 0\n-        return \"\".join(self.tokens)\n-\n-    def compat(self, token, iterable):\n-        startline = False\n-        indents = []\n-        toks_append = self.tokens.append\n-        toknum, tokval = token\n-        if toknum in (NAME, NUMBER):\n-            tokval += ' '\n-        if toknum in (NEWLINE, NL):\n-            startline = True\n-        prevstring = False\n-        for tok in iterable:\n-            toknum, tokval = tok[:2]\n-\n-            if toknum in (NAME, NUMBER):\n-                tokval += ' '\n-\n-            # Insert a space between two consecutive strings\n-            if toknum == STRING:\n-                if prevstring:\n-                    tokval = ' ' + tokval\n-                prevstring = True\n-            else:\n-                prevstring = False\n-\n-            if toknum == INDENT:\n-                indents.append(tokval)\n-                continue\n-            elif toknum == DEDENT:\n-                indents.pop()\n-                continue\n-            elif toknum in (NEWLINE, NL):\n-                startline = True\n-            elif startline and indents:\n-                toks_append(indents[-1])\n-                startline = False\n-            toks_append(tokval)\n-\n-\n-def untokenize(iterable):\n-    \"\"\"Transform tokens back into Python source code.\n-\n-    Each element returned by the iterable must be a token sequence\n-    with at least two elements, a token number and token value.  If\n-    only two tokens are passed, the resulting output is poor.\n-\n-    Round-trip invariant for full input:\n-        Untokenized source will match input source exactly\n-\n-    Round-trip invariant for limited intput::\n-\n-        # Output text will tokenize the back to the input\n-        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n-        newcode = untokenize(t1)\n-        readline = iter(newcode.splitlines(1)).next\n-        t2 = [tok[:2] for tok in generate_tokens(readline)]\n-        if t1 != t2:\n-            raise ValueError(\"t1 should be equal to t2\")\n-    \"\"\"\n-    ut = Untokenizer()\n-    return ut.untokenize(iterable)\n-\n-\n-def generate_tokens(readline):\n-    \"\"\"\n-    The generate_tokens() generator requires one argument, readline, which\n-    must be a callable object which provides the same interface as the\n-    readline() method of built-in file objects. Each call to the function\n-    should return one line of input as a string.  Alternately, readline\n-    can be a callable function terminating with StopIteration::\n-\n-        readline = open(myfile).next    # Example of alternate readline\n-\n-    The generator produces 5-tuples with these members: the token type; the\n-    token string; a 2-tuple (srow, scol) of ints specifying the row and\n-    column where the token begins in the source; a 2-tuple (erow, ecol) of\n-    ints specifying the row and column where the token ends in the source;\n-    and the line on which the token was found. The line passed is the\n-    logical line; continuation lines are included.\n-    \"\"\"\n-    lnum = parenlev = continued = 0\n-    namechars, numchars = string.ascii_letters + '_', '0123456789'\n-    contstr, needcont = '', 0\n-    contline = None\n-    indents = [0]\n-\n-    while 1:                                   # loop over lines in stream\n-        try:\n-            line = readline()\n-        except StopIteration:\n-            line = ''\n-        lnum = lnum + 1\n-        pos, max = 0, len(line)\n-\n-        if contstr:                            # continued string\n-            if not line:\n-                raise TokenError(\"EOF in multi-line string\", strstart)\n-            endmatch = endprog.match(line)\n-            if endmatch:\n-                pos = end = endmatch.end(0)\n-                yield (STRING, contstr + line[:end],\n-                       strstart, (lnum, end), contline + line)\n-                contstr, needcont = '', 0\n-                contline = None\n-            elif needcont and line[-2:] != '\\\\\\n' and line[-3:] != '\\\\\\r\\n':\n-                yield (ERRORTOKEN, contstr + line,\n-                       strstart, (lnum, len(line)), contline)\n-                contstr = ''\n-                contline = None\n-                continue\n-            else:\n-                contstr = contstr + line\n-                contline = contline + line\n-                continue\n-\n-        elif parenlev == 0 and not continued:  # new statement\n-            if not line:\n-                break\n-            column = 0\n-            while pos < max:                   # measure leading whitespace\n-                if line[pos] == ' ':\n-                    column = column + 1\n-                elif line[pos] == '\\t':\n-                    column = (column/tabsize + 1)*tabsize\n-                elif line[pos] == '\\f':\n-                    column = 0\n-                else:\n-                    break\n-                pos = pos + 1\n-            if pos == max:\n-                break\n-\n-            if line[pos] in '#\\r\\n':           # skip comments or blank lines\n-                if line[pos] == '#':\n-                    comment_token = line[pos:].rstrip('\\r\\n')\n-                    nl_pos = pos + len(comment_token)\n-                    yield (COMMENT, comment_token,\n-                           (lnum, pos), (lnum, pos + len(comment_token)), line)\n-                    yield (NL, line[nl_pos:],\n-                           (lnum, nl_pos), (lnum, len(line)), line)\n-                else:\n-                    yield ((NL, COMMENT)[line[pos] == '#'], line[pos:],\n-                           (lnum, pos), (lnum, len(line)), line)\n-                continue\n-\n-            if column > indents[-1]:           # count indents or dedents\n-                indents.append(column)\n-                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n-            while column < indents[-1]:\n-                if column not in indents:\n-                    raise IndentationError(\n-                        \"unindent does not match any outer indentation level\",\n-                        (\"<tokenize>\", lnum, pos, line))\n-                indents = indents[:-1]\n-                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)\n-\n-        else:                                  # continued statement\n-            if not line:\n-                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n-            continued = 0\n-\n-        while pos < max:\n-            pseudomatch = pseudoprog.match(line, pos)\n-            if pseudomatch:                                # scan for tokens\n-                start, end = pseudomatch.span(1)\n-                spos, epos, pos = (lnum, start), (lnum, end), end\n-                token, initial = line[start:end], line[start]\n-\n-                if initial in numchars or \\\n-                        (initial == '.' and token != '.'):      # ordinary number\n-                    yield (NUMBER, token, spos, epos, line)\n-                elif initial in '\\r\\n':\n-                    yield (NL if parenlev > 0 else NEWLINE, token, spos, epos, line)\n-                elif initial == '#':\n-                    if token.endswith(\"\\n\"):\n-                        raise ValueError(\"Token should not end with \\n\")\n-                    yield (COMMENT, token, spos, epos, line)\n-                elif token in triple_quoted:\n-                    endprog = endprogs[token]\n-                    endmatch = endprog.match(line, pos)\n-                    if endmatch:                           # all on one line\n-                        pos = endmatch.end(0)\n-                        token = line[start:pos]\n-                        yield (STRING, token, spos, (lnum, pos), line)\n-                    else:\n-                        strstart = (lnum, start)           # multiple lines\n-                        contstr = line[start:]\n-                        contline = line\n-                        break\n-                elif initial in single_quoted or \\\n-                    token[:2] in single_quoted or \\\n-                        token[:3] in single_quoted:\n-                    if token[-1] == '\\n':                  # continued string\n-                        strstart = (lnum, start)\n-                        endprog = (endprogs[initial] or endprogs[token[1]] or\n-                                   endprogs[token[2]])\n-                        contstr, needcont = line[start:], 1\n-                        contline = line\n-                        break\n-                    else:                                  # ordinary string\n-                        yield (STRING, token, spos, epos, line)\n-                elif initial in namechars:                 # ordinary name\n-                    yield (NAME, token, spos, epos, line)\n-                elif initial == '\\\\':                      # continued stmt\n-                    continued = 1\n-                else:\n-                    if initial in '([{':\n-                        parenlev = parenlev + 1\n-                    elif initial in ')]}':\n-                        parenlev = parenlev - 1\n-                    yield (OP, token, spos, epos, line)\n-            else:\n-                yield (ERRORTOKEN, line[pos],\n-                       (lnum, pos), (lnum, pos + 1), line)\n-                pos = pos + 1\n-\n-    for indent in indents[1:]:                 # pop remaining indent levels\n-        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')\n-    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')\n-\n-if __name__ == '__main__':                     # testing\n-    import sys\n-    if len(sys.argv) > 1:\n-        tokenize(open(sys.argv[1]).readline)\n-    else:\n-        tokenize(sys.stdin.readline)\n",
  "test_patch": "diff --git a/sympy/parsing/tests/test_sympy_parser.py b/sympy/parsing/tests/test_sympy_parser.py\n--- a/sympy/parsing/tests/test_sympy_parser.py\n+++ b/sympy/parsing/tests/test_sympy_parser.py\n@@ -1,8 +1,11 @@\n+import sys\n+\n from sympy.core import Symbol, Function, Float, Rational, Integer, I, Mul, Pow, Eq\n-from sympy.functions import exp, factorial, sin\n+from sympy.core.compatibility import PY3\n+from sympy.functions import exp, factorial, factorial2, sin\n from sympy.logic import And\n from sympy.series import Limit\n-from sympy.utilities.pytest import raises\n+from sympy.utilities.pytest import raises, skip\n \n from sympy.parsing.sympy_parser import (\n     parse_expr, standard_transformations, rationalize, TokenError,\n@@ -19,7 +22,18 @@ def test_sympy_parser():\n         '2+3j': 2 + 3*I,\n         'exp(x)': exp(x),\n         'x!': factorial(x),\n+        'x!!': factorial2(x),\n+        '(x + 1)! - 1': factorial(x + 1) - 1,\n         '3.[3]': Rational(10, 3),\n+        '.0[3]': Rational(1, 30),\n+        '3.2[3]': Rational(97, 30),\n+        '1.3[12]': Rational(433, 330),\n+        '1 + 3.[3]': Rational(13, 3),\n+        '1 + .0[3]': Rational(31, 30),\n+        '1 + 3.2[3]': Rational(127, 30),\n+        '.[0011]': Rational(1, 909),\n+        '0.1[00102] + 1': Rational(366697, 333330),\n+        '1.[0191]': Rational(10190, 9999),\n         '10!': 3628800,\n         '-(2)': -Integer(2),\n         '[-1, -2, 3]': [Integer(-1), Integer(-2), Integer(3)],\n@@ -56,6 +70,22 @@ def test_factorial_fail():\n             assert True\n \n \n+def test_repeated_fail():\n+    inputs = ['1[1]', '.1e1[1]', '0x1[1]', '1.1j[1]', '1.1[1 + 1]',\n+        '0.1[[1]]', '0x1.1[1]']\n+\n+    # All are valid Python, so only raise TypeError for invalid indexing\n+    for text in inputs:\n+        raises(TypeError, lambda: parse_expr(text))\n+\n+    inputs = ['0.1[', '0.1[1', '0.1[]']\n+    for text in inputs:\n+        raises((TokenError, SyntaxError), lambda: parse_expr(text))\n+\n+def test_repeated_dot_only():\n+    assert parse_expr('.[1]') == Rational(1, 9)\n+    assert parse_expr('1 + .[1]') == Rational(10, 9)\n+\n def test_local_dict():\n     local_dict = {\n         'my_function': lambda x: x + 2\n@@ -142,3 +172,21 @@ def test_convert_equals_signs():\n     assert parse_expr(\"y = x\", transformations=transformations) == Eq(y, x)\n     assert parse_expr(\"(2*y = x) = False\",\n         transformations=transformations) == Eq(Eq(2*y, x), False)\n+\n+def test_unicode_names():\n+    if not PY3:\n+        skip(\"test_unicode_names can only pass in Python 3\")\n+\n+    assert parse_expr(u'α') == Symbol(u'α')\n+\n+def test_python3_features():\n+    # Make sure the tokenizer can handle Python 3-only features\n+    if sys.version_info < (3, 6):\n+        skip(\"test_python3_features requires Python 3.6 or newer\")\n+\n+    assert parse_expr(\"123_456\") == 123456\n+    assert parse_expr(\"1.2[3_4]\") == parse_expr(\"1.2[34]\") == Rational(611, 495)\n+    assert parse_expr(\"1.2[012_012]\") == parse_expr(\"1.2[012012]\") == Rational(400, 333)\n+    assert parse_expr('.[3_4]') == parse_expr('.[34]') == Rational(34, 99)\n+    assert parse_expr('.1[3_4]') == parse_expr('.1[34]') == Rational(133, 990)\n+    assert parse_expr('123_123.123_123[3_4]') == parse_expr('123123.123123[34]') == Rational(12189189189211, 99000000)\n",
  "problem_statement": "sympify(u\"α\") does not work\n```\nIn [37]: S(\"α\")\n--------------------------------------------------------------------------\n-\nSympifyError                              Traceback (most recent call last)\n\n/Users/aaronmeurer/Documents/Python/sympy/sympy/<ipython console> in <module>()\n\n/Users/aaronmeurer/Documents/Python/sympy/sympy/sympy/core/sympify.pyc in sympify(a, \nlocals, convert_xor)\n    114             a = a.replace('^','**')\n    115         import ast_parser\n--> 116         return ast_parser.parse_expr(a, locals)\n    117     raise SympifyError(\"%r is NOT a valid SymPy expression\" % a)\n    118 \n\n/Users/aaronmeurer/Documents/Python/sympy/sympy/sympy/core/ast_parser.pyc in \nparse_expr(s, local_dict)\n     89             a = parse(s.strip(), mode=\"eval\")\n     90         except SyntaxError:\n---> 91             raise SympifyError(\"Cannot parse.\")\n     92         a = Transform(local_dict, global_dict).visit(a)\n     93         e = compile(a, \"<string>\", \"eval\")\n\nSympifyError: SympifyError: Cannot parse.\n\nsympify() should be able to parse greek letters, as they are pretty printed for symbols of that \nname:\n\nIn [44]: alpha = Symbol('alpha')\n\nIn [45]: alpha\nOut[45]: α\n```\n\nOriginal issue for #4862: http://code.google.com/p/sympy/issues/detail?id=1763\nOriginal author: https://code.google.com/u/asmeurer@gmail.com/\nOriginal owner: https://code.google.com/u/asmeurer@gmail.com/\n\n",
  "hints_text": "```\nActually, \"α\" is garbage and we shouldn't do anything with it: that's a bytestream\ncontaining whatever value the system's encoding gives to the unicode character alpha.\nThat the interpreter allows such nonsense is a Python 2.* bug. Python 3.* is much\nmore sensible (recall that a Python 3.* string is Python 2.*'s unicode, while Python\n3.*'s bytes is Python 2.*'s string):\n\nPython 3.1.1+ ( r311 :74480, Nov  2 2009, 14:49:22) \n[GCC 4.4.1] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \"α\"\n'α'\n>>> b\"α\"\n  File \"<stdin>\", line 1\nSyntaxError: bytes can only contain ASCII literal characters.\n\nFor comparison:\n\nPython 2.6.4 ( r264 :75706, Nov  2 2009, 14:38:03) \n[GCC 4.4.1] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \"α\"\n'\\xce\\xb1'\n>>> u\"α\"\nu'\\u03b1'\n>>> print u\"α\"\nα\n\nOn the other hand, u\"α\" is a sensible value and sympify should definitely do\nsomething with it, but doesn't:\n\nIn [56]: sympify(u\"α\")\n---------------------------------------------------------------------------\nUnicodeEncodeError                        Traceback (most recent call last)\n\n/media/sda2/Boulot/Projets/sympy-git/<ipython console> in <module>()\n\n/media/sda2/Boulot/Projets/sympy-git/sympy/core/sympify.pyc in sympify(a, locals,\nconvert_xor)\n    109             # and try to parse it. If it fails, then we have no luck and\n\n    110             # return an exception\n\n--> 111             a = str(a)\n    112 \n    113         if convert_xor:\n\nUnicodeEncodeError: 'ascii' codec can't encode characters in position 0-1: ordinal\nnot in range(128)\n\n**Summary:** sympify(u\"α\") does not work  \n**Labels:** -Priority-Medium Priority-High Milestone-Release0.7.0  \n\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1763#c1\nOriginal author: https://code.google.com/u/101272611947379421629/\n\n```\nIn my \"code-refactor-3\" branch, I bypass the encoding problem and get the same error\nas in the initial comment:\n\nIn [1]: sympify(u\"α\")\n---------------------------------------------------------------------------\nSympifyError                              Traceback (most recent call last)\n\n/media/sda2/Boulot/Projets/sympy-git/<ipython console> in <module>()\n\n/media/sda2/Boulot/Projets/sympy-git/sympy/core/sympify.py in sympify(a, locals,\nconvert_xor, strict)\n    123 \n    124     import ast_parser\n--> 125     return ast_parser.parse_expr(a, locals)\n    126 \n    127 def _sympify(a):\n\n/media/sda2/Boulot/Projets/sympy-git/sympy/core/ast_parser.pyc in parse_expr(s,\nlocal_dict)\n     88             a = parse(s.strip(), mode=\"eval\")\n     89         except SyntaxError:\n---> 90             raise SympifyError(\"Cannot parse.\")\n     91         a = Transform(local_dict, global_dict).visit(a)\n     92         e = compile(a, \"<string>\", \"eval\")\n\nSympifyError: SympifyError: 'Cannot parse.'\n\n\nThe fundamental problem is that ast_parser can only handle valid Python2 identifiers,\nwhich are limited to basic ASCII characters. Solving this seems very difficult. OTOH,\nthe good news is that Python3  has solved the problem for us. \nI'd suggest we postpone this until either:\n* we switch to Python3\n* or someone decides to do a complete overhaul of the parser.\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1763#c2\nOriginal author: https://code.google.com/u/101272611947379421629/\n\n```\nIt isn't that important, and there is no point in refactoring the parser when it will not be a problem in Python 3, \nso I vote to postpone to Python 3.  Likely we will go through a period of supporting both, so lets just make sure \nthat it works in Python 3 whenever we have a branch for it.  \n\nSo variable names can be unicode in Python 3?  I didn't know that.\n\n**Labels:** -Priority-High -Milestone-Release0.7.0 Priority-Low Milestone-Release0.8.0  \n\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1763#c3\nOriginal author: https://code.google.com/u/asmeurer@gmail.com/\n\n```\nThere are several motivations to write an own parser: issue 3970 , issue 4075 and issue 3159 .\n```\n\nReferenced issues: #3970, #4075, #3159\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1763#c4\nOriginal author: https://code.google.com/u/Vinzent.Steinberg@gmail.com/\n\n```\nWe now support Python 3, but there's still an error:\n\n>>> S(\"α\")\nTraceback (most recent call last):\n  File \"<console>\", line 1, in <module>\n  File \"/home/vperic/devel/sympy/sympy-py3k/sympy/core/sympify.py\", line 155, in sympify\n    expr = parse_expr(a, locals or {}, rational, convert_xor)\n  File \"/home/vperic/devel/sympy/sympy-py3k/sympy/parsing/sympy_parser.py\", line 112, in parse_expr\n    expr = eval(code, global_dict, local_dict) # take local objects in preference\n  File \"<string>\", line 1, in <module>\nNameError: name 'α' is not defined\n\n(dropping the milestone because it's just annoying :) )\n\n**Labels:** -Milestone-Release0.8.0 Python3  \n\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1763#c5\nOriginal author: https://code.google.com/u/108713607268198052411/\n\n```\nI think we do have our own parser now.  If I'm not mistaken, we could just add a token that converts a string of unicode characters to symbols.  Or maybe we should just limit it to things greek characters.\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1763#c6\nOriginal author: https://code.google.com/u/asmeurer@gmail.com/\n\n```\n**Labels:** Parsing  \n\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1763#c7\nOriginal author: https://code.google.com/u/asmeurer@gmail.com/\n\n```\n**Status:** Valid  \n\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1763#c8\nOriginal author: https://code.google.com/u/asmeurer@gmail.com/\n\nIf it's still open I would like to work on this.\nLooks like it still doesn't work. \nBut there may already be a fix at https://github.com/sympy/sympy/pull/8334\n@asmeurer  I feel this is not fixed  till now, please have a look below, I would like to solve this can you guide me\r\n\r\n```\r\nIn [1]: from sympy import *\r\n\r\nIn [2]: sympify(u\"α\")\r\n---------------------------------------------------------------------------\r\nSympifyError                              Traceback (most recent call last)\r\n<ipython-input-2-cf317bba09e1> in <module>()\r\n----> 1 sympify(u\"α\")\r\n\r\n/home/saiharsh/sympy/sympy/core/sympify.pyc in sympify(a, locals, convert_xor, strict, rational, evaluate)\r\n    329         expr = parse_expr(a, local_dict=locals, transformations=transformations, evaluate=evaluate)\r\n    330     except (TokenError, SyntaxError) as exc:\r\n--> 331         raise SympifyError('could not parse %r' % a, exc)\r\n    332 \r\n    333     return expr\r\n\r\nSympifyError: Sympify of expression 'could not parse u'\\u03b1'' failed, because of exception being raised:\r\nSyntaxError: invalid syntax (<string>, line 1)\r\n```\r\n\nWe just need to update our copy of the Python tokenizer (in sympy_tokenize.py) to be compatible with Python 3. \r\n\r\nI forget the reason why we have a copy of it instead of just using the version from the standard library. Can someone figure out the difference between our tokenizer and the Python 2 tokenizer? \nIt looks like our tokenizer is there to support two extensions:\r\n\r\n- factorial (`x!`)\r\n- repeated decimals (`1.[2]`)\r\n\r\nI would reconsider for both of these if they could just be done with a preparser, so we can just use the standard tokenizer. That would also allow disabling these things, which currently isn't possible. We'd have to do some manual tokenization to make sure we don't preparse the inside of a string, though.\r\n\r\nIf we don't go this route, we need to update the tokenizer to be based on Python 3's grammar. This should be a simple matter of copying the Python 3 tokenize module and re-applying our modifications to it. \nActually to properly handle ! we have to do a proper tokenization. Consider more complicated expressions like `(1 + 2)!`. \r\n\r\nPerhaps it is possible to just tokenize the expression with `!` and postprocess. It seems to produce an ERRORTOKEN and not stop the tokenization.\r\n\r\n```py\r\n>>> import tokenize\r\n>>> import io\r\n>>> for i in tokenize.tokenize(io.BytesIO(b'(1 + 2)! + 1').readline):\r\n...     print(i)\r\nTokenInfo(type=59 (BACKQUOTE), string='utf-8', start=(0, 0), end=(0, 0), line='')\r\nTokenInfo(type=53 (OP), string='(', start=(1, 0), end=(1, 1), line='(1 + 2)! + 1')\r\nTokenInfo(type=2 (NUMBER), string='1', start=(1, 1), end=(1, 2), line='(1 + 2)! + 1')\r\nTokenInfo(type=53 (OP), string='+', start=(1, 3), end=(1, 4), line='(1 + 2)! + 1')\r\nTokenInfo(type=2 (NUMBER), string='2', start=(1, 5), end=(1, 6), line='(1 + 2)! + 1')\r\nTokenInfo(type=53 (OP), string=')', start=(1, 6), end=(1, 7), line='(1 + 2)! + 1')\r\nTokenInfo(type=56 (ERRORTOKEN), string='!', start=(1, 7), end=(1, 8), line='(1 + 2)! + 1')\r\nTokenInfo(type=53 (OP), string='+', start=(1, 9), end=(1, 10), line='(1 + 2)! + 1')\r\nTokenInfo(type=2 (NUMBER), string='1', start=(1, 11), end=(1, 12), line='(1 + 2)! + 1')\r\nTokenInfo(type=0 (ENDMARKER), string='', start=(2, 0), end=(2, 0), line='')\r\n```\r\n\r\nFor `0.[1]`, it's actually valid Python (it's an indexing of a float literal). So this can be post processed as well, probably at the AST level. We only allow this syntax for numeric values, no symbolic, so this isn't an issue. \nOur transformations are currently done only with the tokenization, not the ast (though that [could change](https://github.com/sympy/sympy/issues/10805)). Regardless, handling `0.[1]` is trivial to do with the standard tokenization. \nBy the way you have to use `tokenize.generate_tokens`, not `tokenize.tokenize`, because in Python 2 `tokenize.tokenize` just prints the tokens instead of returning them. ",
  "created_at": "2018-02-05T22:40:31Z",
  "version": "1.1",
  "FAIL_TO_PASS": "[\"test_unicode_names\"]",
  "PASS_TO_PASS": "[\"test_sympy_parser\", \"test_rationalize\", \"test_factorial_fail\", \"test_repeated_fail\", \"test_repeated_dot_only\", \"test_local_dict\", \"test_global_dict\", \"test_issue_2515\", \"test_split_symbols\", \"test_split_symbols_function\", \"test_match_parentheses_implicit_multiplication\", \"test_convert_equals_signs\"]",
  "environment_setup_commit": "ec9e3c0436fbff934fa84e22bf07f1b3ef5bfac3",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:31.091733",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}