{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-10881",
  "base_commit": "4989a9503753a92089f39e154a2bb5d160b5d276",
  "patch": "diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py\n--- a/sklearn/linear_model/logistic.py\n+++ b/sklearn/linear_model/logistic.py\n@@ -707,7 +707,7 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,\n                 func, w0, fprime=None,\n                 args=(X, target, 1. / C, sample_weight),\n                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)\n-            if info[\"warnflag\"] == 1 and verbose > 0:\n+            if info[\"warnflag\"] == 1:\n                 warnings.warn(\"lbfgs failed to converge. Increase the number \"\n                               \"of iterations.\", ConvergenceWarning)\n             # In scipy <= 1.0.0, nit may exceed maxiter.\ndiff --git a/sklearn/svm/base.py b/sklearn/svm/base.py\n--- a/sklearn/svm/base.py\n+++ b/sklearn/svm/base.py\n@@ -907,7 +907,7 @@ def _fit_liblinear(X, y, C, fit_intercept, intercept_scaling, class_weight,\n     # on 32-bit platforms, we can't get to the UINT_MAX limit that\n     # srand supports\n     n_iter_ = max(n_iter_)\n-    if n_iter_ >= max_iter and verbose > 0:\n+    if n_iter_ >= max_iter:\n         warnings.warn(\"Liblinear failed to converge, increase \"\n                       \"the number of iterations.\", ConvergenceWarning)\n \n",
  "test_patch": "diff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py\n--- a/sklearn/linear_model/tests/test_logistic.py\n+++ b/sklearn/linear_model/tests/test_logistic.py\n@@ -800,15 +800,6 @@ def test_logistic_regression_class_weights():\n         assert_array_almost_equal(clf1.coef_, clf2.coef_, decimal=6)\n \n \n-def test_logistic_regression_convergence_warnings():\n-    # Test that warnings are raised if model does not converge\n-\n-    X, y = make_classification(n_samples=20, n_features=20, random_state=0)\n-    clf_lib = LogisticRegression(solver='liblinear', max_iter=2, verbose=1)\n-    assert_warns(ConvergenceWarning, clf_lib.fit, X, y)\n-    assert_equal(clf_lib.n_iter_, 2)\n-\n-\n def test_logistic_regression_multinomial():\n     # Tests for the multinomial option in logistic regression\n \n@@ -1033,7 +1024,6 @@ def test_logreg_predict_proba_multinomial():\n     assert_greater(clf_wrong_loss, clf_multi_loss)\n \n \n-@ignore_warnings\n def test_max_iter():\n     # Test that the maximum number of iteration is reached\n     X, y_bin = iris.data, iris.target.copy()\n@@ -1049,7 +1039,7 @@ def test_max_iter():\n                 lr = LogisticRegression(max_iter=max_iter, tol=1e-15,\n                                         multi_class=multi_class,\n                                         random_state=0, solver=solver)\n-                lr.fit(X, y_bin)\n+                assert_warns(ConvergenceWarning, lr.fit, X, y_bin)\n                 assert_equal(lr.n_iter_[0], max_iter)\n \n \ndiff --git a/sklearn/model_selection/tests/test_search.py b/sklearn/model_selection/tests/test_search.py\n--- a/sklearn/model_selection/tests/test_search.py\n+++ b/sklearn/model_selection/tests/test_search.py\n@@ -33,6 +33,7 @@\n from sklearn.base import BaseEstimator\n from sklearn.base import clone\n from sklearn.exceptions import NotFittedError\n+from sklearn.exceptions import ConvergenceWarning\n from sklearn.datasets import make_classification\n from sklearn.datasets import make_blobs\n from sklearn.datasets import make_multilabel_classification\n@@ -350,7 +351,9 @@ def test_return_train_score_warn():\n     for estimator in estimators:\n         for val in [True, False, 'warn']:\n             estimator.set_params(return_train_score=val)\n-            result[val] = assert_no_warnings(estimator.fit, X, y).cv_results_\n+            fit_func = ignore_warnings(estimator.fit,\n+                                       category=ConvergenceWarning)\n+            result[val] = assert_no_warnings(fit_func, X, y).cv_results_\n \n     train_keys = ['split0_train_score', 'split1_train_score',\n                   'split2_train_score', 'mean_train_score', 'std_train_score']\ndiff --git a/sklearn/svm/tests/test_svm.py b/sklearn/svm/tests/test_svm.py\n--- a/sklearn/svm/tests/test_svm.py\n+++ b/sklearn/svm/tests/test_svm.py\n@@ -871,13 +871,17 @@ def test_consistent_proba():\n     assert_array_almost_equal(proba_1, proba_2)\n \n \n-def test_linear_svc_convergence_warnings():\n+def test_linear_svm_convergence_warnings():\n     # Test that warnings are raised if model does not converge\n \n-    lsvc = svm.LinearSVC(max_iter=2, verbose=1)\n+    lsvc = svm.LinearSVC(random_state=0, max_iter=2)\n     assert_warns(ConvergenceWarning, lsvc.fit, X, Y)\n     assert_equal(lsvc.n_iter_, 2)\n \n+    lsvr = svm.LinearSVR(random_state=0, max_iter=2)\n+    assert_warns(ConvergenceWarning, lsvr.fit, iris.data, iris.target)\n+    assert_equal(lsvr.n_iter_, 2)\n+\n \n def test_svr_coef_sign():\n     # Test that SVR(kernel=\"linear\") has coef_ with the right sign.\n",
  "problem_statement": "No warning when LogisticRegression does not converge\n<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: http://stackoverflow.com/questions/tagged/scikit-learn\r\n- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn\r\nFor more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->\r\nI've run LogisticRegressionCV on the Wisconsin Breast Cancer data, and the output of clf.n_iter_ was 100 for all but 1 of the variables. The default of 100 iterations was probably not sufficient in this case. Should there not be some kind of warning? I have done some tests and ~3000 iterations was probably a better choice for max_iter...\r\n\r\n#### Steps/Code to Reproduce\r\n```py\r\nfrom sklearn.datasets import load_breast_cancer\r\nfrom sklearn.linear_model import LogisticRegressionCV\r\n\r\ndata = load_breast_cancer()\r\ny = data.target\r\nX = data.data\r\n\r\nclf = LogisticRegressionCV()\r\nclf.fit(X, y)\r\nprint(clf.n_iter_)\r\n```\r\n\r\n<!--\r\nExample:\r\n```python\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nfrom sklearn.decomposition import LatentDirichletAllocation\r\n\r\ndocs = [\"Help I have a bug\" for i in range(1000)]\r\n\r\nvectorizer = CountVectorizer(input=docs, analyzer='word')\r\nlda_features = vectorizer.fit_transform(docs)\r\n\r\nlda_model = LatentDirichletAllocation(\r\n    n_topics=10,\r\n    learning_method='online',\r\n    evaluate_every=10,\r\n    n_jobs=4,\r\n)\r\nmodel = lda_model.fit(lda_features)\r\n```\r\nIf the code is too long, feel free to put it in a public gist and link\r\nit in the issue: https://gist.github.com\r\n-->\r\n\r\n#### Expected Results\r\n<!-- Example: No error is thrown. Please paste or describe the expected results.-->\r\nSome kind of error to be shown. E.g: \"result did not converge, try increasing the maximum number of iterations (max_iter)\"\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport numpy; print(\"NumPy\", numpy.__version__)\r\nimport scipy; print(\"SciPy\", scipy.__version__)\r\nimport sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\n-->\r\n\r\n>>> import platform; print(platform.platform())\r\nDarwin-16.7.0-x86_64-i386-64bit\r\n>>> import sys; print(\"Python\", sys.version)\r\n('Python', '2.7.14 |Anaconda, Inc.| (default, Oct  5 2017, 02:28:52) \\n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]')\r\n>>> import numpy; print(\"NumPy\", numpy.__version__)\r\n('NumPy', '1.13.3')\r\n>>> import scipy; print(\"SciPy\", scipy.__version__)\r\n('SciPy', '1.0.0')\r\n>>> import sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\n('Scikit-Learn', '0.19.1')\r\n\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
  "hints_text": "If you use verbose=1 in your snippet, you'll get plenty of ConvergenceWarning.\r\n\r\n```py\r\nfrom sklearn.datasets import load_breast_cancer\r\nfrom sklearn.linear_model import LogisticRegressionCV\r\n\r\ndata = load_breast_cancer()\r\ny = data.target\r\nX = data.data\r\n\r\nclf = LogisticRegressionCV(verbose=1)\r\nclf.fit(X, y)\r\nprint(clf.n_iter_)\r\n```\r\n\r\nI am going to close this one, please reopen if  you strongly disagree.\nI also think that `LogisticRegression` ~~(and `LogisticRegressionCV`)~~ should print a `ConvergenceWarning` when it fails to converge with default parameters.\r\n\r\n It does not make sense to expect the users to set the verbosity in order to get the warning. Also other methods don't do this:  e.g. MLPClassifier [may raise ConvergenceWarnings](https://github.com/scikit-learn/scikit-learn/blob/de29f3f22db6e017aef9dc77935d8ef43d2d7b44/sklearn/neural_network/tests/test_mlp.py#L70) so does [MultiTaskElasticNet](https://github.com/scikit-learn/scikit-learn/blob/da71b827b8b56bd8305b7fe6c13724c7b5355209/sklearn/linear_model/tests/test_coordinate_descent.py#L407) or [LassoLars](https://github.com/scikit-learn/scikit-learn/blob/34aad31924dc9c8551e6ff8edf4e729dbfc1b973/sklearn/linear_model/tests/test_least_angle.py#L349). \r\n\r\n The culprit is this [line](https://github.com/scikit-learn/scikit-learn/blob/da71b827b8b56bd8305b7fe6c13724c7b5355209/sklearn/linear_model/logistic.py#L710-L712) that only affects the `lbfgs` solver. This is more critical if we can to make `lbfgs` the default solver https://github.com/scikit-learn/scikit-learn/issues/9997 in `LogisticRegression`\nAfter more thoughts, the case of `LogisticRegressionCV` is more debatable, as there are always CV parameters that may fail to converge and handling it the same way as `GridSearchCV` would be probably more reasonable.\r\n\r\nHowever,\r\n```py\r\nfrom sklearn.datasets import load_breast_cancer\r\nfrom sklearn.linear_model import LogisticRegression\r\n\r\ndata = load_breast_cancer()\r\ny = data.target\r\nX = data.data\r\n\r\nclf = LogisticRegression(solver='lbfgs')\r\nclf.fit(X, y)\r\nprint(clf.n_iter_)\r\n```\r\nsilently fails to converge, and this is a bug IMO. This was also recently relevant in https://github.com/scikit-learn/scikit-learn/issues/10813\r\n\nA quick git grep seems to confirm that ConvergenceWarning are generally issued when verbose=0, reopening.\r\n\r\nThe same thing happens for solver='liblinear' (the default solver at the time of writing) by the way (you need to decrease max_iter e.g. set it to 1). There should be a test that checks the ConvergenceWarning for all solvers.\nI see that nobody is working on it. Can I do it ?\nSure,  thank you @AlexandreSev \nUnless @oliverangelil was planning to submit a PR..\nI think I need to correct also this [line](https://github.com/scikit-learn/scikit-learn/blob/da71b827b8b56bd8305b7fe6c13724c7b5355209/sklearn/linear_model/tests/test_logistic.py#L807). Do you agree ?\r\nMoreover, I just saw that the name of this [function](https://github.com/scikit-learn/scikit-learn/blob/da71b827b8b56bd8305b7fe6c13724c7b5355209/sklearn/linear_model/tests/test_logistic.py#L92) is not perfectly correct, since it does not test the convergence warning.\r\nMaybe we could write a test a bit like this [one](https://github.com/scikit-learn/scikit-learn/blob/da71b827b8b56bd8305b7fe6c13724c7b5355209/sklearn/linear_model/tests/test_logistic.py#L1037) to test all the warnings. What do you think ?\nYou can reuse test_max_iter indeed to check for all solvers that there has been a warning and add an assert_warns_message in it. If you do that, test_logistic_regression_convergence_warnings is not really needed any more. There is no need to touch test_lr_liblinear_warning.\r\n\r\nThe best is to open a PR and see how that goes! Not that you have mentioned changing the tests but you will also need to change sklearn/linear_model/logistic.py to make sure a ConvergenceWarning is issued for all the solvers.\r\n\r\n\nyes I think there are a few cases where we only warn if verbose. I'd rather\nconsistently warn even if verbose=0\n",
  "created_at": "2018-03-28T12:36:45Z",
  "version": "0.20",
  "FAIL_TO_PASS": "[\"sklearn/linear_model/tests/test_logistic.py::test_max_iter\", \"sklearn/svm/tests/test_svm.py::test_linear_svm_convergence_warnings\"]",
  "PASS_TO_PASS": "[\"sklearn/linear_model/tests/test_logistic.py::test_predict_2_classes\", \"sklearn/linear_model/tests/test_logistic.py::test_error\", \"sklearn/linear_model/tests/test_logistic.py::test_lr_liblinear_warning\", \"sklearn/linear_model/tests/test_logistic.py::test_predict_3_classes\", \"sklearn/linear_model/tests/test_logistic.py::test_predict_iris\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_validation\", \"sklearn/linear_model/tests/test_logistic.py::test_check_solver_option\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_binary\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_binary_probabilities\", \"sklearn/linear_model/tests/test_logistic.py::test_sparsify\", \"sklearn/linear_model/tests/test_logistic.py::test_inconsistent_input\", \"sklearn/linear_model/tests/test_logistic.py::test_write_parameters\", \"sklearn/linear_model/tests/test_logistic.py::test_nan\", \"sklearn/linear_model/tests/test_logistic.py::test_consistency_path\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_path_convergence_fail\", \"sklearn/linear_model/tests/test_logistic.py::test_liblinear_dual_random_state\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_loss_and_grad\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_grad_hess\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_logistic_regression_string_inputs\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv_sparse\", \"sklearn/linear_model/tests/test_logistic.py::test_intercept_logistic_helper\", \"sklearn/linear_model/tests/test_logistic.py::test_ovr_multinomial_iris\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_solvers\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_solvers_multiclass\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regressioncv_class_weights\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_sample_weights\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_class_weights\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multinomial\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_grad_hess\", \"sklearn/linear_model/tests/test_logistic.py::test_liblinear_decision_function_zero\", \"sklearn/linear_model/tests/test_logistic.py::test_liblinear_logregcv_sparse\", \"sklearn/linear_model/tests/test_logistic.py::test_saga_sparse\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_intercept_scaling\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_intercept_scaling_zero\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_l1\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_l1_sparse_data\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_cv_penalty\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_predict_proba_multinomial\", \"sklearn/linear_model/tests/test_logistic.py::test_n_iter\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start\", \"sklearn/linear_model/tests/test_logistic.py::test_saga_vs_liblinear\", \"sklearn/linear_model/tests/test_logistic.py::test_dtype_match\", \"sklearn/model_selection/tests/test_search.py::test_parameter_grid\", \"sklearn/model_selection/tests/test_search.py::test_grid_search\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_with_fit_params\", \"sklearn/model_selection/tests/test_search.py::test_random_search_with_fit_params\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_fit_params_deprecation\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_fit_params_two_places\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_no_score\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_score_method\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_groups\", \"sklearn/model_selection/tests/test_search.py::test_return_train_score_warn\", \"sklearn/model_selection/tests/test_search.py::test_classes__property\", \"sklearn/model_selection/tests/test_search.py::test_trivial_cv_results_attr\", \"sklearn/model_selection/tests/test_search.py::test_no_refit\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_error\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_one_grid_point\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_when_param_grid_includes_range\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_bad_param_grid\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_sparse\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_sparse_scoring\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_precomputed_kernel\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_precomputed_kernel_error_nonsquare\", \"sklearn/model_selection/tests/test_search.py::test_refit\", \"sklearn/model_selection/tests/test_search.py::test_gridsearch_nd\", \"sklearn/model_selection/tests/test_search.py::test_X_as_list\", \"sklearn/model_selection/tests/test_search.py::test_y_as_list\", \"sklearn/model_selection/tests/test_search.py::test_pandas_input\", \"sklearn/model_selection/tests/test_search.py::test_unsupervised_grid_search\", \"sklearn/model_selection/tests/test_search.py::test_gridsearch_no_predict\", \"sklearn/model_selection/tests/test_search.py::test_param_sampler\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_cv_results\", \"sklearn/model_selection/tests/test_search.py::test_random_search_cv_results\", \"sklearn/model_selection/tests/test_search.py::test_search_iid_param\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_cv_results_multimetric\", \"sklearn/model_selection/tests/test_search.py::test_random_search_cv_results_multimetric\", \"sklearn/model_selection/tests/test_search.py::test_search_cv_results_rank_tie_breaking\", \"sklearn/model_selection/tests/test_search.py::test_search_cv_results_none_param\", \"sklearn/model_selection/tests/test_search.py::test_search_cv_timing\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_correct_score_results\", \"sklearn/model_selection/tests/test_search.py::test_fit_grid_point\", \"sklearn/model_selection/tests/test_search.py::test_pickle\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_with_multioutput_data\", \"sklearn/model_selection/tests/test_search.py::test_predict_proba_disabled\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_allows_nans\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_failing_classifier\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_failing_classifier_raise\", \"sklearn/model_selection/tests/test_search.py::test_parameters_sampler_replacement\", \"sklearn/model_selection/tests/test_search.py::test_stochastic_gradient_loss_param\", \"sklearn/model_selection/tests/test_search.py::test_search_train_scores_set_to_false\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_cv_splits_consistency\", \"sklearn/model_selection/tests/test_search.py::test_transform_inverse_transform_round_trip\", \"sklearn/model_selection/tests/test_search.py::test_deprecated_grid_search_iid\", \"sklearn/svm/tests/test_svm.py::test_libsvm_parameters\", \"sklearn/svm/tests/test_svm.py::test_libsvm_iris\", \"sklearn/svm/tests/test_svm.py::test_precomputed\", \"sklearn/svm/tests/test_svm.py::test_svr\", \"sklearn/svm/tests/test_svm.py::test_linearsvr\", \"sklearn/svm/tests/test_svm.py::test_linearsvr_fit_sampleweight\", \"sklearn/svm/tests/test_svm.py::test_svr_errors\", \"sklearn/svm/tests/test_svm.py::test_oneclass\", \"sklearn/svm/tests/test_svm.py::test_oneclass_decision_function\", \"sklearn/svm/tests/test_svm.py::test_oneclass_score_samples\", \"sklearn/svm/tests/test_svm.py::test_tweak_params\", \"sklearn/svm/tests/test_svm.py::test_probability\", \"sklearn/svm/tests/test_svm.py::test_decision_function\", \"sklearn/svm/tests/test_svm.py::test_decision_function_shape\", \"sklearn/svm/tests/test_svm.py::test_svr_predict\", \"sklearn/svm/tests/test_svm.py::test_weight\", \"sklearn/svm/tests/test_svm.py::test_sample_weights\", \"sklearn/svm/tests/test_svm.py::test_auto_weight\", \"sklearn/svm/tests/test_svm.py::test_bad_input\", \"sklearn/svm/tests/test_svm.py::test_unicode_kernel\", \"sklearn/svm/tests/test_svm.py::test_sparse_precomputed\", \"sklearn/svm/tests/test_svm.py::test_linearsvc_parameters\", \"sklearn/svm/tests/test_svm.py::test_linearsvx_loss_penalty_deprecations\", \"sklearn/svm/tests/test_svm.py::test_linear_svx_uppercase_loss_penality_raises_error\", \"sklearn/svm/tests/test_svm.py::test_linearsvc\", \"sklearn/svm/tests/test_svm.py::test_linearsvc_crammer_singer\", \"sklearn/svm/tests/test_svm.py::test_linearsvc_fit_sampleweight\", \"sklearn/svm/tests/test_svm.py::test_crammer_singer_binary\", \"sklearn/svm/tests/test_svm.py::test_linearsvc_iris\", \"sklearn/svm/tests/test_svm.py::test_dense_liblinear_intercept_handling\", \"sklearn/svm/tests/test_svm.py::test_liblinear_set_coef\", \"sklearn/svm/tests/test_svm.py::test_immutable_coef_property\", \"sklearn/svm/tests/test_svm.py::test_linearsvc_verbose\", \"sklearn/svm/tests/test_svm.py::test_svc_clone_with_callable_kernel\", \"sklearn/svm/tests/test_svm.py::test_svc_bad_kernel\", \"sklearn/svm/tests/test_svm.py::test_timeout\", \"sklearn/svm/tests/test_svm.py::test_unfitted\", \"sklearn/svm/tests/test_svm.py::test_consistent_proba\", \"sklearn/svm/tests/test_svm.py::test_svr_coef_sign\", \"sklearn/svm/tests/test_svm.py::test_linear_svc_intercept_scaling\", \"sklearn/svm/tests/test_svm.py::test_lsvc_intercept_scaling_zero\", \"sklearn/svm/tests/test_svm.py::test_hasattr_predict_proba\", \"sklearn/svm/tests/test_svm.py::test_decision_function_shape_two_class\", \"sklearn/svm/tests/test_svm.py::test_ovr_decision_function\", \"sklearn/svm/tests/test_svm.py::test_gamma_auto\", \"sklearn/svm/tests/test_svm.py::test_gamma_scale\"]",
  "environment_setup_commit": "55bf5d93e5674f13a1134d93a11fd0cd11aabcd1",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.955759",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}