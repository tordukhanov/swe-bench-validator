{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-10870",
  "base_commit": "b0e91e4110942e5b3c4333b1c6b6dfefbd1a6124",
  "patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -172,11 +172,14 @@ def _initialize(self, X, resp):\n     def fit(self, X, y=None):\n         \"\"\"Estimate model parameters with the EM algorithm.\n \n-        The method fits the model `n_init` times and set the parameters with\n+        The method fits the model ``n_init`` times and sets the parameters with\n         which the model has the largest likelihood or lower bound. Within each\n-        trial, the method iterates between E-step and M-step for `max_iter`\n+        trial, the method iterates between E-step and M-step for ``max_iter``\n         times until the change of likelihood or lower bound is less than\n-        `tol`, otherwise, a `ConvergenceWarning` is raised.\n+        ``tol``, otherwise, a ``ConvergenceWarning`` is raised.\n+        If ``warm_start`` is ``True``, then ``n_init`` is ignored and a single\n+        initialization is performed upon the first call. Upon consecutive\n+        calls, training starts where it left off.\n \n         Parameters\n         ----------\n@@ -232,27 +235,28 @@ def fit_predict(self, X, y=None):\n \n             if do_init:\n                 self._initialize_parameters(X, random_state)\n-                self.lower_bound_ = -np.infty\n+\n+            lower_bound = (-np.infty if do_init else self.lower_bound_)\n \n             for n_iter in range(1, self.max_iter + 1):\n-                prev_lower_bound = self.lower_bound_\n+                prev_lower_bound = lower_bound\n \n                 log_prob_norm, log_resp = self._e_step(X)\n                 self._m_step(X, log_resp)\n-                self.lower_bound_ = self._compute_lower_bound(\n+                lower_bound = self._compute_lower_bound(\n                     log_resp, log_prob_norm)\n \n-                change = self.lower_bound_ - prev_lower_bound\n+                change = lower_bound - prev_lower_bound\n                 self._print_verbose_msg_iter_end(n_iter, change)\n \n                 if abs(change) < self.tol:\n                     self.converged_ = True\n                     break\n \n-            self._print_verbose_msg_init_end(self.lower_bound_)\n+            self._print_verbose_msg_init_end(lower_bound)\n \n-            if self.lower_bound_ > max_lower_bound:\n-                max_lower_bound = self.lower_bound_\n+            if lower_bound > max_lower_bound:\n+                max_lower_bound = lower_bound\n                 best_params = self._get_parameters()\n                 best_n_iter = n_iter\n \n@@ -265,6 +269,7 @@ def fit_predict(self, X, y=None):\n \n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n+        self.lower_bound_ = max_lower_bound\n \n         return log_resp.argmax(axis=1)\n \ndiff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -512,6 +512,8 @@ class GaussianMixture(BaseMixture):\n         If 'warm_start' is True, the solution of the last fitting is used as\n         initialization for the next call of fit(). This can speed up\n         convergence when fit is called several times on similar problems.\n+        In that case, 'n_init' is ignored and only a single initialization\n+        occurs upon the first call.\n         See :term:`the Glossary <warm_start>`.\n \n     verbose : int, default to 0.\n@@ -575,7 +577,8 @@ class GaussianMixture(BaseMixture):\n         Number of step used by the best fit of EM to reach the convergence.\n \n     lower_bound_ : float\n-        Log-likelihood of the best fit of EM.\n+        Lower bound value on the log-likelihood (of the training data with\n+        respect to the model) of the best fit of EM.\n \n     See Also\n     --------\n",
  "test_patch": "diff --git a/sklearn/mixture/tests/test_gaussian_mixture.py b/sklearn/mixture/tests/test_gaussian_mixture.py\n--- a/sklearn/mixture/tests/test_gaussian_mixture.py\n+++ b/sklearn/mixture/tests/test_gaussian_mixture.py\n@@ -764,7 +764,6 @@ def test_gaussian_mixture_verbose():\n \n \n def test_warm_start():\n-\n     random_state = 0\n     rng = np.random.RandomState(random_state)\n     n_samples, n_features, n_components = 500, 2, 2\n@@ -806,6 +805,25 @@ def test_warm_start():\n     assert_true(h.converged_)\n \n \n+@ignore_warnings(category=ConvergenceWarning)\n+def test_convergence_detected_with_warm_start():\n+    # We check that convergence is detected when warm_start=True\n+    rng = np.random.RandomState(0)\n+    rand_data = RandomData(rng)\n+    n_components = rand_data.n_components\n+    X = rand_data.X['full']\n+\n+    for max_iter in (1, 2, 50):\n+        gmm = GaussianMixture(n_components=n_components, warm_start=True,\n+                              max_iter=max_iter, random_state=rng)\n+        for _ in range(100):\n+            gmm.fit(X)\n+            if gmm.converged_:\n+                break\n+        assert gmm.converged_\n+        assert max_iter >= gmm.n_iter_\n+\n+\n def test_score():\n     covar_type = 'full'\n     rng = np.random.RandomState(0)\n@@ -991,14 +1009,14 @@ def test_sample():\n @ignore_warnings(category=ConvergenceWarning)\n def test_init():\n     # We check that by increasing the n_init number we have a better solution\n-    random_state = 0\n-    rand_data = RandomData(np.random.RandomState(random_state), scale=1)\n-    n_components = rand_data.n_components\n-    X = rand_data.X['full']\n+    for random_state in range(25):\n+        rand_data = RandomData(np.random.RandomState(random_state), scale=1)\n+        n_components = rand_data.n_components\n+        X = rand_data.X['full']\n \n-    gmm1 = GaussianMixture(n_components=n_components, n_init=1,\n-                           max_iter=1, random_state=random_state).fit(X)\n-    gmm2 = GaussianMixture(n_components=n_components, n_init=100,\n-                           max_iter=1, random_state=random_state).fit(X)\n+        gmm1 = GaussianMixture(n_components=n_components, n_init=1,\n+                               max_iter=1, random_state=random_state).fit(X)\n+        gmm2 = GaussianMixture(n_components=n_components, n_init=10,\n+                               max_iter=1, random_state=random_state).fit(X)\n \n-    assert_greater(gmm2.lower_bound_, gmm1.lower_bound_)\n+        assert gmm2.lower_bound_ >= gmm1.lower_bound_\n",
  "problem_statement": "In Gaussian mixtures, when n_init > 1, the lower_bound_ is not always the max\n#### Description\r\nIn Gaussian mixtures, when `n_init` is set to any value greater than 1, the `lower_bound_` is not the max lower bound across all initializations, but just the lower bound of the last initialization.\r\n\r\nThe bug can be fixed by adding the following line just before `return self` in `BaseMixture.fit()`:\r\n\r\n```python\r\nself.lower_bound_ = max_lower_bound\r\n```\r\n\r\nThe test that should have caught this bug is `test_init()` in `mixture/tests/test_gaussian_mixture.py`, but it just does a single test, so it had a 50% chance of missing the issue. It should be updated to try many random states.\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nimport numpy as np\r\nfrom sklearn.mixture import GaussianMixture\r\n\r\nX = np.random.rand(1000, 10)\r\nfor random_state in range(100):\r\n    gm1 = GaussianMixture(n_components=2, n_init=1, random_state=random_state).fit(X)\r\n    gm2 = GaussianMixture(n_components=2, n_init=10, random_state=random_state).fit(X)\r\n    assert gm2.lower_bound_ > gm1.lower_bound_, random_state\r\n```\r\n\r\n#### Expected Results\r\nNo error.\r\n\r\n#### Actual Results\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\nAssertionError: 4\r\n```\r\n\r\n#### Versions\r\n\r\n```\r\n>>> import platform; print(platform.platform())\r\nDarwin-17.4.0-x86_64-i386-64bit\r\n>>> import sys; print(\"Python\", sys.version)\r\nPython 3.6.4 (default, Dec 21 2017, 20:33:21)\r\n[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.38)]\r\n>>> import numpy; print(\"NumPy\", numpy.__version__)\r\nNumPy 1.14.2\r\n>>> import scipy; print(\"SciPy\", scipy.__version__)\r\nSciPy 1.0.0\r\n>>> import sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\nScikit-Learn 0.19.1\r\n```\nIn Gaussian mixtures, when n_init > 1, the lower_bound_ is not always the max\n#### Description\r\nIn Gaussian mixtures, when `n_init` is set to any value greater than 1, the `lower_bound_` is not the max lower bound across all initializations, but just the lower bound of the last initialization.\r\n\r\nThe bug can be fixed by adding the following line just before `return self` in `BaseMixture.fit()`:\r\n\r\n```python\r\nself.lower_bound_ = max_lower_bound\r\n```\r\n\r\nThe test that should have caught this bug is `test_init()` in `mixture/tests/test_gaussian_mixture.py`, but it just does a single test, so it had a 50% chance of missing the issue. It should be updated to try many random states.\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nimport numpy as np\r\nfrom sklearn.mixture import GaussianMixture\r\n\r\nX = np.random.rand(1000, 10)\r\nfor random_state in range(100):\r\n    gm1 = GaussianMixture(n_components=2, n_init=1, random_state=random_state).fit(X)\r\n    gm2 = GaussianMixture(n_components=2, n_init=10, random_state=random_state).fit(X)\r\n    assert gm2.lower_bound_ > gm1.lower_bound_, random_state\r\n```\r\n\r\n#### Expected Results\r\nNo error.\r\n\r\n#### Actual Results\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\nAssertionError: 4\r\n```\r\n\r\n#### Versions\r\n\r\n```\r\n>>> import platform; print(platform.platform())\r\nDarwin-17.4.0-x86_64-i386-64bit\r\n>>> import sys; print(\"Python\", sys.version)\r\nPython 3.6.4 (default, Dec 21 2017, 20:33:21)\r\n[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.38)]\r\n>>> import numpy; print(\"NumPy\", numpy.__version__)\r\nNumPy 1.14.2\r\n>>> import scipy; print(\"SciPy\", scipy.__version__)\r\nSciPy 1.0.0\r\n>>> import sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\nScikit-Learn 0.19.1\r\n```\n",
  "hints_text": "\n",
  "created_at": "2018-03-25T14:06:57Z",
  "version": "0.20",
  "FAIL_TO_PASS": "[\"sklearn/mixture/tests/test_gaussian_mixture.py::test_init\"]",
  "PASS_TO_PASS": "[\"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_attributes\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_check_X\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_check_weights\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_check_means\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_check_precisions\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_suffstat_sk_full\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_suffstat_sk_tied\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_suffstat_sk_diag\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_suffstat_sk_spherical\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_compute_log_det_cholesky\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_log_probabilities\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_estimate_log_prob_resp\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_predict_predict_proba\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_predict\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_best_params\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_convergence_warning\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_multiple_init\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_n_parameters\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_bic_1d_1component\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_aic_bic\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_verbose\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_warm_start\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_convergence_detected_with_warm_start\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_score\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_score_samples\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_monotonic_likelihood\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_regularisation\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_property\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_sample\"]",
  "environment_setup_commit": "55bf5d93e5674f13a1134d93a11fd0cd11aabcd1",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.955580",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}