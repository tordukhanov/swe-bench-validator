{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-25500",
  "base_commit": "4db04923a754b6a2defa1b172f55d492b85d165e",
  "patch": "diff --git a/sklearn/isotonic.py b/sklearn/isotonic.py\n--- a/sklearn/isotonic.py\n+++ b/sklearn/isotonic.py\n@@ -360,23 +360,16 @@ def fit(self, X, y, sample_weight=None):\n         self._build_f(X, y)\n         return self\n \n-    def transform(self, T):\n-        \"\"\"Transform new data by linear interpolation.\n-\n-        Parameters\n-        ----------\n-        T : array-like of shape (n_samples,) or (n_samples, 1)\n-            Data to transform.\n+    def _transform(self, T):\n+        \"\"\"`_transform` is called by both `transform` and `predict` methods.\n \n-            .. versionchanged:: 0.24\n-               Also accepts 2d array with 1 feature.\n+        Since `transform` is wrapped to output arrays of specific types (e.g.\n+        NumPy arrays, pandas DataFrame), we cannot make `predict` call `transform`\n+        directly.\n \n-        Returns\n-        -------\n-        y_pred : ndarray of shape (n_samples,)\n-            The transformed data.\n+        The above behaviour could be changed in the future, if we decide to output\n+        other type of arrays when calling `predict`.\n         \"\"\"\n-\n         if hasattr(self, \"X_thresholds_\"):\n             dtype = self.X_thresholds_.dtype\n         else:\n@@ -397,6 +390,24 @@ def transform(self, T):\n \n         return res\n \n+    def transform(self, T):\n+        \"\"\"Transform new data by linear interpolation.\n+\n+        Parameters\n+        ----------\n+        T : array-like of shape (n_samples,) or (n_samples, 1)\n+            Data to transform.\n+\n+            .. versionchanged:: 0.24\n+               Also accepts 2d array with 1 feature.\n+\n+        Returns\n+        -------\n+        y_pred : ndarray of shape (n_samples,)\n+            The transformed data.\n+        \"\"\"\n+        return self._transform(T)\n+\n     def predict(self, T):\n         \"\"\"Predict new data by linear interpolation.\n \n@@ -410,7 +421,7 @@ def predict(self, T):\n         y_pred : ndarray of shape (n_samples,)\n             Transformed data.\n         \"\"\"\n-        return self.transform(T)\n+        return self._transform(T)\n \n     # We implement get_feature_names_out here instead of using\n     # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n",
  "test_patch": "diff --git a/sklearn/tests/test_isotonic.py b/sklearn/tests/test_isotonic.py\n--- a/sklearn/tests/test_isotonic.py\n+++ b/sklearn/tests/test_isotonic.py\n@@ -5,6 +5,7 @@\n \n import pytest\n \n+import sklearn\n from sklearn.datasets import make_regression\n from sklearn.isotonic import (\n     check_increasing,\n@@ -680,3 +681,24 @@ def test_get_feature_names_out(shape):\n     assert isinstance(names, np.ndarray)\n     assert names.dtype == object\n     assert_array_equal([\"isotonicregression0\"], names)\n+\n+\n+def test_isotonic_regression_output_predict():\n+    \"\"\"Check that `predict` does return the expected output type.\n+\n+    We need to check that `transform` will output a DataFrame and a NumPy array\n+    when we set `transform_output` to `pandas`.\n+\n+    Non-regression test for:\n+    https://github.com/scikit-learn/scikit-learn/issues/25499\n+    \"\"\"\n+    pd = pytest.importorskip(\"pandas\")\n+    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n+    regressor = IsotonicRegression()\n+    with sklearn.config_context(transform_output=\"pandas\"):\n+        regressor.fit(X, y)\n+        X_trans = regressor.transform(X)\n+        y_pred = regressor.predict(X)\n+\n+    assert isinstance(X_trans, pd.DataFrame)\n+    assert isinstance(y_pred, np.ndarray)\n",
  "problem_statement": "CalibratedClassifierCV doesn't work with `set_config(transform_output=\"pandas\")`\n### Describe the bug\r\n\r\nCalibratedClassifierCV with isotonic regression doesn't work when we previously set `set_config(transform_output=\"pandas\")`.\r\nThe IsotonicRegression seems to return a dataframe, which is a problem for `_CalibratedClassifier`  in `predict_proba` where it tries to put the dataframe in a numpy array row `proba[:, class_idx] = calibrator.predict(this_pred)`.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import set_config\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\nfrom sklearn.linear_model import SGDClassifier\r\n\r\nset_config(transform_output=\"pandas\")\r\nmodel = CalibratedClassifierCV(SGDClassifier(), method='isotonic')\r\nmodel.fit(np.arange(90).reshape(30, -1), np.arange(30) % 2)\r\nmodel.predict(np.arange(90).reshape(30, -1))\r\n```\r\n\r\n### Expected Results\r\n\r\nIt should not crash.\r\n\r\n### Actual Results\r\n\r\n```\r\n../core/model_trainer.py:306: in train_model\r\n    cv_predictions = cross_val_predict(pipeline,\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:968: in cross_val_predict\r\n    predictions = parallel(\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:1085: in __call__\r\n    if self.dispatch_one_batch(iterator):\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:901: in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:819: in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:208: in apply_async\r\n    result = ImmediateResult(func)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:597: in __init__\r\n    self.results = batch()\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in __call__\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in <listcomp>\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/utils/fixes.py:117: in __call__\r\n    return self.function(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:1052: in _fit_and_predict\r\n    predictions = func(X_test)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/pipeline.py:548: in predict_proba\r\n    return self.steps[-1][1].predict_proba(Xt, **predict_proba_params)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:477: in predict_proba\r\n    proba = calibrated_classifier.predict_proba(X)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:764: in predict_proba\r\n    proba[:, class_idx] = calibrator.predict(this_pred)\r\nE   ValueError: could not broadcast input array from shape (20,1) into shape (20,)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0]\r\nexecutable: /home/philippe/.anaconda3/envs/strategy-training/bin/python\r\n   machine: Linux-5.15.0-57-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 22.2.2\r\n   setuptools: 62.3.2\r\n        numpy: 1.23.5\r\n        scipy: 1.9.3\r\n       Cython: None\r\n       pandas: 1.4.1\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```\r\n\n",
  "hints_text": "I can reproduce it. We need to investigate but I would expect the inner estimator not being able to handle some dataframe because we expected NumPy arrays before.\nThis could be a bit like https://github.com/scikit-learn/scikit-learn/pull/25370 where things get confused when pandas output is configured. I think the solution is different (TSNE's PCA is truely \"internal only\") but it seems like there might be something more general to investigate/think about related to pandas output and nested estimators.\nThere is something quite smelly regarding the interaction between `IsotonicRegression` and pandas output:\r\n\r\n<img width=\"1079\" alt=\"image\" src=\"https://user-images.githubusercontent.com/7454015/215147695-8aa08b83-705b-47a4-ab7c-43acb222098f.png\">\r\n\r\nIt seems that we output a pandas Series when calling `predict` which is something that we don't do for any other estimator. `IsotonicRegression` is already quite special since it accepts a single feature. I need to investigate more to understand why we wrap the output of the `predict` method.\nOK the reason is that `IsotonicRegression().predict(X)` call `IsotonicRegression().transform(X)` ;)\nI don't know if we should have:\r\n\r\n```python\r\ndef predict(self, T):\r\n    with config_context(transform_output=\"default\"):\r\n        return self.transform(T)\r\n```\r\n\r\nor\r\n\r\n```python\r\ndef predict(self, T):\r\n    return np.array(self.transform(T), copy=False).squeeze()\r\n```\nAnother solution would be to have a private `_transform` function called by both `transform` and `predict`. In this way, the `predict` call will not call the wrapper that is around the public `transform` method. I think this is even cleaner than the previous code.\n/take",
  "created_at": "2023-01-27T19:49:28Z",
  "version": "1.3",
  "FAIL_TO_PASS": "[\"sklearn/tests/test_isotonic.py::test_isotonic_regression_output_predict\"]",
  "PASS_TO_PASS": "[\"sklearn/tests/test_isotonic.py::test_permutation_invariance\", \"sklearn/tests/test_isotonic.py::test_check_increasing_small_number_of_samples\", \"sklearn/tests/test_isotonic.py::test_check_increasing_up\", \"sklearn/tests/test_isotonic.py::test_check_increasing_up_extreme\", \"sklearn/tests/test_isotonic.py::test_check_increasing_down\", \"sklearn/tests/test_isotonic.py::test_check_increasing_down_extreme\", \"sklearn/tests/test_isotonic.py::test_check_ci_warn\", \"sklearn/tests/test_isotonic.py::test_isotonic_regression\", \"sklearn/tests/test_isotonic.py::test_isotonic_regression_ties_min\", \"sklearn/tests/test_isotonic.py::test_isotonic_regression_ties_max\", \"sklearn/tests/test_isotonic.py::test_isotonic_regression_ties_secondary_\", \"sklearn/tests/test_isotonic.py::test_isotonic_regression_with_ties_in_differently_sized_groups\", \"sklearn/tests/test_isotonic.py::test_isotonic_regression_reversed\", \"sklearn/tests/test_isotonic.py::test_isotonic_regression_auto_decreasing\", \"sklearn/tests/test_isotonic.py::test_isotonic_regression_auto_increasing\", \"sklearn/tests/test_isotonic.py::test_assert_raises_exceptions\", \"sklearn/tests/test_isotonic.py::test_isotonic_sample_weight_parameter_default_value\", \"sklearn/tests/test_isotonic.py::test_isotonic_min_max_boundaries\", \"sklearn/tests/test_isotonic.py::test_isotonic_sample_weight\", \"sklearn/tests/test_isotonic.py::test_isotonic_regression_oob_raise\", \"sklearn/tests/test_isotonic.py::test_isotonic_regression_oob_clip\", \"sklearn/tests/test_isotonic.py::test_isotonic_regression_oob_nan\", \"sklearn/tests/test_isotonic.py::test_isotonic_regression_pickle\", \"sklearn/tests/test_isotonic.py::test_isotonic_duplicate_min_entry\", \"sklearn/tests/test_isotonic.py::test_isotonic_ymin_ymax\", \"sklearn/tests/test_isotonic.py::test_isotonic_zero_weight_loop\", \"sklearn/tests/test_isotonic.py::test_fast_predict\", \"sklearn/tests/test_isotonic.py::test_isotonic_copy_before_fit\", \"sklearn/tests/test_isotonic.py::test_isotonic_dtype\", \"sklearn/tests/test_isotonic.py::test_isotonic_mismatched_dtype[int32]\", \"sklearn/tests/test_isotonic.py::test_isotonic_mismatched_dtype[int64]\", \"sklearn/tests/test_isotonic.py::test_isotonic_mismatched_dtype[float32]\", \"sklearn/tests/test_isotonic.py::test_isotonic_mismatched_dtype[float64]\", \"sklearn/tests/test_isotonic.py::test_make_unique_dtype\", \"sklearn/tests/test_isotonic.py::test_make_unique_tolerance[float64]\", \"sklearn/tests/test_isotonic.py::test_make_unique_tolerance[float32]\", \"sklearn/tests/test_isotonic.py::test_isotonic_make_unique_tolerance\", \"sklearn/tests/test_isotonic.py::test_isotonic_non_regression_inf_slope\", \"sklearn/tests/test_isotonic.py::test_isotonic_thresholds[True]\", \"sklearn/tests/test_isotonic.py::test_isotonic_thresholds[False]\", \"sklearn/tests/test_isotonic.py::test_input_shape_validation\", \"sklearn/tests/test_isotonic.py::test_isotonic_2darray_more_than_1_feature\", \"sklearn/tests/test_isotonic.py::test_isotonic_regression_sample_weight_not_overwritten\", \"sklearn/tests/test_isotonic.py::test_get_feature_names_out[1d]\", \"sklearn/tests/test_isotonic.py::test_get_feature_names_out[2d]\"]",
  "environment_setup_commit": "1e8a5b833d1b58f3ab84099c4582239af854b23a",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:31.020345",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}