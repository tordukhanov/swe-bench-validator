{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-12827",
  "base_commit": "4a7075aa8841789eab30578f56bc48d8e6dc6042",
  "patch": "diff --git a/sklearn/preprocessing/data.py b/sklearn/preprocessing/data.py\n--- a/sklearn/preprocessing/data.py\n+++ b/sklearn/preprocessing/data.py\n@@ -1995,10 +1995,12 @@ class QuantileTransformer(BaseEstimator, TransformerMixin):\n     to spread out the most frequent values. It also reduces the impact of\n     (marginal) outliers: this is therefore a robust preprocessing scheme.\n \n-    The transformation is applied on each feature independently.\n-    The cumulative distribution function of a feature is used to project the\n-    original values. Features values of new/unseen data that fall below\n-    or above the fitted range will be mapped to the bounds of the output\n+    The transformation is applied on each feature independently. First an\n+    estimate of the cumulative distribution function of a feature is\n+    used to map the original values to a uniform distribution. The obtained\n+    values are then mapped to the desired output distribution using the\n+    associated quantile function. Features values of new/unseen data that fall\n+    below or above the fitted range will be mapped to the bounds of the output\n     distribution. Note that this transform is non-linear. It may distort linear\n     correlations between variables measured at the same scale but renders\n     variables measured at different scales more directly comparable.\n@@ -2198,11 +2200,7 @@ def fit(self, X, y=None):\n     def _transform_col(self, X_col, quantiles, inverse):\n         \"\"\"Private function to transform a single feature\"\"\"\n \n-        if self.output_distribution == 'normal':\n-            output_distribution = 'norm'\n-        else:\n-            output_distribution = self.output_distribution\n-        output_distribution = getattr(stats, output_distribution)\n+        output_distribution = self.output_distribution\n \n         if not inverse:\n             lower_bound_x = quantiles[0]\n@@ -2214,15 +2212,22 @@ def _transform_col(self, X_col, quantiles, inverse):\n             upper_bound_x = 1\n             lower_bound_y = quantiles[0]\n             upper_bound_y = quantiles[-1]\n-            #  for inverse transform, match a uniform PDF\n+            #  for inverse transform, match a uniform distribution\n             with np.errstate(invalid='ignore'):  # hide NaN comparison warnings\n-                X_col = output_distribution.cdf(X_col)\n+                if output_distribution == 'normal':\n+                    X_col = stats.norm.cdf(X_col)\n+                # else output distribution is already a uniform distribution\n+\n         # find index for lower and higher bounds\n         with np.errstate(invalid='ignore'):  # hide NaN comparison warnings\n-            lower_bounds_idx = (X_col - BOUNDS_THRESHOLD <\n-                                lower_bound_x)\n-            upper_bounds_idx = (X_col + BOUNDS_THRESHOLD >\n-                                upper_bound_x)\n+            if output_distribution == 'normal':\n+                lower_bounds_idx = (X_col - BOUNDS_THRESHOLD <\n+                                    lower_bound_x)\n+                upper_bounds_idx = (X_col + BOUNDS_THRESHOLD >\n+                                    upper_bound_x)\n+            if output_distribution == 'uniform':\n+                lower_bounds_idx = (X_col == lower_bound_x)\n+                upper_bounds_idx = (X_col == upper_bound_x)\n \n         isfinite_mask = ~np.isnan(X_col)\n         X_col_finite = X_col[isfinite_mask]\n@@ -2244,18 +2249,20 @@ def _transform_col(self, X_col, quantiles, inverse):\n \n         X_col[upper_bounds_idx] = upper_bound_y\n         X_col[lower_bounds_idx] = lower_bound_y\n-        # for forward transform, match the output PDF\n+        # for forward transform, match the output distribution\n         if not inverse:\n             with np.errstate(invalid='ignore'):  # hide NaN comparison warnings\n-                X_col = output_distribution.ppf(X_col)\n-            # find the value to clip the data to avoid mapping to\n-            # infinity. Clip such that the inverse transform will be\n-            # consistent\n-            clip_min = output_distribution.ppf(BOUNDS_THRESHOLD -\n-                                               np.spacing(1))\n-            clip_max = output_distribution.ppf(1 - (BOUNDS_THRESHOLD -\n-                                                    np.spacing(1)))\n-            X_col = np.clip(X_col, clip_min, clip_max)\n+                if output_distribution == 'normal':\n+                    X_col = stats.norm.ppf(X_col)\n+                    # find the value to clip the data to avoid mapping to\n+                    # infinity. Clip such that the inverse transform will be\n+                    # consistent\n+                    clip_min = stats.norm.ppf(BOUNDS_THRESHOLD - np.spacing(1))\n+                    clip_max = stats.norm.ppf(1 - (BOUNDS_THRESHOLD -\n+                                                   np.spacing(1)))\n+                    X_col = np.clip(X_col, clip_min, clip_max)\n+                # else output distribution is uniform and the ppf is the\n+                # identity function so we let X_col unchanged\n \n         return X_col\n \n@@ -2272,7 +2279,7 @@ def _check_inputs(self, X, accept_sparse_negative=False):\n                 raise ValueError('QuantileTransformer only accepts'\n                                  ' non-negative sparse matrices.')\n \n-        # check the output PDF\n+        # check the output distribution\n         if self.output_distribution not in ('normal', 'uniform'):\n             raise ValueError(\"'output_distribution' has to be either 'normal'\"\n                              \" or 'uniform'. Got '{}' instead.\".format(\n@@ -2379,10 +2386,12 @@ def quantile_transform(X, axis=0, n_quantiles=1000,\n     to spread out the most frequent values. It also reduces the impact of\n     (marginal) outliers: this is therefore a robust preprocessing scheme.\n \n-    The transformation is applied on each feature independently.\n-    The cumulative distribution function of a feature is used to project the\n-    original values. Features values of new/unseen data that fall below\n-    or above the fitted range will be mapped to the bounds of the output\n+    The transformation is applied on each feature independently. First an\n+    estimate of the cumulative distribution function of a feature is\n+    used to map the original values to a uniform distribution. The obtained\n+    values are then mapped to the desired output distribution using the\n+    associated quantile function. Features values of new/unseen data that fall\n+    below or above the fitted range will be mapped to the bounds of the output\n     distribution. Note that this transform is non-linear. It may distort linear\n     correlations between variables measured at the same scale but renders\n     variables measured at different scales more directly comparable.\n",
  "test_patch": "diff --git a/sklearn/preprocessing/tests/test_data.py b/sklearn/preprocessing/tests/test_data.py\n--- a/sklearn/preprocessing/tests/test_data.py\n+++ b/sklearn/preprocessing/tests/test_data.py\n@@ -54,6 +54,7 @@\n from sklearn.preprocessing.data import PolynomialFeatures\n from sklearn.preprocessing.data import PowerTransformer\n from sklearn.preprocessing.data import power_transform\n+from sklearn.preprocessing.data import BOUNDS_THRESHOLD\n from sklearn.exceptions import DataConversionWarning, NotFittedError\n \n from sklearn.base import clone\n@@ -1471,12 +1472,13 @@ def test_quantile_transform_bounds():\n \n \n def test_quantile_transform_and_inverse():\n-    # iris dataset\n-    X = iris.data\n-    transformer = QuantileTransformer(n_quantiles=1000, random_state=0)\n-    X_trans = transformer.fit_transform(X)\n-    X_trans_inv = transformer.inverse_transform(X_trans)\n-    assert_array_almost_equal(X, X_trans_inv)\n+    X_1 = iris.data\n+    X_2 = np.array([[0.], [BOUNDS_THRESHOLD / 10], [1.5], [2], [3], [3], [4]])\n+    for X in [X_1, X_2]:\n+        transformer = QuantileTransformer(n_quantiles=1000, random_state=0)\n+        X_trans = transformer.fit_transform(X)\n+        X_trans_inv = transformer.inverse_transform(X_trans)\n+        assert_array_almost_equal(X, X_trans_inv, decimal=9)\n \n \n def test_quantile_transform_nan():\n",
  "problem_statement": "DOC: add details to QuantileTransformer documentation\nI think the documentation of `QuantileTransformer` should say how it is implemented. There is even a [stats.stackexchange question](https://stats.stackexchange.com/questions/325570/quantile-transformation-with-gaussian-distribution-sklearn-implementation/327102#327102) about it and we could take some elements of the answer.\r\n\r\nBesides I was thinking that to map to a uniform distribution, the implementation was just computing the empirical cdf of the columns but it does not seem to be the case.\r\n\n",
  "hints_text": "Sounds like a good improvement. Could you maybe submit a PR with the improvements you have in mind? I guess it should go in the relevant [user guide](https://scikit-learn.org/dev/modules/preprocessing.html#preprocessing-transformer).\nI will open a PR with the improvements of the doc. \r\n\r\n> Besides I was thinking that to map to a uniform distribution, the implementation was just computing the empirical cdf of the columns but it does not seem to be the case.\r\n\r\nI investigated the code a bit more. I think that when `output_distribution='uniform'` the statement [L2250](https://github.com/scikit-learn/scikit-learn/blob/cd8fe16/sklearn/preprocessing/data.py#L2250)\r\n\r\n```python\r\nX_col = output_distribution.ppf(X_col)\r\n```\r\n\r\nis not needed as for a uniform distribution on (0, 1) the quantile function `ppf` is the identity function. In this case the implementation would just compute the empirical cdf which would correspond to what I think it should be doing. Unless there was a reason to use `ppf` even for the uniform output? cc @glemaitre @ogrisel\r\n\r\nPS: I also had a look at the `QuantileTransformer` PR, this is impressive work!\r\n\r\n",
  "created_at": "2018-12-19T09:17:59Z",
  "version": "0.21",
  "FAIL_TO_PASS": "[\"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_and_inverse\"]",
  "PASS_TO_PASS": "[\"sklearn/preprocessing/tests/test_data.py::test_polynomial_features\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_feature_names\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_feature_array_order\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[1-True-False-int]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[2-True-False-int]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[2-True-False-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[2-True-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[3-False-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[3-False-True-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[4-False-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[4-False-True-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X[1-True-False-int]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X[2-True-False-int]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X[2-True-False-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X[2-True-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X[3-False-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X[3-False-True-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_floats[2-True-False-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_floats[2-True-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_floats[3-False-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_floats[3-False-True-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[0-2-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[1-2-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[2-2-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[0-3-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[1-3-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[2-3-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[0-2-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[1-2-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[2-2-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[0-3-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[1-3-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[2-3-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_degree_4[True-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_degree_4[True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_degree_4[False-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_degree_4[False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[2-1-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[2-2-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[3-1-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[3-2-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[3-3-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[2-1-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[2-2-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[3-1-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[3-2-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[3-3-False]\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_1d\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_dtype\", \"sklearn/preprocessing/tests/test_data.py::test_scale_1d\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_numerical_stability\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_2d_arrays\", \"sklearn/preprocessing/tests/test_data.py::test_handle_zeros_in_scale\", \"sklearn/preprocessing/tests/test_data.py::test_minmax_scaler_partial_fit\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_partial_fit\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_partial_fit_numerical_stability\", \"sklearn/preprocessing/tests/test_data.py::test_partial_fit_sparse_input\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_trasform_with_partial_fit\", \"sklearn/preprocessing/tests/test_data.py::test_min_max_scaler_iris\", \"sklearn/preprocessing/tests/test_data.py::test_min_max_scaler_zero_variance_features\", \"sklearn/preprocessing/tests/test_data.py::test_minmax_scale_axis1\", \"sklearn/preprocessing/tests/test_data.py::test_min_max_scaler_1d\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_without_centering\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[asarray-True-True]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[asarray-True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[asarray-False-True]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[asarray-False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[csc_matrix-True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[csc_matrix-False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[csr_matrix-True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[csr_matrix-False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_return_identity\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_int\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_without_copy\", \"sklearn/preprocessing/tests/test_data.py::test_scale_sparse_with_mean_raise_exception\", \"sklearn/preprocessing/tests/test_data.py::test_scale_input_finiteness_validation\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_error_sparse\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_attributes[X0-True-True]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_attributes[X0-True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_attributes[X0-False-True]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_attributes[X0-False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_attributes[X1-True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_attributes[X1-False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_col_zero_sparse\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_2d_arrays\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[positive-0]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[positive-0.05]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[positive-0.1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[positive-0.5]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[positive-1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[negative-0]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[negative-0.05]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[negative-0.1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[negative-0.5]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[negative-1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[zeros-0]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[zeros-0.05]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[zeros-0.1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[zeros-0.5]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[zeros-1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[None-0]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[None-0.05]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[None-0.1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[None-0.5]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[None-1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_transform_one_row_csr\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_iris\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_iris_quantiles\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_iris\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_check_error\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_sparse_ignore_zeros\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_dense_toy\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_subsampling\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_sparse_toy\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_axis1\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_bounds\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_nan\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_invalid_range\", \"sklearn/preprocessing/tests/test_data.py::test_scale_function_without_centering\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scale_axis1\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scale_1d_array\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_zero_variance_features\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_zero_variance_features\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_large_negative_value\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_transform_one_row_csr\", \"sklearn/preprocessing/tests/test_data.py::test_warning_scaling_integers\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_1d\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_partial_fit\", \"sklearn/preprocessing/tests/test_data.py::test_normalizer_l1\", \"sklearn/preprocessing/tests/test_data.py::test_normalizer_l2\", \"sklearn/preprocessing/tests/test_data.py::test_normalizer_max\", \"sklearn/preprocessing/tests/test_data.py::test_normalize\", \"sklearn/preprocessing/tests/test_data.py::test_binarizer\", \"sklearn/preprocessing/tests/test_data.py::test_center_kernel\", \"sklearn/preprocessing/tests/test_data.py::test_cv_pipeline_precomputed\", \"sklearn/preprocessing/tests/test_data.py::test_fit_transform\", \"sklearn/preprocessing/tests/test_data.py::test_add_dummy_feature\", \"sklearn/preprocessing/tests/test_data.py::test_add_dummy_feature_coo\", \"sklearn/preprocessing/tests/test_data.py::test_add_dummy_feature_csc\", \"sklearn/preprocessing/tests/test_data.py::test_add_dummy_feature_csr\", \"sklearn/preprocessing/tests/test_data.py::test_fit_cold_start\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_valid_axis\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_notfitted[box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_notfitted[yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X0-True-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X0-True-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X0-False-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X0-False-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X1-True-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X1-True-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X1-False-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X1-False-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_1d\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_2d\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_boxcox_strictly_positive_exception\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_yeojohnson_any_input[X0]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_yeojohnson_any_input[X1]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_yeojohnson_any_input[X2]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_yeojohnson_any_input[X3]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_shape_exception[box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_shape_exception[yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_method_exception\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_lambda_zero\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_lambda_one\", \"sklearn/preprocessing/tests/test_data.py::test_optimization_power_transformer[box-cox-0.1]\", \"sklearn/preprocessing/tests/test_data.py::test_optimization_power_transformer[box-cox-0.5]\", \"sklearn/preprocessing/tests/test_data.py::test_optimization_power_transformer[yeo-johnson-0.1]\", \"sklearn/preprocessing/tests/test_data.py::test_optimization_power_transformer[yeo-johnson-0.5]\", \"sklearn/preprocessing/tests/test_data.py::test_optimization_power_transformer[yeo-johnson-1.0]\", \"sklearn/preprocessing/tests/test_data.py::test_yeo_johnson_darwin_example\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_fit_transform[True-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_fit_transform[True-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_fit_transform[False-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_fit_transform[False-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_True[True-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_True[True-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_True[False-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_True[False-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_False[True-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_False[True-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_False[False-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_False[False-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transform_default_method\"]",
  "environment_setup_commit": "7813f7efb5b2012412888b69e73d76f2df2b50b6",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.968291",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}