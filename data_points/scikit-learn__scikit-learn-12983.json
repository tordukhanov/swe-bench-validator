{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-12983",
  "base_commit": "a547311b5faae0809b8935e1f1d00ff901109f84",
  "patch": "diff --git a/sklearn/ensemble/_gb_losses.py b/sklearn/ensemble/_gb_losses.py\nnew file mode 100644\n--- /dev/null\n+++ b/sklearn/ensemble/_gb_losses.py\n@@ -0,0 +1,884 @@\n+\"\"\"Losses and corresponding default initial estimators for gradient boosting\n+decision trees.\n+\"\"\"\n+\n+from abc import ABCMeta\n+from abc import abstractmethod\n+\n+import numpy as np\n+from scipy.special import expit\n+\n+from ..tree._tree import TREE_LEAF\n+from ..utils.fixes import logsumexp\n+from ..utils.stats import _weighted_percentile\n+from ..dummy import DummyClassifier\n+from ..dummy import DummyRegressor\n+\n+\n+class LossFunction(metaclass=ABCMeta):\n+    \"\"\"Abstract base class for various loss functions.\n+\n+    Parameters\n+    ----------\n+    n_classes : int\n+        Number of classes.\n+\n+    Attributes\n+    ----------\n+    K : int\n+        The number of regression trees to be induced;\n+        1 for regression and binary classification;\n+        ``n_classes`` for multi-class classification.\n+    \"\"\"\n+\n+    is_multi_class = False\n+\n+    def __init__(self, n_classes):\n+        self.K = n_classes\n+\n+    def init_estimator(self):\n+        \"\"\"Default ``init`` estimator for loss function. \"\"\"\n+        raise NotImplementedError()\n+\n+    @abstractmethod\n+    def __call__(self, y, raw_predictions, sample_weight=None):\n+        \"\"\"Compute the loss.\n+\n+        Parameters\n+        ----------\n+        y : 1d array, shape (n_samples,)\n+            True labels.\n+\n+        raw_predictions : 2d array, shape (n_samples, K)\n+            The raw predictions (i.e. values from the tree leaves).\n+\n+        sample_weight : 1d array, shape (n_samples,), optional\n+            Sample weights.\n+        \"\"\"\n+\n+    @abstractmethod\n+    def negative_gradient(self, y, raw_predictions, **kargs):\n+        \"\"\"Compute the negative gradient.\n+\n+        Parameters\n+        ----------\n+        y : 1d array, shape (n_samples,)\n+            The target labels.\n+\n+        raw_predictions : 2d array, shape (n_samples, K)\n+            The raw predictions (i.e. values from the tree leaves) of the\n+            tree ensemble at iteration ``i - 1``.\n+        \"\"\"\n+\n+    def update_terminal_regions(self, tree, X, y, residual, raw_predictions,\n+                                sample_weight, sample_mask,\n+                                learning_rate=0.1, k=0):\n+        \"\"\"Update the terminal regions (=leaves) of the given tree and\n+        updates the current predictions of the model. Traverses tree\n+        and invokes template method `_update_terminal_region`.\n+\n+        Parameters\n+        ----------\n+        tree : tree.Tree\n+            The tree object.\n+        X : 2d array, shape (n, m)\n+            The data array.\n+        y : 1d array, shape (n,)\n+            The target labels.\n+        residual : 1d array, shape (n,)\n+            The residuals (usually the negative gradient).\n+        raw_predictions : 2d array, shape (n_samples, K)\n+            The raw predictions (i.e. values from the tree leaves) of the\n+            tree ensemble at iteration ``i - 1``.\n+        sample_weight : 1d array, shape (n,)\n+            The weight of each sample.\n+        sample_mask : 1d array, shape (n,)\n+            The sample mask to be used.\n+        learning_rate : float, default=0.1\n+            Learning rate shrinks the contribution of each tree by\n+             ``learning_rate``.\n+        k : int, default=0\n+            The index of the estimator being updated.\n+\n+        \"\"\"\n+        # compute leaf for each sample in ``X``.\n+        terminal_regions = tree.apply(X)\n+\n+        # mask all which are not in sample mask.\n+        masked_terminal_regions = terminal_regions.copy()\n+        masked_terminal_regions[~sample_mask] = -1\n+\n+        # update each leaf (= perform line search)\n+        for leaf in np.where(tree.children_left == TREE_LEAF)[0]:\n+            self._update_terminal_region(tree, masked_terminal_regions,\n+                                         leaf, X, y, residual,\n+                                         raw_predictions[:, k], sample_weight)\n+\n+        # update predictions (both in-bag and out-of-bag)\n+        raw_predictions[:, k] += \\\n+            learning_rate * tree.value[:, 0, 0].take(terminal_regions, axis=0)\n+\n+    @abstractmethod\n+    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n+                                residual, raw_predictions, sample_weight):\n+        \"\"\"Template method for updating terminal regions (i.e., leaves).\"\"\"\n+\n+    @abstractmethod\n+    def get_init_raw_predictions(self, X, estimator):\n+        \"\"\"Return the initial raw predictions.\n+\n+        Parameters\n+        ----------\n+        X : 2d array, shape (n_samples, n_features)\n+            The data array.\n+        estimator : estimator instance\n+            The estimator to use to compute the predictions.\n+\n+        Returns\n+        -------\n+        raw_predictions : 2d array, shape (n_samples, K)\n+            The initial raw predictions. K is equal to 1 for binary\n+            classification and regression, and equal to the number of classes\n+            for multiclass classification. ``raw_predictions`` is casted\n+            into float64.\n+        \"\"\"\n+        pass\n+\n+\n+class RegressionLossFunction(LossFunction, metaclass=ABCMeta):\n+    \"\"\"Base class for regression loss functions.\n+\n+    Parameters\n+    ----------\n+    n_classes : int\n+        Number of classes.\n+    \"\"\"\n+    def __init__(self, n_classes):\n+        if n_classes != 1:\n+            raise ValueError(\"``n_classes`` must be 1 for regression but \"\n+                             \"was %r\" % n_classes)\n+        super().__init__(n_classes)\n+\n+    def check_init_estimator(self, estimator):\n+        \"\"\"Make sure estimator has the required fit and predict methods.\n+\n+        Parameters\n+        ----------\n+        estimator : estimator instance\n+            The init estimator to check.\n+        \"\"\"\n+        if not (hasattr(estimator, 'fit') and hasattr(estimator, 'predict')):\n+            raise ValueError(\n+                \"The init parameter must be a valid estimator and \"\n+                \"support both fit and predict.\"\n+            )\n+\n+    def get_init_raw_predictions(self, X, estimator):\n+        predictions = estimator.predict(X)\n+        return predictions.reshape(-1, 1).astype(np.float64)\n+\n+\n+class LeastSquaresError(RegressionLossFunction):\n+    \"\"\"Loss function for least squares (LS) estimation.\n+    Terminal regions do not need to be updated for least squares.\n+\n+    Parameters\n+    ----------\n+    n_classes : int\n+        Number of classes.\n+    \"\"\"\n+\n+    def init_estimator(self):\n+        return DummyRegressor(strategy='mean')\n+\n+    def __call__(self, y, raw_predictions, sample_weight=None):\n+        \"\"\"Compute the least squares loss.\n+\n+        Parameters\n+        ----------\n+        y : 1d array, shape (n_samples,)\n+            True labels.\n+\n+        raw_predictions : 2d array, shape (n_samples, K)\n+            The raw_predictions (i.e. values from the tree leaves).\n+\n+        sample_weight : 1d array, shape (n_samples,), optional\n+            Sample weights.\n+        \"\"\"\n+        if sample_weight is None:\n+            return np.mean((y - raw_predictions.ravel()) ** 2)\n+        else:\n+            return (1 / sample_weight.sum() * np.sum(\n+                sample_weight * ((y - raw_predictions.ravel()) ** 2)))\n+\n+    def negative_gradient(self, y, raw_predictions, **kargs):\n+        \"\"\"Compute the negative gradient.\n+\n+        Parameters\n+        ----------\n+        y : 1d array, shape (n_samples,)\n+            The target labels.\n+\n+        raw_predictions : 1d array, shape (n_samples,)\n+            The raw predictions (i.e. values from the tree leaves) of the\n+            tree ensemble at iteration ``i - 1``.\n+        \"\"\"\n+        return y - raw_predictions.ravel()\n+\n+    def update_terminal_regions(self, tree, X, y, residual, raw_predictions,\n+                                sample_weight, sample_mask,\n+                                learning_rate=0.1, k=0):\n+        \"\"\"Least squares does not need to update terminal regions.\n+\n+        But it has to update the predictions.\n+\n+        Parameters\n+        ----------\n+        tree : tree.Tree\n+            The tree object.\n+        X : 2d array, shape (n, m)\n+            The data array.\n+        y : 1d array, shape (n,)\n+            The target labels.\n+        residual : 1d array, shape (n,)\n+            The residuals (usually the negative gradient).\n+        raw_predictions : 2d array, shape (n_samples, K)\n+            The raw predictions (i.e. values from the tree leaves) of the\n+            tree ensemble at iteration ``i - 1``.\n+        sample_weight : 1d array, shape (n,)\n+            The weight of each sample.\n+        sample_mask : 1d array, shape (n,)\n+            The sample mask to be used.\n+        learning_rate : float, default=0.1\n+            Learning rate shrinks the contribution of each tree by\n+             ``learning_rate``.\n+        k : int, default=0\n+            The index of the estimator being updated.\n+        \"\"\"\n+        # update predictions\n+        raw_predictions[:, k] += learning_rate * tree.predict(X).ravel()\n+\n+    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n+                                residual, raw_predictions, sample_weight):\n+        pass\n+\n+\n+class LeastAbsoluteError(RegressionLossFunction):\n+    \"\"\"Loss function for least absolute deviation (LAD) regression.\n+\n+    Parameters\n+    ----------\n+    n_classes : int\n+        Number of classes\n+    \"\"\"\n+    def init_estimator(self):\n+        return DummyRegressor(strategy='quantile', quantile=.5)\n+\n+    def __call__(self, y, raw_predictions, sample_weight=None):\n+        \"\"\"Compute the least absolute error.\n+\n+        Parameters\n+        ----------\n+        y : array, shape (n_samples,)\n+            True labels.\n+\n+        raw_predictions : array, shape (n_samples, K)\n+            The raw_predictions (i.e. values from the tree leaves).\n+\n+        sample_weight : 1d array, shape (n_samples,), optional\n+            Sample weights.\n+        \"\"\"\n+        if sample_weight is None:\n+            return np.abs(y - raw_predictions.ravel()).mean()\n+        else:\n+            return (1 / sample_weight.sum() * np.sum(\n+                sample_weight * np.abs(y - raw_predictions.ravel())))\n+\n+    def negative_gradient(self, y, raw_predictions, **kargs):\n+        \"\"\"Compute the negative gradient.\n+\n+        1.0 if y - raw_predictions > 0.0 else -1.0\n+\n+        Parameters\n+        ----------\n+        y : 1d array, shape (n_samples,)\n+            The target labels.\n+\n+        raw_predictions : array, shape (n_samples, K)\n+            The raw predictions (i.e. values from the tree leaves) of the\n+            tree ensemble at iteration ``i - 1``.\n+        \"\"\"\n+        raw_predictions = raw_predictions.ravel()\n+        return 2 * (y - raw_predictions > 0) - 1\n+\n+    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n+                                residual, raw_predictions, sample_weight):\n+        \"\"\"LAD updates terminal regions to median estimates.\"\"\"\n+        terminal_region = np.where(terminal_regions == leaf)[0]\n+        sample_weight = sample_weight.take(terminal_region, axis=0)\n+        diff = (y.take(terminal_region, axis=0) -\n+                raw_predictions.take(terminal_region, axis=0))\n+        tree.value[leaf, 0, 0] = _weighted_percentile(diff, sample_weight,\n+                                                      percentile=50)\n+\n+\n+class HuberLossFunction(RegressionLossFunction):\n+    \"\"\"Huber loss function for robust regression.\n+\n+    M-Regression proposed in Friedman 2001.\n+\n+    References\n+    ----------\n+    J. Friedman, Greedy Function Approximation: A Gradient Boosting\n+    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n+\n+    Parameters\n+    ----------\n+    n_classes : int\n+        Number of classes.\n+\n+    alpha : float, default=0.9\n+        Percentile at which to extract score.\n+    \"\"\"\n+\n+    def __init__(self, n_classes, alpha=0.9):\n+        super().__init__(n_classes)\n+        self.alpha = alpha\n+        self.gamma = None\n+\n+    def init_estimator(self):\n+        return DummyRegressor(strategy='quantile', quantile=.5)\n+\n+    def __call__(self, y, raw_predictions, sample_weight=None):\n+        \"\"\"Compute the Huber loss.\n+\n+        Parameters\n+        ----------\n+        y : 1d array, shape (n_samples,)\n+            True labels.\n+\n+        raw_predictions : 2d array, shape (n_samples, K)\n+            The raw predictions (i.e. values from the tree leaves) of the\n+            tree ensemble.\n+\n+        sample_weight : 1d array, shape (n_samples,), optional\n+            Sample weights.\n+        \"\"\"\n+        raw_predictions = raw_predictions.ravel()\n+        diff = y - raw_predictions\n+        gamma = self.gamma\n+        if gamma is None:\n+            if sample_weight is None:\n+                gamma = np.percentile(np.abs(diff), self.alpha * 100)\n+            else:\n+                gamma = _weighted_percentile(np.abs(diff), sample_weight,\n+                                             self.alpha * 100)\n+\n+        gamma_mask = np.abs(diff) <= gamma\n+        if sample_weight is None:\n+            sq_loss = np.sum(0.5 * diff[gamma_mask] ** 2)\n+            lin_loss = np.sum(gamma * (np.abs(diff[~gamma_mask]) -\n+                                       gamma / 2))\n+            loss = (sq_loss + lin_loss) / y.shape[0]\n+        else:\n+            sq_loss = np.sum(0.5 * sample_weight[gamma_mask] *\n+                             diff[gamma_mask] ** 2)\n+            lin_loss = np.sum(gamma * sample_weight[~gamma_mask] *\n+                              (np.abs(diff[~gamma_mask]) - gamma / 2))\n+            loss = (sq_loss + lin_loss) / sample_weight.sum()\n+        return loss\n+\n+    def negative_gradient(self, y, raw_predictions, sample_weight=None,\n+                          **kargs):\n+        \"\"\"Compute the negative gradient.\n+\n+        Parameters\n+        ----------\n+        y : 1d array, shape (n_samples,)\n+            The target labels.\n+\n+        raw_predictions : 2d array, shape (n_samples, K)\n+            The raw predictions (i.e. values from the tree leaves) of the\n+            tree ensemble at iteration ``i - 1``.\n+\n+        sample_weight : 1d array, shape (n_samples,), optional\n+            Sample weights.\n+        \"\"\"\n+        raw_predictions = raw_predictions.ravel()\n+        diff = y - raw_predictions\n+        if sample_weight is None:\n+            gamma = np.percentile(np.abs(diff), self.alpha * 100)\n+        else:\n+            gamma = _weighted_percentile(np.abs(diff), sample_weight,\n+                                         self.alpha * 100)\n+        gamma_mask = np.abs(diff) <= gamma\n+        residual = np.zeros((y.shape[0],), dtype=np.float64)\n+        residual[gamma_mask] = diff[gamma_mask]\n+        residual[~gamma_mask] = gamma * np.sign(diff[~gamma_mask])\n+        self.gamma = gamma\n+        return residual\n+\n+    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n+                                residual, raw_predictions, sample_weight):\n+        terminal_region = np.where(terminal_regions == leaf)[0]\n+        sample_weight = sample_weight.take(terminal_region, axis=0)\n+        gamma = self.gamma\n+        diff = (y.take(terminal_region, axis=0)\n+                - raw_predictions.take(terminal_region, axis=0))\n+        median = _weighted_percentile(diff, sample_weight, percentile=50)\n+        diff_minus_median = diff - median\n+        tree.value[leaf, 0] = median + np.mean(\n+            np.sign(diff_minus_median) *\n+            np.minimum(np.abs(diff_minus_median), gamma))\n+\n+\n+class QuantileLossFunction(RegressionLossFunction):\n+    \"\"\"Loss function for quantile regression.\n+\n+    Quantile regression allows to estimate the percentiles\n+    of the conditional distribution of the target.\n+\n+    Parameters\n+    ----------\n+    n_classes : int\n+        Number of classes.\n+\n+    alpha : float, optional (default = 0.9)\n+        The percentile.\n+    \"\"\"\n+    def __init__(self, n_classes, alpha=0.9):\n+        super().__init__(n_classes)\n+        self.alpha = alpha\n+        self.percentile = alpha * 100\n+\n+    def init_estimator(self):\n+        return DummyRegressor(strategy='quantile', quantile=self.alpha)\n+\n+    def __call__(self, y, raw_predictions, sample_weight=None):\n+        \"\"\"Compute the Quantile loss.\n+\n+        Parameters\n+        ----------\n+        y : 1d array, shape (n_samples,)\n+            True labels.\n+\n+        raw_predictions : 2d array, shape (n_samples, K)\n+            The raw predictions (i.e. values from the tree leaves) of the\n+            tree ensemble.\n+\n+        sample_weight : 1d array, shape (n_samples,), optional\n+            Sample weights.\n+        \"\"\"\n+        raw_predictions = raw_predictions.ravel()\n+        diff = y - raw_predictions\n+        alpha = self.alpha\n+\n+        mask = y > raw_predictions\n+        if sample_weight is None:\n+            loss = (alpha * diff[mask].sum() -\n+                    (1 - alpha) * diff[~mask].sum()) / y.shape[0]\n+        else:\n+            loss = ((alpha * np.sum(sample_weight[mask] * diff[mask]) -\n+                    (1 - alpha) * np.sum(sample_weight[~mask] *\n+                                         diff[~mask])) / sample_weight.sum())\n+        return loss\n+\n+    def negative_gradient(self, y, raw_predictions, **kargs):\n+        \"\"\"Compute the negative gradient.\n+\n+        Parameters\n+        ----------\n+        y : 1d array, shape (n_samples,)\n+            The target labels.\n+\n+        raw_predictions : 2d array, shape (n_samples, K)\n+            The raw_predictions (i.e. values from the tree leaves) of the\n+            tree ensemble at iteration ``i - 1``.\n+        \"\"\"\n+        alpha = self.alpha\n+        raw_predictions = raw_predictions.ravel()\n+        mask = y > raw_predictions\n+        return (alpha * mask) - ((1 - alpha) * ~mask)\n+\n+    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n+                                residual, raw_predictions, sample_weight):\n+        terminal_region = np.where(terminal_regions == leaf)[0]\n+        diff = (y.take(terminal_region, axis=0)\n+                - raw_predictions.take(terminal_region, axis=0))\n+        sample_weight = sample_weight.take(terminal_region, axis=0)\n+\n+        val = _weighted_percentile(diff, sample_weight, self.percentile)\n+        tree.value[leaf, 0] = val\n+\n+\n+class ClassificationLossFunction(LossFunction, metaclass=ABCMeta):\n+    \"\"\"Base class for classification loss functions. \"\"\"\n+\n+    def _raw_prediction_to_proba(self, raw_predictions):\n+        \"\"\"Template method to convert raw predictions into probabilities.\n+\n+        Parameters\n+        ----------\n+        raw_predictions : 2d array, shape (n_samples, K)\n+            The raw predictions (i.e. values from the tree leaves) of the\n+            tree ensemble.\n+\n+        Returns\n+        -------\n+        probas : 2d array, shape (n_samples, K)\n+            The predicted probabilities.\n+        \"\"\"\n+\n+    @abstractmethod\n+    def _raw_prediction_to_decision(self, raw_predictions):\n+        \"\"\"Template method to convert raw predictions to decisions.\n+\n+        Parameters\n+        ----------\n+        raw_predictions : 2d array, shape (n_samples, K)\n+            The raw predictions (i.e. values from the tree leaves) of the\n+            tree ensemble.\n+\n+        Returns\n+        -------\n+        encoded_predictions : 2d array, shape (n_samples, K)\n+            The predicted encoded labels.\n+        \"\"\"\n+\n+    def check_init_estimator(self, estimator):\n+        \"\"\"Make sure estimator has fit and predict_proba methods.\n+\n+        Parameters\n+        ----------\n+        estimator : estimator instance\n+            The init estimator to check.\n+        \"\"\"\n+        if not (hasattr(estimator, 'fit') and\n+                hasattr(estimator, 'predict_proba')):\n+            raise ValueError(\n+                \"The init parameter must be a valid estimator \"\n+                \"and support both fit and predict_proba.\"\n+            )\n+\n+\n+class BinomialDeviance(ClassificationLossFunction):\n+    \"\"\"Binomial deviance loss function for binary classification.\n+\n+    Binary classification is a special case; here, we only need to\n+    fit one tree instead of ``n_classes`` trees.\n+\n+    Parameters\n+    ----------\n+    n_classes : int\n+        Number of classes.\n+    \"\"\"\n+    def __init__(self, n_classes):\n+        if n_classes != 2:\n+            raise ValueError(\"{0:s} requires 2 classes; got {1:d} class(es)\"\n+                             .format(self.__class__.__name__, n_classes))\n+        # we only need to fit one tree for binary clf.\n+        super().__init__(n_classes=1)\n+\n+    def init_estimator(self):\n+        # return the most common class, taking into account the samples\n+        # weights\n+        return DummyClassifier(strategy='prior')\n+\n+    def __call__(self, y, raw_predictions, sample_weight=None):\n+        \"\"\"Compute the deviance (= 2 * negative log-likelihood).\n+\n+        Parameters\n+        ----------\n+        y : 1d array, shape (n_samples,)\n+            True labels.\n+\n+        raw_predictions : 2d array, shape (n_samples, K)\n+            The raw predictions (i.e. values from the tree leaves) of the\n+            tree ensemble.\n+\n+        sample_weight : 1d array , shape (n_samples,), optional\n+            Sample weights.\n+        \"\"\"\n+        # logaddexp(0, v) == log(1.0 + exp(v))\n+        raw_predictions = raw_predictions.ravel()\n+        if sample_weight is None:\n+            return -2 * np.mean((y * raw_predictions) -\n+                                np.logaddexp(0, raw_predictions))\n+        else:\n+            return (-2 / sample_weight.sum() * np.sum(\n+                sample_weight * ((y * raw_predictions) -\n+                                 np.logaddexp(0, raw_predictions))))\n+\n+    def negative_gradient(self, y, raw_predictions, **kargs):\n+        \"\"\"Compute the residual (= negative gradient).\n+\n+        Parameters\n+        ----------\n+        y : 1d array, shape (n_samples,)\n+            True labels.\n+\n+        raw_predictions : 2d array, shape (n_samples, K)\n+            The raw_predictions (i.e. values from the tree leaves) of the\n+            tree ensemble at iteration ``i - 1``.\n+        \"\"\"\n+        return y - expit(raw_predictions.ravel())\n+\n+    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n+                                residual, raw_predictions, sample_weight):\n+        \"\"\"Make a single Newton-Raphson step.\n+\n+        our node estimate is given by:\n+\n+            sum(w * (y - prob)) / sum(w * prob * (1 - prob))\n+\n+        we take advantage that: y - prob = residual\n+        \"\"\"\n+        terminal_region = np.where(terminal_regions == leaf)[0]\n+        residual = residual.take(terminal_region, axis=0)\n+        y = y.take(terminal_region, axis=0)\n+        sample_weight = sample_weight.take(terminal_region, axis=0)\n+\n+        numerator = np.sum(sample_weight * residual)\n+        denominator = np.sum(sample_weight *\n+                             (y - residual) * (1 - y + residual))\n+\n+        # prevents overflow and division by zero\n+        if abs(denominator) < 1e-150:\n+            tree.value[leaf, 0, 0] = 0.0\n+        else:\n+            tree.value[leaf, 0, 0] = numerator / denominator\n+\n+    def _raw_prediction_to_proba(self, raw_predictions):\n+        proba = np.ones((raw_predictions.shape[0], 2), dtype=np.float64)\n+        proba[:, 1] = expit(raw_predictions.ravel())\n+        proba[:, 0] -= proba[:, 1]\n+        return proba\n+\n+    def _raw_prediction_to_decision(self, raw_predictions):\n+        proba = self._raw_prediction_to_proba(raw_predictions)\n+        return np.argmax(proba, axis=1)\n+\n+    def get_init_raw_predictions(self, X, estimator):\n+        probas = estimator.predict_proba(X)\n+        proba_pos_class = probas[:, 1]\n+        eps = np.finfo(np.float32).eps\n+        proba_pos_class = np.clip(proba_pos_class, eps, 1 - eps)\n+        # log(x / (1 - x)) is the inverse of the sigmoid (expit) function\n+        raw_predictions = np.log(proba_pos_class / (1 - proba_pos_class))\n+        return raw_predictions.reshape(-1, 1).astype(np.float64)\n+\n+\n+class MultinomialDeviance(ClassificationLossFunction):\n+    \"\"\"Multinomial deviance loss function for multi-class classification.\n+\n+    For multi-class classification we need to fit ``n_classes`` trees at\n+    each stage.\n+\n+    Parameters\n+    ----------\n+    n_classes : int\n+        Number of classes.\n+    \"\"\"\n+\n+    is_multi_class = True\n+\n+    def __init__(self, n_classes):\n+        if n_classes < 3:\n+            raise ValueError(\"{0:s} requires more than 2 classes.\".format(\n+                self.__class__.__name__))\n+        super().__init__(n_classes)\n+\n+    def init_estimator(self):\n+        return DummyClassifier(strategy='prior')\n+\n+    def __call__(self, y, raw_predictions, sample_weight=None):\n+        \"\"\"Compute the Multinomial deviance.\n+\n+        Parameters\n+        ----------\n+        y : 1d array, shape (n_samples,)\n+            True labels.\n+\n+        raw_predictions : 2d array, shape (n_samples, K)\n+            The raw predictions (i.e. values from the tree leaves) of the\n+            tree ensemble.\n+\n+        sample_weight : 1d array, shape (n_samples,), optional\n+            Sample weights.\n+        \"\"\"\n+        # create one-hot label encoding\n+        Y = np.zeros((y.shape[0], self.K), dtype=np.float64)\n+        for k in range(self.K):\n+            Y[:, k] = y == k\n+\n+        if sample_weight is None:\n+            return np.sum(-1 * (Y * raw_predictions).sum(axis=1) +\n+                          logsumexp(raw_predictions, axis=1))\n+        else:\n+            return np.sum(\n+                -1 * sample_weight * (Y * raw_predictions).sum(axis=1) +\n+                logsumexp(raw_predictions, axis=1))\n+\n+    def negative_gradient(self, y, raw_predictions, k=0, **kwargs):\n+        \"\"\"Compute negative gradient for the ``k``-th class.\n+\n+        Parameters\n+        ----------\n+        y : 1d array, shape (n_samples,)\n+            The target labels.\n+\n+        raw_predictions : 2d array, shape (n_samples, K)\n+            The raw_predictions (i.e. values from the tree leaves) of the\n+            tree ensemble at iteration ``i - 1``.\n+\n+        k : int, optional default=0\n+            The index of the class.\n+        \"\"\"\n+        return y - np.nan_to_num(np.exp(raw_predictions[:, k] -\n+                                        logsumexp(raw_predictions, axis=1)))\n+\n+    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n+                                residual, raw_predictions, sample_weight):\n+        \"\"\"Make a single Newton-Raphson step. \"\"\"\n+        terminal_region = np.where(terminal_regions == leaf)[0]\n+        residual = residual.take(terminal_region, axis=0)\n+        y = y.take(terminal_region, axis=0)\n+        sample_weight = sample_weight.take(terminal_region, axis=0)\n+\n+        numerator = np.sum(sample_weight * residual)\n+        numerator *= (self.K - 1) / self.K\n+\n+        denominator = np.sum(sample_weight * (y - residual) *\n+                             (1 - y + residual))\n+\n+        # prevents overflow and division by zero\n+        if abs(denominator) < 1e-150:\n+            tree.value[leaf, 0, 0] = 0.0\n+        else:\n+            tree.value[leaf, 0, 0] = numerator / denominator\n+\n+    def _raw_prediction_to_proba(self, raw_predictions):\n+        return np.nan_to_num(\n+            np.exp(raw_predictions -\n+                   (logsumexp(raw_predictions, axis=1)[:, np.newaxis])))\n+\n+    def _raw_prediction_to_decision(self, raw_predictions):\n+        proba = self._raw_prediction_to_proba(raw_predictions)\n+        return np.argmax(proba, axis=1)\n+\n+    def get_init_raw_predictions(self, X, estimator):\n+        probas = estimator.predict_proba(X)\n+        eps = np.finfo(np.float32).eps\n+        probas = np.clip(probas, eps, 1 - eps)\n+        raw_predictions = np.log(probas).astype(np.float64)\n+        return raw_predictions\n+\n+\n+class ExponentialLoss(ClassificationLossFunction):\n+    \"\"\"Exponential loss function for binary classification.\n+\n+    Same loss as AdaBoost.\n+\n+    References\n+    ----------\n+    Greg Ridgeway, Generalized Boosted Models: A guide to the gbm package, 2007\n+\n+    Parameters\n+    ----------\n+    n_classes : int\n+        Number of classes.\n+    \"\"\"\n+    def __init__(self, n_classes):\n+        if n_classes != 2:\n+            raise ValueError(\"{0:s} requires 2 classes; got {1:d} class(es)\"\n+                             .format(self.__class__.__name__, n_classes))\n+        # we only need to fit one tree for binary clf.\n+        super().__init__(n_classes=1)\n+\n+    def init_estimator(self):\n+        return DummyClassifier(strategy='prior')\n+\n+    def __call__(self, y, raw_predictions, sample_weight=None):\n+        \"\"\"Compute the exponential loss\n+\n+        Parameters\n+        ----------\n+        y : 1d array, shape (n_samples,)\n+            True labels.\n+\n+        raw_predictions : 2d array, shape (n_samples, K)\n+            The raw predictions (i.e. values from the tree leaves) of the\n+            tree ensemble.\n+\n+        sample_weight : 1d array, shape (n_samples,), optional\n+            Sample weights.\n+        \"\"\"\n+        raw_predictions = raw_predictions.ravel()\n+        if sample_weight is None:\n+            return np.mean(np.exp(-(2. * y - 1.) * raw_predictions))\n+        else:\n+            return (1.0 / sample_weight.sum() * np.sum(\n+                sample_weight * np.exp(-(2 * y - 1) * raw_predictions)))\n+\n+    def negative_gradient(self, y, raw_predictions, **kargs):\n+        \"\"\"Compute the residual (= negative gradient).\n+\n+        Parameters\n+        ----------\n+        y : 1d array, shape (n_samples,)\n+            True labels.\n+\n+        raw_predictions : 2d array, shape (n_samples, K)\n+            The raw predictions (i.e. values from the tree leaves) of the\n+            tree ensemble at iteration ``i - 1``.\n+        \"\"\"\n+        y_ = -(2. * y - 1.)\n+        return y_ * np.exp(y_ * raw_predictions.ravel())\n+\n+    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n+                                residual, raw_predictions, sample_weight):\n+        terminal_region = np.where(terminal_regions == leaf)[0]\n+        raw_predictions = raw_predictions.take(terminal_region, axis=0)\n+        y = y.take(terminal_region, axis=0)\n+        sample_weight = sample_weight.take(terminal_region, axis=0)\n+\n+        y_ = 2. * y - 1.\n+\n+        numerator = np.sum(y_ * sample_weight * np.exp(-y_ * raw_predictions))\n+        denominator = np.sum(sample_weight * np.exp(-y_ * raw_predictions))\n+\n+        # prevents overflow and division by zero\n+        if abs(denominator) < 1e-150:\n+            tree.value[leaf, 0, 0] = 0.0\n+        else:\n+            tree.value[leaf, 0, 0] = numerator / denominator\n+\n+    def _raw_prediction_to_proba(self, raw_predictions):\n+        proba = np.ones((raw_predictions.shape[0], 2), dtype=np.float64)\n+        proba[:, 1] = expit(2.0 * raw_predictions.ravel())\n+        proba[:, 0] -= proba[:, 1]\n+        return proba\n+\n+    def _raw_prediction_to_decision(self, raw_predictions):\n+        return (raw_predictions.ravel() >= 0).astype(np.int)\n+\n+    def get_init_raw_predictions(self, X, estimator):\n+        probas = estimator.predict_proba(X)\n+        proba_pos_class = probas[:, 1]\n+        eps = np.finfo(np.float32).eps\n+        proba_pos_class = np.clip(proba_pos_class, eps, 1 - eps)\n+        # according to The Elements of Statistical Learning sec. 10.5, the\n+        # minimizer of the exponential loss is .5 * log odds ratio. So this is\n+        # the equivalent to .5 * binomial_deviance.get_init_raw_predictions()\n+        raw_predictions = .5 * np.log(proba_pos_class / (1 - proba_pos_class))\n+        return raw_predictions.reshape(-1, 1).astype(np.float64)\n+\n+\n+LOSS_FUNCTIONS = {\n+    'ls': LeastSquaresError,\n+    'lad': LeastAbsoluteError,\n+    'huber': HuberLossFunction,\n+    'quantile': QuantileLossFunction,\n+    'deviance': None,    # for both, multinomial and binomial\n+    'exponential': ExponentialLoss,\n+}\ndiff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py\n--- a/sklearn/ensemble/gradient_boosting.py\n+++ b/sklearn/ensemble/gradient_boosting.py\n@@ -26,6 +26,8 @@\n from .base import BaseEnsemble\n from ..base import ClassifierMixin\n from ..base import RegressorMixin\n+from ..base import BaseEstimator\n+from ..base import is_classifier\n \n from ._gradient_boosting import predict_stages\n from ._gradient_boosting import predict_stage\n@@ -44,6 +46,7 @@\n from ..tree.tree import DecisionTreeRegressor\n from ..tree._tree import DTYPE\n from ..tree._tree import TREE_LEAF\n+from . import _gb_losses\n \n from ..utils import check_random_state\n from ..utils import check_array\n@@ -58,6 +61,14 @@\n from ..exceptions import NotFittedError\n \n \n+# FIXME: 0.23\n+# All the losses and corresponding init estimators have been moved to the\n+# _losses module in 0.21. We deprecate them and keep them here for now in case\n+# someone has imported them. None of these losses can be used as a parameter\n+# to a GBDT estimator anyway (loss param only accepts strings).\n+\n+@deprecated(\"QuantileEstimator is deprecated in version \"\n+            \"0.21 and will be removed in version 0.23.\")\n class QuantileEstimator:\n     \"\"\"An estimator predicting the alpha-quantile of the training targets.\n \n@@ -111,6 +122,8 @@ def predict(self, X):\n         return y\n \n \n+@deprecated(\"MeanEstimator is deprecated in version \"\n+            \"0.21 and will be removed in version 0.23.\")\n class MeanEstimator:\n     \"\"\"An estimator predicting the mean of the training targets.\"\"\"\n     def fit(self, X, y, sample_weight=None):\n@@ -152,6 +165,8 @@ def predict(self, X):\n         return y\n \n \n+@deprecated(\"LogOddsEstimator is deprecated in version \"\n+            \"0.21 and will be removed in version 0.23.\")\n class LogOddsEstimator:\n     \"\"\"An estimator predicting the log odds ratio.\"\"\"\n     scale = 1.0\n@@ -202,11 +217,15 @@ def predict(self, X):\n         return y\n \n \n+@deprecated(\"ScaledLogOddsEstimator is deprecated in version \"\n+            \"0.21 and will be removed in version 0.23.\")\n class ScaledLogOddsEstimator(LogOddsEstimator):\n     \"\"\"Log odds ratio scaled by 0.5 -- for exponential loss. \"\"\"\n     scale = 0.5\n \n \n+@deprecated(\"PriorProbablityEstimator is deprecated in version \"\n+            \"0.21 and will be removed in version 0.23.\")\n class PriorProbabilityEstimator:\n     \"\"\"An estimator predicting the probability of each\n     class in the training data.\n@@ -250,8 +269,16 @@ def predict(self, X):\n         return y\n \n \n+@deprecated(\"Using ZeroEstimator is deprecated in version \"\n+            \"0.21 and will be removed in version 0.23.\")\n class ZeroEstimator:\n-    \"\"\"An estimator that simply predicts zero. \"\"\"\n+    \"\"\"An estimator that simply predicts zero.\n+\n+    .. deprecated:: 0.21\n+        Using ``ZeroEstimator`` or ``init='zero'`` is deprecated in version\n+        0.21 and will be removed in version 0.23.\n+\n+    \"\"\"\n \n     def fit(self, X, y, sample_weight=None):\n         \"\"\"Fit the estimator.\n@@ -295,7 +322,13 @@ def predict(self, X):\n         y.fill(0.0)\n         return y\n \n+    def predict_proba(self, X):\n+        return self.predict(X)\n+\n \n+@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"\n+            \"deprecated in version \"\n+            \"0.21 and will be removed in version 0.23.\")\n class LossFunction(metaclass=ABCMeta):\n     \"\"\"Abstract base class for various loss functions.\n \n@@ -403,6 +436,9 @@ def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n         \"\"\"Template method for updating terminal regions (=leaves). \"\"\"\n \n \n+@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"\n+            \"deprecated in version \"\n+            \"0.21 and will be removed in version 0.23.\")\n class RegressionLossFunction(LossFunction, metaclass=ABCMeta):\n     \"\"\"Base class for regression loss functions.\n \n@@ -418,6 +454,9 @@ def __init__(self, n_classes):\n         super().__init__(n_classes)\n \n \n+@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"\n+            \"deprecated in version \"\n+            \"0.21 and will be removed in version 0.23.\")\n class LeastSquaresError(RegressionLossFunction):\n     \"\"\"Loss function for least squares (LS) estimation.\n     Terminal regions need not to be updated for least squares.\n@@ -501,6 +540,9 @@ def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n         pass\n \n \n+@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"\n+            \"deprecated in version \"\n+            \"0.21 and will be removed in version 0.23.\")\n class LeastAbsoluteError(RegressionLossFunction):\n     \"\"\"Loss function for least absolute deviation (LAD) regression.\n \n@@ -557,6 +599,9 @@ def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n         tree.value[leaf, 0, 0] = _weighted_percentile(diff, sample_weight, percentile=50)\n \n \n+@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"\n+            \"deprecated in version \"\n+            \"0.21 and will be removed in version 0.23.\")\n class HuberLossFunction(RegressionLossFunction):\n     \"\"\"Huber loss function for robust regression.\n \n@@ -660,6 +705,9 @@ def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n             np.minimum(np.abs(diff_minus_median), gamma))\n \n \n+@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"\n+            \"deprecated in version \"\n+            \"0.21 and will be removed in version 0.23.\")\n class QuantileLossFunction(RegressionLossFunction):\n     \"\"\"Loss function for quantile regression.\n \n@@ -737,6 +785,9 @@ def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n         tree.value[leaf, 0] = val\n \n \n+@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"\n+            \"deprecated in version \"\n+            \"0.21 and will be removed in version 0.23.\")\n class ClassificationLossFunction(LossFunction, metaclass=ABCMeta):\n     \"\"\"Base class for classification loss functions. \"\"\"\n \n@@ -755,6 +806,9 @@ def _score_to_decision(self, score):\n         \"\"\"\n \n \n+@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"\n+            \"deprecated in version \"\n+            \"0.21 and will be removed in version 0.23.\")\n class BinomialDeviance(ClassificationLossFunction):\n     \"\"\"Binomial deviance loss function for binary classification.\n \n@@ -846,6 +900,9 @@ def _score_to_decision(self, score):\n         return np.argmax(proba, axis=1)\n \n \n+@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"\n+            \"deprecated in version \"\n+            \"0.21 and will be removed in version 0.23.\")\n class MultinomialDeviance(ClassificationLossFunction):\n     \"\"\"Multinomial deviance loss function for multi-class classification.\n \n@@ -941,6 +998,9 @@ def _score_to_decision(self, score):\n         return np.argmax(proba, axis=1)\n \n \n+@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"\n+            \"deprecated in version \"\n+            \"0.21 and will be removed in version 0.23.\")\n class ExponentialLoss(ClassificationLossFunction):\n     \"\"\"Exponential loss function for binary classification.\n \n@@ -1028,19 +1088,7 @@ def _score_to_decision(self, score):\n         return (score.ravel() >= 0.0).astype(np.int)\n \n \n-LOSS_FUNCTIONS = {'ls': LeastSquaresError,\n-                  'lad': LeastAbsoluteError,\n-                  'huber': HuberLossFunction,\n-                  'quantile': QuantileLossFunction,\n-                  'deviance': None,    # for both, multinomial and binomial\n-                  'exponential': ExponentialLoss,\n-                  }\n-\n-\n-INIT_ESTIMATORS = {'zero': ZeroEstimator}\n-\n-\n-class VerboseReporter:\n+class VerboseReporter(object):\n     \"\"\"Reports verbose output to stdout.\n \n     Parameters\n@@ -1151,7 +1199,7 @@ def __init__(self, loss, learning_rate, n_estimators, criterion,\n         self.n_iter_no_change = n_iter_no_change\n         self.tol = tol\n \n-    def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,\n+    def _fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask,\n                    random_state, X_idx_sorted, X_csc=None, X_csr=None):\n         \"\"\"Fit another stage of ``n_classes_`` trees to the boosting model. \"\"\"\n \n@@ -1159,17 +1207,17 @@ def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,\n         loss = self.loss_\n         original_y = y\n \n-        # Need to pass a copy of y_pred to negative_gradient() because y_pred\n-        # is partially updated at the end of the loop in\n-        # update_terminal_regions(), and gradients need to be evaluated at\n+        # Need to pass a copy of raw_predictions to negative_gradient()\n+        # because raw_predictions is partially updated at the end of the loop\n+        # in update_terminal_regions(), and gradients need to be evaluated at\n         # iteration i - 1.\n-        y_pred_copy = y_pred.copy()\n+        raw_predictions_copy = raw_predictions.copy()\n \n         for k in range(loss.K):\n             if loss.is_multi_class:\n                 y = np.array(original_y == k, dtype=np.float64)\n \n-            residual = loss.negative_gradient(y, y_pred_copy, k=k,\n+            residual = loss.negative_gradient(y, raw_predictions_copy, k=k,\n                                               sample_weight=sample_weight)\n \n             # induce regression tree on residuals\n@@ -1196,14 +1244,14 @@ def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,\n                      check_input=False, X_idx_sorted=X_idx_sorted)\n \n             # update tree leaves\n-            loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,\n-                                         sample_weight, sample_mask,\n-                                         learning_rate=self.learning_rate, k=k)\n+            loss.update_terminal_regions(\n+                tree.tree_, X, y, residual, raw_predictions, sample_weight,\n+                sample_mask, learning_rate=self.learning_rate, k=k)\n \n             # add tree to ensemble\n             self.estimators_[i, k] = tree\n \n-        return y_pred\n+        return raw_predictions\n \n     def _check_params(self):\n         \"\"\"Check validity of parameters and raise ValueError if not valid. \"\"\"\n@@ -1216,15 +1264,15 @@ def _check_params(self):\n                              \"was %r\" % self.learning_rate)\n \n         if (self.loss not in self._SUPPORTED_LOSS\n-                or self.loss not in LOSS_FUNCTIONS):\n+                or self.loss not in _gb_losses.LOSS_FUNCTIONS):\n             raise ValueError(\"Loss '{0:s}' not supported. \".format(self.loss))\n \n         if self.loss == 'deviance':\n-            loss_class = (MultinomialDeviance\n+            loss_class = (_gb_losses.MultinomialDeviance\n                           if len(self.classes_) > 2\n-                          else BinomialDeviance)\n+                          else _gb_losses.BinomialDeviance)\n         else:\n-            loss_class = LOSS_FUNCTIONS[self.loss]\n+            loss_class = _gb_losses.LOSS_FUNCTIONS[self.loss]\n \n         if self.loss in ('huber', 'quantile'):\n             self.loss_ = loss_class(self.n_classes_, self.alpha)\n@@ -1236,15 +1284,14 @@ def _check_params(self):\n                              \"was %r\" % self.subsample)\n \n         if self.init is not None:\n-            if isinstance(self.init, str):\n-                if self.init not in INIT_ESTIMATORS:\n-                    raise ValueError('init=\"%s\" is not supported' % self.init)\n-            else:\n-                if (not hasattr(self.init, 'fit')\n-                        or not hasattr(self.init, 'predict')):\n-                    raise ValueError(\"init=%r must be valid BaseEstimator \"\n-                                     \"and support both fit and \"\n-                                     \"predict\" % self.init)\n+            # init must be an estimator or 'zero'\n+            if isinstance(self.init, BaseEstimator):\n+                self.loss_.check_init_estimator(self.init)\n+            elif not (isinstance(self.init, str) and self.init == 'zero'):\n+                raise ValueError(\n+                    \"The init parameter must be an estimator or 'zero'. \"\n+                    \"Got init={}\".format(self.init)\n+                )\n \n         if not (0.0 < self.alpha < 1.0):\n             raise ValueError(\"alpha must be in (0.0, 1.0) but \"\n@@ -1293,12 +1340,9 @@ def _check_params(self):\n     def _init_state(self):\n         \"\"\"Initialize model state and allocate model state data structures. \"\"\"\n \n-        if self.init is None:\n+        self.init_ = self.init\n+        if self.init_ is None:\n             self.init_ = self.loss_.init_estimator()\n-        elif isinstance(self.init, str):\n-            self.init_ = INIT_ESTIMATORS[self.init]()\n-        else:\n-            self.init_ = self.init\n \n         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),\n                                     dtype=np.object)\n@@ -1396,10 +1440,13 @@ def fit(self, X, y, sample_weight=None, monitor=None):\n         # Check input\n         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)\n         n_samples, self.n_features_ = X.shape\n-        if sample_weight is None:\n+\n+        sample_weight_is_none = sample_weight is None\n+        if sample_weight_is_none:\n             sample_weight = np.ones(n_samples, dtype=np.float32)\n         else:\n             sample_weight = column_or_1d(sample_weight, warn=True)\n+            sample_weight_is_none = False\n \n         check_consistent_length(X, y, sample_weight)\n \n@@ -1410,6 +1457,17 @@ def fit(self, X, y, sample_weight=None, monitor=None):\n                 train_test_split(X, y, sample_weight,\n                                  random_state=self.random_state,\n                                  test_size=self.validation_fraction))\n+            if is_classifier(self):\n+                if self.n_classes_ != np.unique(y).shape[0]:\n+                    # We choose to error here. The problem is that the init\n+                    # estimator would be trained on y, which has some missing\n+                    # classes now, so its predictions would not have the\n+                    # correct shape.\n+                    raise ValueError(\n+                        'The training data after the early stopping split '\n+                        'is missing some classes. Try using another random '\n+                        'seed.'\n+                    )\n         else:\n             X_val = y_val = sample_weight_val = None\n \n@@ -1419,11 +1477,25 @@ def fit(self, X, y, sample_weight=None, monitor=None):\n             # init state\n             self._init_state()\n \n-            # fit initial model - FIXME make sample_weight optional\n-            self.init_.fit(X, y, sample_weight)\n+            # fit initial model and initialize raw predictions\n+            if self.init_ == 'zero':\n+                raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),\n+                                           dtype=np.float64)\n+            else:\n+                try:\n+                    self.init_.fit(X, y, sample_weight=sample_weight)\n+                except TypeError:\n+                    if sample_weight_is_none:\n+                        self.init_.fit(X, y)\n+                    else:\n+                        raise ValueError(\n+                            \"The initial estimator {} does not support sample \"\n+                            \"weights.\".format(self.init_.__class__.__name__))\n+\n+                raw_predictions = \\\n+                    self.loss_.get_init_raw_predictions(X, self.init_)\n+\n \n-            # init predictions\n-            y_pred = self.init_.predict(X)\n             begin_at_stage = 0\n \n             # The rng state must be preserved if warm_start is True\n@@ -1443,7 +1515,7 @@ def fit(self, X, y, sample_weight=None, monitor=None):\n             # below) are more constrained than fit. It accepts only CSR\n             # matrices.\n             X = check_array(X, dtype=DTYPE, order=\"C\", accept_sparse='csr')\n-            y_pred = self._decision_function(X)\n+            raw_predictions = self._raw_predict(X)\n             self._resize_state()\n \n         if self.presort is True and issparse(X):\n@@ -1462,9 +1534,9 @@ def fit(self, X, y, sample_weight=None, monitor=None):\n                                              dtype=np.int32)\n \n         # fit the boosting stages\n-        n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,\n-                                    X_val, y_val, sample_weight_val,\n-                                    begin_at_stage, monitor, X_idx_sorted)\n+        n_stages = self._fit_stages(\n+            X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,\n+            sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n \n         # change shape of arrays after fit (early-stopping or additional ests)\n         if n_stages != self.estimators_.shape[0]:\n@@ -1476,7 +1548,7 @@ def fit(self, X, y, sample_weight=None, monitor=None):\n         self.n_estimators_ = n_stages\n         return self\n \n-    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,\n+    def _fit_stages(self, X, y, raw_predictions, sample_weight, random_state,\n                     X_val, y_val, sample_weight_val,\n                     begin_at_stage=0, monitor=None, X_idx_sorted=None):\n         \"\"\"Iteratively fits the stages.\n@@ -1510,7 +1582,7 @@ def _fit_stages(self, X, y, y_pred, sample_weight, random_state,\n             loss_history = np.full(self.n_iter_no_change, np.inf)\n             # We create a generator to get the predictions for X_val after\n             # the addition of each successive stage\n-            y_val_pred_iter = self._staged_decision_function(X_val)\n+            y_val_pred_iter = self._staged_raw_predict(X_val)\n \n         # perform boosting iterations\n         i = begin_at_stage\n@@ -1522,26 +1594,26 @@ def _fit_stages(self, X, y, y_pred, sample_weight, random_state,\n                                                   random_state)\n                 # OOB score before adding this stage\n                 old_oob_score = loss_(y[~sample_mask],\n-                                      y_pred[~sample_mask],\n+                                      raw_predictions[~sample_mask],\n                                       sample_weight[~sample_mask])\n \n             # fit next stage of trees\n-            y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n-                                     sample_mask, random_state, X_idx_sorted,\n-                                     X_csc, X_csr)\n+            raw_predictions = self._fit_stage(\n+                i, X, y, raw_predictions, sample_weight, sample_mask,\n+                random_state, X_idx_sorted, X_csc, X_csr)\n \n             # track deviance (= loss)\n             if do_oob:\n                 self.train_score_[i] = loss_(y[sample_mask],\n-                                             y_pred[sample_mask],\n+                                             raw_predictions[sample_mask],\n                                              sample_weight[sample_mask])\n                 self.oob_improvement_[i] = (\n                     old_oob_score - loss_(y[~sample_mask],\n-                                          y_pred[~sample_mask],\n+                                          raw_predictions[~sample_mask],\n                                           sample_weight[~sample_mask]))\n             else:\n                 # no need to fancy index w/ no subsampling\n-                self.train_score_[i] = loss_(y, y_pred, sample_weight)\n+                self.train_score_[i] = loss_(y, raw_predictions, sample_weight)\n \n             if self.verbose > 0:\n                 verbose_reporter.update(i, self)\n@@ -1572,26 +1644,30 @@ def _make_estimator(self, append=True):\n         # we don't need _make_estimator\n         raise NotImplementedError()\n \n-    def _init_decision_function(self, X):\n-        \"\"\"Check input and compute prediction of ``init``. \"\"\"\n+    def _raw_predict_init(self, X):\n+        \"\"\"Check input and compute raw predictions of the init estimtor.\"\"\"\n         self._check_initialized()\n         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)\n         if X.shape[1] != self.n_features_:\n             raise ValueError(\"X.shape[1] should be {0:d}, not {1:d}.\".format(\n                 self.n_features_, X.shape[1]))\n-        score = self.init_.predict(X).astype(np.float64)\n-        return score\n-\n-    def _decision_function(self, X):\n-        # for use in inner loop, not raveling the output in single-class case,\n-        # not doing input validation.\n-        score = self._init_decision_function(X)\n-        predict_stages(self.estimators_, X, self.learning_rate, score)\n-        return score\n+        if self.init_ == 'zero':\n+            raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),\n+                                       dtype=np.float64)\n+        else:\n+            raw_predictions = self.loss_.get_init_raw_predictions(\n+                X, self.init_).astype(np.float64)\n+        return raw_predictions\n \n+    def _raw_predict(self, X):\n+        \"\"\"Return the sum of the trees raw predictions (+ init estimator).\"\"\"\n+        raw_predictions = self._raw_predict_init(X)\n+        predict_stages(self.estimators_, X, self.learning_rate,\n+                       raw_predictions)\n+        return raw_predictions\n \n-    def _staged_decision_function(self, X):\n-        \"\"\"Compute decision function of ``X`` for each iteration.\n+    def _staged_raw_predict(self, X):\n+        \"\"\"Compute raw predictions of ``X`` for each iteration.\n \n         This method allows monitoring (i.e. determine error on testing set)\n         after each stage.\n@@ -1605,17 +1681,18 @@ def _staged_decision_function(self, X):\n \n         Returns\n         -------\n-        score : generator of array, shape (n_samples, k)\n-            The decision function of the input samples. The order of the\n+        raw_predictions : generator of array, shape (n_samples, k)\n+            The raw predictions of the input samples. The order of the\n             classes corresponds to that in the attribute `classes_`.\n             Regression and binary classification are special cases with\n             ``k == 1``, otherwise ``k==n_classes``.\n         \"\"\"\n         X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n-        score = self._init_decision_function(X)\n+        raw_predictions = self._raw_predict_init(X)\n         for i in range(self.estimators_.shape[0]):\n-            predict_stage(self.estimators_, i, X, self.learning_rate, score)\n-            yield score.copy()\n+            predict_stage(self.estimators_, i, X, self.learning_rate,\n+                          raw_predictions)\n+            yield raw_predictions.copy()\n \n     @property\n     def feature_importances_(self):\n@@ -1793,10 +1870,11 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):\n            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n            will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n \n-    init : estimator, optional\n-        An estimator object that is used to compute the initial\n-        predictions. ``init`` has to provide ``fit`` and ``predict``.\n-        If None it uses ``loss.init_estimator``.\n+    init : estimator or 'zero', optional (default=None)\n+        An estimator object that is used to compute the initial predictions.\n+        ``init`` has to provide `fit` and `predict_proba`. If 'zero', the\n+        initial raw predictions are set to zero. By default, a\n+        ``DummyEstimator`` predicting the classes priors is used.\n \n     random_state : int, RandomState instance or None, optional (default=None)\n         If int, random_state is the seed used by the random number generator;\n@@ -1984,16 +2062,17 @@ def decision_function(self, X):\n         Returns\n         -------\n         score : array, shape (n_samples, n_classes) or (n_samples,)\n-            The decision function of the input samples. The order of the\n-            classes corresponds to that in the attribute `classes_`.\n-            Regression and binary classification produce an array of shape\n-            [n_samples].\n+            The decision function of the input samples, which corresponds to\n+            the raw values predicted from the trees of the ensemble . The\n+            order of the classes corresponds to that in the attribute\n+            `classes_`. Regression and binary classification produce an\n+            array of shape [n_samples].\n         \"\"\"\n         X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n-        score = self._decision_function(X)\n-        if score.shape[1] == 1:\n-            return score.ravel()\n-        return score\n+        raw_predictions = self._raw_predict(X)\n+        if raw_predictions.shape[1] == 1:\n+            return raw_predictions.ravel()\n+        return raw_predictions\n \n     def staged_decision_function(self, X):\n         \"\"\"Compute decision function of ``X`` for each iteration.\n@@ -2011,12 +2090,13 @@ def staged_decision_function(self, X):\n         Returns\n         -------\n         score : generator of array, shape (n_samples, k)\n-            The decision function of the input samples. The order of the\n+            The decision function of the input samples, which corresponds to\n+            the raw values predicted from the trees of the ensemble . The\n             classes corresponds to that in the attribute `classes_`.\n             Regression and binary classification are special cases with\n             ``k == 1``, otherwise ``k==n_classes``.\n         \"\"\"\n-        yield from self._staged_decision_function(X)\n+        yield from self._staged_raw_predict(X)\n \n     def predict(self, X):\n         \"\"\"Predict class for X.\n@@ -2033,9 +2113,10 @@ def predict(self, X):\n         y : array, shape (n_samples,)\n             The predicted values.\n         \"\"\"\n-        score = self.decision_function(X)\n-        decisions = self.loss_._score_to_decision(score)\n-        return self.classes_.take(decisions, axis=0)\n+        raw_predictions = self.decision_function(X)\n+        encoded_labels = \\\n+            self.loss_._raw_prediction_to_decision(raw_predictions)\n+        return self.classes_.take(encoded_labels, axis=0)\n \n     def staged_predict(self, X):\n         \"\"\"Predict class at each stage for X.\n@@ -2055,9 +2136,10 @@ def staged_predict(self, X):\n         y : generator of array of shape (n_samples,)\n             The predicted value of the input samples.\n         \"\"\"\n-        for score in self._staged_decision_function(X):\n-            decisions = self.loss_._score_to_decision(score)\n-            yield self.classes_.take(decisions, axis=0)\n+        for raw_predictions in self._staged_raw_predict(X):\n+            encoded_labels = \\\n+                self.loss_._raw_prediction_to_decision(raw_predictions)\n+            yield self.classes_.take(encoded_labels, axis=0)\n \n     def predict_proba(self, X):\n         \"\"\"Predict class probabilities for X.\n@@ -2080,9 +2162,9 @@ def predict_proba(self, X):\n             The class probabilities of the input samples. The order of the\n             classes corresponds to that in the attribute `classes_`.\n         \"\"\"\n-        score = self.decision_function(X)\n+        raw_predictions = self.decision_function(X)\n         try:\n-            return self.loss_._score_to_proba(score)\n+            return self.loss_._raw_prediction_to_proba(raw_predictions)\n         except NotFittedError:\n             raise\n         except AttributeError:\n@@ -2132,8 +2214,8 @@ def staged_predict_proba(self, X):\n             The predicted value of the input samples.\n         \"\"\"\n         try:\n-            for score in self._staged_decision_function(X):\n-                yield self.loss_._score_to_proba(score)\n+            for raw_predictions in self._staged_raw_predict(X):\n+                yield self.loss_._raw_prediction_to_proba(raw_predictions)\n         except NotFittedError:\n             raise\n         except AttributeError:\n@@ -2251,10 +2333,12 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):\n            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n            will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n \n-    init : estimator, optional (default=None)\n-        An estimator object that is used to compute the initial\n-        predictions. ``init`` has to provide ``fit`` and ``predict``.\n-        If None it uses ``loss.init_estimator``.\n+    init : estimator or 'zero', optional (default=None)\n+        An estimator object that is used to compute the initial predictions.\n+        ``init`` has to provide `fit` and `predict`. If 'zero', the initial\n+        raw predictions are set to zero. By default a ``DummyEstimator`` is\n+        used, predicting either the average target value (for loss='ls'), or\n+        a quantile for the other losses.\n \n     random_state : int, RandomState instance or None, optional (default=None)\n         If int, random_state is the seed used by the random number generator;\n@@ -2426,7 +2510,8 @@ def predict(self, X):\n             The predicted values.\n         \"\"\"\n         X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n-        return self._decision_function(X).ravel()\n+        # In regression we can directly return the raw value from the trees.\n+        return self._raw_predict(X).ravel()\n \n     def staged_predict(self, X):\n         \"\"\"Predict regression target at each stage for X.\n@@ -2446,8 +2531,8 @@ def staged_predict(self, X):\n         y : generator of array of shape (n_samples,)\n             The predicted value of the input samples.\n         \"\"\"\n-        for y in self._staged_decision_function(X):\n-            yield y.ravel()\n+        for raw_predictions in self._staged_raw_predict(X):\n+            yield raw_predictions.ravel()\n \n     def apply(self, X):\n         \"\"\"Apply trees in the ensemble to X, return leaf indices.\n",
  "test_patch": "diff --git a/sklearn/ensemble/tests/test_gradient_boosting.py b/sklearn/ensemble/tests/test_gradient_boosting.py\n--- a/sklearn/ensemble/tests/test_gradient_boosting.py\n+++ b/sklearn/ensemble/tests/test_gradient_boosting.py\n@@ -13,11 +13,15 @@\n \n from sklearn import datasets\n from sklearn.base import clone\n-from sklearn.datasets import make_classification, fetch_california_housing\n+from sklearn.base import BaseEstimator\n+from sklearn.datasets import (make_classification, fetch_california_housing,\n+                              make_regression)\n from sklearn.ensemble import GradientBoostingClassifier\n from sklearn.ensemble import GradientBoostingRegressor\n from sklearn.ensemble.gradient_boosting import ZeroEstimator\n from sklearn.ensemble._gradient_boosting import predict_stages\n+from sklearn.preprocessing import OneHotEncoder\n+from sklearn.svm import LinearSVC\n from sklearn.metrics import mean_squared_error\n from sklearn.model_selection import train_test_split\n from sklearn.utils import check_random_state, tosequence\n@@ -34,6 +38,8 @@\n from sklearn.utils.testing import skip_if_32bit\n from sklearn.exceptions import DataConversionWarning\n from sklearn.exceptions import NotFittedError\n+from sklearn.dummy import DummyClassifier, DummyRegressor\n+\n \n GRADIENT_BOOSTING_ESTIMATORS = [GradientBoostingClassifier,\n                                 GradientBoostingRegressor]\n@@ -1046,13 +1052,7 @@ def test_complete_regression():\n \n \n def test_zero_estimator_reg():\n-    # Test if ZeroEstimator works for regression.\n-    est = GradientBoostingRegressor(n_estimators=20, max_depth=1,\n-                                    random_state=1, init=ZeroEstimator())\n-    est.fit(boston.data, boston.target)\n-    y_pred = est.predict(boston.data)\n-    mse = mean_squared_error(boston.target, y_pred)\n-    assert_almost_equal(mse, 33.0, decimal=0)\n+    # Test if init='zero' works for regression.\n \n     est = GradientBoostingRegressor(n_estimators=20, max_depth=1,\n                                     random_state=1, init='zero')\n@@ -1067,14 +1067,9 @@ def test_zero_estimator_reg():\n \n \n def test_zero_estimator_clf():\n-    # Test if ZeroEstimator works for classification.\n+    # Test if init='zero' works for classification.\n     X = iris.data\n     y = np.array(iris.target)\n-    est = GradientBoostingClassifier(n_estimators=20, max_depth=1,\n-                                     random_state=1, init=ZeroEstimator())\n-    est.fit(X, y)\n-\n-    assert_greater(est.score(X, y), 0.96)\n \n     est = GradientBoostingClassifier(n_estimators=20, max_depth=1,\n                                      random_state=1, init='zero')\n@@ -1324,3 +1319,81 @@ def test_gradient_boosting_validation_fraction():\n     gbr3.fit(X_train, y_train)\n     assert gbr.n_estimators_ < gbr3.n_estimators_\n     assert gbc.n_estimators_ < gbc3.n_estimators_\n+\n+\n+class _NoSampleWeightWrapper(BaseEstimator):\n+    def __init__(self, est):\n+        self.est = est\n+\n+    def fit(self, X, y):\n+        self.est.fit(X, y)\n+\n+    def predict(self, X):\n+        return self.est.predict(X)\n+\n+    def predict_proba(self, X):\n+        return self.est.predict_proba(X)\n+\n+\n+def _make_multiclass():\n+    return make_classification(n_classes=3, n_clusters_per_class=1)\n+\n+\n+@pytest.mark.parametrize(\n+    \"gb, dataset_maker, init_estimator\",\n+    [(GradientBoostingClassifier, make_classification, DummyClassifier),\n+     (GradientBoostingClassifier, _make_multiclass, DummyClassifier),\n+     (GradientBoostingRegressor, make_regression, DummyRegressor)],\n+    ids=[\"binary classification\", \"multiclass classification\", \"regression\"])\n+def test_gradient_boosting_with_init(gb, dataset_maker, init_estimator):\n+    # Check that GradientBoostingRegressor works when init is a sklearn\n+    # estimator.\n+    # Check that an error is raised if trying to fit with sample weight but\n+    # inital estimator does not support sample weight\n+\n+    X, y = dataset_maker()\n+    sample_weight = np.random.RandomState(42).rand(100)\n+\n+    # init supports sample weights\n+    init_est = init_estimator()\n+    gb(init=init_est).fit(X, y, sample_weight=sample_weight)\n+\n+    # init does not support sample weights\n+    init_est = _NoSampleWeightWrapper(init_estimator())\n+    gb(init=init_est).fit(X, y)  # ok no sample weights\n+    with pytest.raises(ValueError,\n+                       match=\"estimator.*does not support sample weights\"):\n+        gb(init=init_est).fit(X, y, sample_weight=sample_weight)\n+\n+\n+@pytest.mark.parametrize('estimator, missing_method', [\n+    (GradientBoostingClassifier(init=LinearSVC()), 'predict_proba'),\n+    (GradientBoostingRegressor(init=OneHotEncoder()), 'predict')\n+])\n+def test_gradient_boosting_init_wrong_methods(estimator, missing_method):\n+    # Make sure error is raised if init estimators don't have the required\n+    # methods (fit, predict, predict_proba)\n+\n+    message = (\"The init parameter must be a valid estimator and support \"\n+               \"both fit and \" + missing_method)\n+    with pytest.raises(ValueError, match=message):\n+        estimator.fit(X, y)\n+\n+\n+def test_early_stopping_n_classes():\n+    # when doing early stopping (_, y_train, _, _ = train_test_split(X, y))\n+    # there might be classes in y that are missing in y_train. As the init\n+    # estimator will be trained on y_train, we need to raise an error if this\n+    # happens.\n+\n+    X = [[1, 2], [2, 3], [3, 4], [4, 5]]\n+    y = [0, 1, 1, 1]\n+    gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=4)\n+    with pytest.raises(\n+                ValueError,\n+                match='The training data after the early stopping split'):\n+        gb.fit(X, y)\n+\n+    # No error with another random seed\n+    gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=0)\n+    gb.fit(X, y)\ndiff --git a/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py b/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py\n--- a/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py\n+++ b/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py\n@@ -3,19 +3,21 @@\n \"\"\"\n \n import numpy as np\n-from numpy.testing import assert_array_equal\n from numpy.testing import assert_almost_equal\n+from numpy.testing import assert_allclose\n from numpy.testing import assert_equal\n \n from sklearn.utils import check_random_state\n-from sklearn.utils.testing import assert_raises\n-from sklearn.ensemble.gradient_boosting import BinomialDeviance\n-from sklearn.ensemble.gradient_boosting import LogOddsEstimator\n-from sklearn.ensemble.gradient_boosting import LeastSquaresError\n-from sklearn.ensemble.gradient_boosting import RegressionLossFunction\n-from sklearn.ensemble.gradient_boosting import LOSS_FUNCTIONS\n-from sklearn.ensemble.gradient_boosting import _weighted_percentile\n-from sklearn.ensemble.gradient_boosting import QuantileLossFunction\n+from sklearn.utils.stats import _weighted_percentile\n+from sklearn.ensemble._gb_losses import RegressionLossFunction\n+from sklearn.ensemble._gb_losses import LeastSquaresError\n+from sklearn.ensemble._gb_losses import LeastAbsoluteError\n+from sklearn.ensemble._gb_losses import HuberLossFunction\n+from sklearn.ensemble._gb_losses import QuantileLossFunction\n+from sklearn.ensemble._gb_losses import BinomialDeviance\n+from sklearn.ensemble._gb_losses import MultinomialDeviance\n+from sklearn.ensemble._gb_losses import ExponentialLoss\n+from sklearn.ensemble._gb_losses import LOSS_FUNCTIONS\n \n \n def test_binomial_deviance():\n@@ -52,17 +54,6 @@ def test_binomial_deviance():\n         assert_almost_equal(bd.negative_gradient(*datum), alt_ng(*datum))\n \n \n-def test_log_odds_estimator():\n-    # Check log odds estimator.\n-    est = LogOddsEstimator()\n-    assert_raises(ValueError, est.fit, None, np.array([1]))\n-\n-    est.fit(None, np.array([1.0, 0.0]))\n-    assert_equal(est.prior, 0.0)\n-    assert_array_equal(est.predict(np.array([[1.0], [1.0]])),\n-                       np.array([[0.0], [0.0]]))\n-\n-\n def test_sample_weight_smoke():\n     rng = check_random_state(13)\n     y = rng.rand(100)\n@@ -100,16 +91,16 @@ def test_sample_weight_init_estimators():\n         loss = Loss(k)\n         init_est = loss.init_estimator()\n         init_est.fit(X, y)\n-        out = init_est.predict(X)\n+        out = loss.get_init_raw_predictions(X, init_est)\n         assert_equal(out.shape, (y.shape[0], 1))\n \n         sw_init_est = loss.init_estimator()\n         sw_init_est.fit(X, y, sample_weight=sample_weight)\n-        sw_out = init_est.predict(X)\n+        sw_out = loss.get_init_raw_predictions(X, sw_init_est)\n         assert_equal(sw_out.shape, (y.shape[0], 1))\n \n         # check if predictions match\n-        assert_array_equal(out, sw_out)\n+        assert_allclose(out, sw_out, rtol=1e-2)\n \n \n def test_weighted_percentile():\n@@ -155,7 +146,6 @@ def test_quantile_loss_function():\n def test_sample_weight_deviance():\n     # Test if deviance supports sample weights.\n     rng = check_random_state(13)\n-    X = rng.rand(100, 2)\n     sample_weight = np.ones(100)\n     reg_y = rng.rand(100)\n     clf_y = rng.randint(0, 2, size=100)\n@@ -184,3 +174,102 @@ def test_sample_weight_deviance():\n         deviance_w_w = loss(y, p, sample_weight)\n         deviance_wo_w = loss(y, p)\n         assert deviance_wo_w == deviance_w_w\n+\n+\n+def test_init_raw_predictions_shapes():\n+    # Make sure get_init_raw_predictions returns float64 arrays with shape\n+    # (n_samples, K) where K is 1 for binary classification and regression, and\n+    # K = n_classes for multiclass classification\n+    rng = np.random.RandomState(0)\n+\n+    n_samples = 100\n+    X = rng.normal(size=(n_samples, 5))\n+    y = rng.normal(size=n_samples)\n+    for loss in (LeastSquaresError(n_classes=1),\n+                 LeastAbsoluteError(n_classes=1),\n+                 QuantileLossFunction(n_classes=1),\n+                 HuberLossFunction(n_classes=1)):\n+        init_estimator = loss.init_estimator().fit(X, y)\n+        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)\n+        assert raw_predictions.shape == (n_samples, 1)\n+        assert raw_predictions.dtype == np.float64\n+\n+    y = rng.randint(0, 2, size=n_samples)\n+    for loss in (BinomialDeviance(n_classes=2),\n+                 ExponentialLoss(n_classes=2)):\n+        init_estimator = loss.init_estimator().fit(X, y)\n+        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)\n+        assert raw_predictions.shape == (n_samples, 1)\n+        assert raw_predictions.dtype == np.float64\n+\n+    for n_classes in range(3, 5):\n+        y = rng.randint(0, n_classes, size=n_samples)\n+        loss = MultinomialDeviance(n_classes=n_classes)\n+        init_estimator = loss.init_estimator().fit(X, y)\n+        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)\n+        assert raw_predictions.shape == (n_samples, n_classes)\n+        assert raw_predictions.dtype == np.float64\n+\n+\n+def test_init_raw_predictions_values():\n+    # Make sure the get_init_raw_predictions() returns the expected values for\n+    # each loss.\n+    rng = np.random.RandomState(0)\n+\n+    n_samples = 100\n+    X = rng.normal(size=(n_samples, 5))\n+    y = rng.normal(size=n_samples)\n+\n+    # Least squares loss\n+    loss = LeastSquaresError(n_classes=1)\n+    init_estimator = loss.init_estimator().fit(X, y)\n+    raw_predictions = loss.get_init_raw_predictions(y, init_estimator)\n+    # Make sure baseline prediction is the mean of all targets\n+    assert_almost_equal(raw_predictions, y.mean())\n+\n+    # Least absolute and huber loss\n+    for Loss in (LeastAbsoluteError, HuberLossFunction):\n+        loss = Loss(n_classes=1)\n+        init_estimator = loss.init_estimator().fit(X, y)\n+        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)\n+        # Make sure baseline prediction is the median of all targets\n+        assert_almost_equal(raw_predictions, np.median(y))\n+\n+    # Quantile loss\n+    for alpha in (.1, .5, .9):\n+        loss = QuantileLossFunction(n_classes=1, alpha=alpha)\n+        init_estimator = loss.init_estimator().fit(X, y)\n+        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)\n+        # Make sure baseline prediction is the alpha-quantile of all targets\n+        assert_almost_equal(raw_predictions, np.percentile(y, alpha * 100))\n+\n+    y = rng.randint(0, 2, size=n_samples)\n+\n+    # Binomial deviance\n+    loss = BinomialDeviance(n_classes=2)\n+    init_estimator = loss.init_estimator().fit(X, y)\n+    # Make sure baseline prediction is equal to link_function(p), where p\n+    # is the proba of the positive class. We want predict_proba() to return p,\n+    # and by definition\n+    # p = inverse_link_function(raw_prediction) = sigmoid(raw_prediction)\n+    # So we want raw_prediction = link_function(p) = log(p / (1 - p))\n+    raw_predictions = loss.get_init_raw_predictions(y, init_estimator)\n+    p = y.mean()\n+    assert_almost_equal(raw_predictions, np.log(p / (1 - p)))\n+\n+    # Exponential loss\n+    loss = ExponentialLoss(n_classes=2)\n+    init_estimator = loss.init_estimator().fit(X, y)\n+    raw_predictions = loss.get_init_raw_predictions(y, init_estimator)\n+    p = y.mean()\n+    assert_almost_equal(raw_predictions, .5 * np.log(p / (1 - p)))\n+\n+    # Multinomial deviance loss\n+    for n_classes in range(3, 5):\n+        y = rng.randint(0, n_classes, size=n_samples)\n+        loss = MultinomialDeviance(n_classes=n_classes)\n+        init_estimator = loss.init_estimator().fit(X, y)\n+        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)\n+        for k in range(n_classes):\n+            p = (y == k).mean()\n+        assert_almost_equal(raw_predictions[:, k], np.log(p))\n",
  "problem_statement": "[MRG] FIX gradient boosting with sklearn estimator as init\nFixes #10302, Fixes #12429, Fixes #2691\r\n\r\nGradient Boosting used to fail when init was a sklearn estimator, which is a bit ironic :)\r\nIssue was that the predict output didn't have the expected shape. And apparently there was no test for the init parameter with other estimator than default.\r\n\r\n*Edit* Also accept initial estimator which does not support sample weights as long as the gradient boosting is not fitted with sample weights\n",
  "hints_text": "",
  "created_at": "2019-01-14T23:41:48Z",
  "version": "0.21",
  "FAIL_TO_PASS": "[\"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_toy[deviance-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_toy[deviance-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_toy[deviance-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_toy[exponential-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_toy[exponential-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_toy[exponential-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classifier_parameter_checks\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_regressor_parameter_checks\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_loss_function\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_synthetic[deviance-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_synthetic[deviance-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_synthetic[deviance-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_synthetic[exponential-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_synthetic[exponential-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_synthetic[exponential-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[1.0-ls-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[1.0-ls-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[1.0-ls-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[1.0-lad-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[1.0-lad-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[1.0-lad-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[1.0-huber-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[1.0-huber-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[1.0-huber-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[0.5-ls-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[0.5-ls-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[0.5-ls-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[0.5-lad-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[0.5-lad-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[0.5-lad-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[0.5-huber-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[0.5-huber-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[0.5-huber-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[None-1.0-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[None-1.0-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[None-1.0-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[None-0.5-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[None-0.5-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[None-0.5-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[1-1.0-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[1-1.0-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[1-1.0-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[1-0.5-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[1-0.5-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[1-0.5-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_regression_synthetic\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_feature_importances\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_probability_log\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_check_inputs\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_check_inputs_predict\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_check_inputs_predict_stages\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_check_max_features\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_max_feature_regression\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_max_feature_auto\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_staged_predict\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_staged_predict_proba\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_staged_functions_defensive[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_staged_functions_defensive[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_serialization\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_degenerate_targets\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_quantile_loss\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_symbol_labels\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_float_class_labels\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_shape_y\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_mem_layout\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_oob_improvement\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_oob_improvement_raise\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_oob_multilcass_iris\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_verbose_output\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_more_verbose_output\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_n_estimators[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_n_estimators[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_max_depth[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_max_depth[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_clear[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_clear[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_zero_n_estimators[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_zero_n_estimators[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_smaller_n_estimators[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_smaller_n_estimators[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_equal_n_estimators[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_equal_n_estimators[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_oob_switch[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_oob_switch[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_oob[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_oob[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_sparse[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_sparse[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_fortran[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_fortran[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_monitor_early_stopping[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_monitor_early_stopping[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_complete_classification\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_complete_regression\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_zero_estimator_reg\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_zero_estimator_clf\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_max_leaf_nodes_max_depth[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_max_leaf_nodes_max_depth[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_min_impurity_split[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_min_impurity_split[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_min_impurity_decrease[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_min_impurity_decrease[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_wo_nestimators_change\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_probability_exponential\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_non_uniform_weights_toy_edge_case_reg\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_non_uniform_weights_toy_edge_case_clf\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_sparse_input[csr_matrix-GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_sparse_input[csr_matrix-GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_sparse_input[csc_matrix-GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_sparse_input[csc_matrix-GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_sparse_input[coo_matrix-GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_sparse_input[coo_matrix-GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_early_stopping\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_validation_fraction\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_with_init[binary\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_with_init[multiclass\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_with_init[regression]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_init_wrong_methods[estimator0-predict_proba]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_init_wrong_methods[estimator1-predict]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_early_stopping_n_classes\", \"sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py::test_binomial_deviance\", \"sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py::test_sample_weight_smoke\", \"sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py::test_sample_weight_init_estimators\", \"sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py::test_weighted_percentile\", \"sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py::test_weighted_percentile_equal\", \"sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py::test_weighted_percentile_zero_weight\", \"sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py::test_quantile_loss_function\", \"sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py::test_sample_weight_deviance\", \"sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py::test_init_raw_predictions_shapes\", \"sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py::test_init_raw_predictions_values\"]",
  "PASS_TO_PASS": "[\"sklearn/ensemble/tests/test_gradient_boosting.py::test_feature_importance_regression\"]",
  "environment_setup_commit": "7813f7efb5b2012412888b69e73d76f2df2b50b6",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.978353",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}