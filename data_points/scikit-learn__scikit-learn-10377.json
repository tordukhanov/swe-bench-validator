{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-10377",
  "base_commit": "5e26bf902621933bc8c7f3ce21c2085ee32651d3",
  "patch": "diff --git a/sklearn/metrics/classification.py b/sklearn/metrics/classification.py\n--- a/sklearn/metrics/classification.py\n+++ b/sklearn/metrics/classification.py\n@@ -1072,6 +1072,7 @@ def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None,\n                 raise ValueError('All labels must be in [0, n labels). '\n                                  'Got %d < 0' % np.min(labels))\n \n+        if n_labels is not None:\n             y_true = y_true[:, labels[:n_labels]]\n             y_pred = y_pred[:, labels[:n_labels]]\n \n",
  "test_patch": "diff --git a/sklearn/metrics/tests/test_classification.py b/sklearn/metrics/tests/test_classification.py\n--- a/sklearn/metrics/tests/test_classification.py\n+++ b/sklearn/metrics/tests/test_classification.py\n@@ -197,6 +197,14 @@ def test_precision_recall_f_extra_labels():\n         assert_raises(ValueError, recall_score, y_true_bin, y_pred_bin,\n                       labels=np.arange(-1, 4), average=average)\n \n+    # tests non-regression on issue #10307\n+    y_true = np.array([[0, 1, 1], [1, 0, 0]])\n+    y_pred = np.array([[1, 1, 1], [1, 0, 1]])\n+    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred,\n+                                                 average='samples',\n+                                                 labels=[0, 1])\n+    assert_almost_equal(np.array([p, r, f]), np.array([3 / 4, 1, 5 / 6]))\n+\n \n @ignore_warnings\n def test_precision_recall_f_ignored_labels():\n",
  "problem_statement": "BUG Inconsistent f1_score behavior when combining label indicator input with labels attribute\n#### Description\r\nWhen using label indicator inputs for y_pred and y_true, metrics.f1_score calculates the macro average over all label-specific f-scores whenever the labels parameter includes column index 0. It should only average over the label-specific scores indicated by the labels parameter, as it does when 0 is not present in the labels parameter.\r\n\r\n<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nimport numpy as np\r\nfrom sklearn.metrics import f1_score, precision_recall_fscore_support\r\n\r\ny_true = np.array([[0, 1, 0, 0],\r\n                   [1, 0, 0, 0],\r\n                   [1, 0, 0, 0]])\r\ny_pred = np.array([[0, 1, 0, 0],\r\n                   [0, 0, 1, 0],\r\n                   [0, 1, 0, 0]])\r\n\r\np, r, f, s = precision_recall_fscore_support(y_true, y_pred)\r\nprint(f)\r\nprint(f1_score(y_true, y_pred, labels=[0,1], average='macro'))\r\nprint(f1_score(y_true, y_pred, labels=[0,1,2], average='macro'))\r\nprint(f1_score(y_true, y_pred, labels=[1,3], average='macro'))\r\nprint(f1_score(y_true, y_pred, labels=[1,2,3], average='macro'))\r\n```\r\n#### Expected Results\r\n```\r\n[ 0.          0.66666667  0.          0.        ]\r\n0.333333333333\r\n0.222222222222\r\n0.333333333333\r\n0.222222222222\r\n```\r\n#### Actual Results\r\n```\r\n[ 0.          0.66666667  0.          0.        ]\r\n0.166666666667\r\n0.166666666667\r\n0.333333333333\r\n0.222222222222\r\n```\r\n\r\n<!-- Please paste or specifically describe the actual output or traceback. -->\r\n\r\n#### Versions\r\nWindows-7-6.1.7601-SP1\r\nPython 3.5.3 |Anaconda custom (64-bit)| (default, May 15 2017, 10:43:23) [MSC v.1900 64 bit (AMD64)]\r\nNumPy 1.13.1\r\nSciPy 0.19.0\r\nScikit-Learn 0.19.0\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
  "hints_text": "Thanks for the clear issue description. Your diagnosis is not quite correct. The error is made when `labels` is a prefix of the available labels.\r\n\r\nThis is probably my fault, and I apologise.\r\n\r\nThe problem is the combination of https://github.com/scikit-learn/scikit-learn/blob/4f710cdd088aa8851e8b049e4faafa03767fda10/sklearn/metrics/classification.py#L1056, https://github.com/scikit-learn/scikit-learn/blob/4f710cdd088aa8851e8b049e4faafa03767fda10/sklearn/metrics/classification.py#L1066, and https://github.com/scikit-learn/scikit-learn/blob/4f710cdd088aa8851e8b049e4faafa03767fda10/sklearn/metrics/classification.py#L1075. We should be slicing `y_true = y_true[:, :n_labels]` in any case that `n_labels < len(labels)`, not only when `np.all(labels == present_labels)`.\r\n\r\nWould you like to offer a PR to fix it?\nCan I take this up?\nSure, go for it",
  "created_at": "2017-12-27T16:39:20Z",
  "version": "0.20",
  "FAIL_TO_PASS": "[\"sklearn/metrics/tests/test_classification.py::test_precision_recall_f_extra_labels\"]",
  "PASS_TO_PASS": "[\"sklearn/metrics/tests/test_classification.py::test_multilabel_accuracy_score_subset_accuracy\", \"sklearn/metrics/tests/test_classification.py::test_precision_recall_f1_score_binary\", \"sklearn/metrics/tests/test_classification.py::test_precision_recall_f_binary_single_class\", \"sklearn/metrics/tests/test_classification.py::test_precision_recall_f_ignored_labels\", \"sklearn/metrics/tests/test_classification.py::test_average_precision_score_score_non_binary_class\", \"sklearn/metrics/tests/test_classification.py::test_average_precision_score_duplicate_values\", \"sklearn/metrics/tests/test_classification.py::test_average_precision_score_tied_values\", \"sklearn/metrics/tests/test_classification.py::test_precision_recall_fscore_support_errors\", \"sklearn/metrics/tests/test_classification.py::test_precision_recall_f_unused_pos_label\", \"sklearn/metrics/tests/test_classification.py::test_confusion_matrix_binary\", \"sklearn/metrics/tests/test_classification.py::test_cohen_kappa\", \"sklearn/metrics/tests/test_classification.py::test_matthews_corrcoef_nan\", \"sklearn/metrics/tests/test_classification.py::test_matthews_corrcoef_against_numpy_corrcoef\", \"sklearn/metrics/tests/test_classification.py::test_matthews_corrcoef_against_jurman\", \"sklearn/metrics/tests/test_classification.py::test_matthews_corrcoef\", \"sklearn/metrics/tests/test_classification.py::test_matthews_corrcoef_multiclass\", \"sklearn/metrics/tests/test_classification.py::test_matthews_corrcoef_overflow\", \"sklearn/metrics/tests/test_classification.py::test_precision_recall_f1_score_multiclass\", \"sklearn/metrics/tests/test_classification.py::test_precision_refcall_f1_score_multilabel_unordered_labels\", \"sklearn/metrics/tests/test_classification.py::test_precision_recall_f1_score_binary_averaged\", \"sklearn/metrics/tests/test_classification.py::test_zero_precision_recall\", \"sklearn/metrics/tests/test_classification.py::test_confusion_matrix_multiclass\", \"sklearn/metrics/tests/test_classification.py::test_confusion_matrix_sample_weight\", \"sklearn/metrics/tests/test_classification.py::test_confusion_matrix_multiclass_subset_labels\", \"sklearn/metrics/tests/test_classification.py::test_confusion_matrix_dtype\", \"sklearn/metrics/tests/test_classification.py::test_classification_report_multiclass\", \"sklearn/metrics/tests/test_classification.py::test_classification_report_multiclass_with_digits\", \"sklearn/metrics/tests/test_classification.py::test_classification_report_multiclass_with_string_label\", \"sklearn/metrics/tests/test_classification.py::test_classification_report_multiclass_with_unicode_label\", \"sklearn/metrics/tests/test_classification.py::test_classification_report_multiclass_with_long_string_label\", \"sklearn/metrics/tests/test_classification.py::test_classification_report_labels_target_names_unequal_length\", \"sklearn/metrics/tests/test_classification.py::test_classification_report_no_labels_target_names_unequal_length\", \"sklearn/metrics/tests/test_classification.py::test_multilabel_classification_report\", \"sklearn/metrics/tests/test_classification.py::test_multilabel_zero_one_loss_subset\", \"sklearn/metrics/tests/test_classification.py::test_multilabel_hamming_loss\", \"sklearn/metrics/tests/test_classification.py::test_multilabel_jaccard_similarity_score\", \"sklearn/metrics/tests/test_classification.py::test_precision_recall_f1_score_multilabel_1\", \"sklearn/metrics/tests/test_classification.py::test_precision_recall_f1_score_multilabel_2\", \"sklearn/metrics/tests/test_classification.py::test_precision_recall_f1_score_with_an_empty_prediction\", \"sklearn/metrics/tests/test_classification.py::test_precision_recall_f1_no_labels\", \"sklearn/metrics/tests/test_classification.py::test_prf_warnings\", \"sklearn/metrics/tests/test_classification.py::test_recall_warnings\", \"sklearn/metrics/tests/test_classification.py::test_precision_warnings\", \"sklearn/metrics/tests/test_classification.py::test_fscore_warnings\", \"sklearn/metrics/tests/test_classification.py::test_prf_average_binary_data_non_binary\", \"sklearn/metrics/tests/test_classification.py::test__check_targets\", \"sklearn/metrics/tests/test_classification.py::test__check_targets_multiclass_with_both_y_true_and_y_pred_binary\", \"sklearn/metrics/tests/test_classification.py::test_hinge_loss_binary\", \"sklearn/metrics/tests/test_classification.py::test_hinge_loss_multiclass\", \"sklearn/metrics/tests/test_classification.py::test_hinge_loss_multiclass_missing_labels_with_labels_none\", \"sklearn/metrics/tests/test_classification.py::test_hinge_loss_multiclass_with_missing_labels\", \"sklearn/metrics/tests/test_classification.py::test_hinge_loss_multiclass_invariance_lists\", \"sklearn/metrics/tests/test_classification.py::test_log_loss\", \"sklearn/metrics/tests/test_classification.py::test_log_loss_pandas_input\", \"sklearn/metrics/tests/test_classification.py::test_brier_score_loss\"]",
  "environment_setup_commit": "55bf5d93e5674f13a1134d93a11fd0cd11aabcd1",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.949924",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}