{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-15138",
  "base_commit": "7c47337f7b15a5368c922ed1781a267bf66c7367",
  "patch": "diff --git a/sklearn/ensemble/_stacking.py b/sklearn/ensemble/_stacking.py\n--- a/sklearn/ensemble/_stacking.py\n+++ b/sklearn/ensemble/_stacking.py\n@@ -8,6 +8,7 @@\n \n import numpy as np\n from joblib import Parallel, delayed\n+import scipy.sparse as sparse\n \n from ..base import clone\n from ..base import ClassifierMixin, RegressorMixin, TransformerMixin\n@@ -37,13 +38,15 @@ class _BaseStacking(TransformerMixin, _BaseHeterogeneousEnsemble,\n \n     @abstractmethod\n     def __init__(self, estimators, final_estimator=None, cv=None,\n-                 stack_method='auto', n_jobs=None, verbose=0):\n+                 stack_method='auto', n_jobs=None, verbose=0,\n+                 passthrough=False):\n         super().__init__(estimators=estimators)\n         self.final_estimator = final_estimator\n         self.cv = cv\n         self.stack_method = stack_method\n         self.n_jobs = n_jobs\n         self.verbose = verbose\n+        self.passthrough = passthrough\n \n     def _clone_final_estimator(self, default):\n         if self.final_estimator is not None:\n@@ -51,8 +54,14 @@ def _clone_final_estimator(self, default):\n         else:\n             self.final_estimator_ = clone(default)\n \n-    def _concatenate_predictions(self, predictions):\n-        \"\"\"Concatenate the predictions of each first layer learner.\n+    def _concatenate_predictions(self, X, predictions):\n+        \"\"\"Concatenate the predictions of each first layer learner and\n+        possibly the input dataset `X`.\n+\n+        If `X` is sparse and `self.passthrough` is False, the output of\n+        `transform` will be dense (the predictions). If `X` is sparse\n+        and `self.passthrough` is True, the output of `transform` will\n+        be sparse.\n \n         This helper is in charge of ensuring the preditions are 2D arrays and\n         it will drop one of the probability column when using probabilities\n@@ -72,7 +81,12 @@ def _concatenate_predictions(self, predictions):\n                     X_meta.append(preds[:, 1:])\n                 else:\n                     X_meta.append(preds)\n-        return np.concatenate(X_meta, axis=1)\n+        if self.passthrough:\n+            X_meta.append(X)\n+            if sparse.issparse(X):\n+                return sparse.hstack(X_meta, format=X.format)\n+\n+        return np.hstack(X_meta)\n \n     @staticmethod\n     def _method_name(name, estimator, method):\n@@ -165,7 +179,7 @@ def fit(self, X, y, sample_weight=None):\n             if est != 'drop'\n         ]\n \n-        X_meta = self._concatenate_predictions(predictions)\n+        X_meta = self._concatenate_predictions(X, predictions)\n         if sample_weight is not None:\n             try:\n                 self.final_estimator_.fit(\n@@ -192,7 +206,7 @@ def _transform(self, X):\n             for est, meth in zip(self.estimators_, self.stack_method_)\n             if est != 'drop'\n         ]\n-        return self._concatenate_predictions(predictions)\n+        return self._concatenate_predictions(X, predictions)\n \n     @if_delegate_has_method(delegate='final_estimator_')\n     def predict(self, X, **predict_params):\n@@ -288,6 +302,12 @@ class StackingClassifier(ClassifierMixin, _BaseStacking):\n         `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n         using all processors. See Glossary for more details.\n \n+    passthrough : bool, default=False\n+        When False, only the predictions of estimators will be used as\n+        training data for `final_estimator`. When True, the\n+        `final_estimator` is trained on the predictions as well as the\n+        original training data.\n+\n     Attributes\n     ----------\n     estimators_ : list of estimators\n@@ -344,13 +364,15 @@ class StackingClassifier(ClassifierMixin, _BaseStacking):\n \n     \"\"\"\n     def __init__(self, estimators, final_estimator=None, cv=None,\n-                 stack_method='auto', n_jobs=None, verbose=0):\n+                 stack_method='auto', n_jobs=None, passthrough=False,\n+                 verbose=0):\n         super().__init__(\n             estimators=estimators,\n             final_estimator=final_estimator,\n             cv=cv,\n             stack_method=stack_method,\n             n_jobs=n_jobs,\n+            passthrough=passthrough,\n             verbose=verbose\n         )\n \n@@ -525,6 +547,12 @@ class StackingRegressor(RegressorMixin, _BaseStacking):\n         `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n         using all processors. See Glossary for more details.\n \n+    passthrough : bool, default=False\n+        When False, only the predictions of estimators will be used as\n+        training data for `final_estimator`. When True, the\n+        `final_estimator` is trained on the predictions as well as the\n+        original training data.\n+\n     Attributes\n     ----------\n     estimators_ : list of estimator\n@@ -569,13 +597,14 @@ class StackingRegressor(RegressorMixin, _BaseStacking):\n \n     \"\"\"\n     def __init__(self, estimators, final_estimator=None, cv=None, n_jobs=None,\n-                 verbose=0):\n+                 passthrough=False, verbose=0):\n         super().__init__(\n             estimators=estimators,\n             final_estimator=final_estimator,\n             cv=cv,\n             stack_method=\"predict\",\n             n_jobs=n_jobs,\n+            passthrough=passthrough,\n             verbose=verbose\n         )\n \n",
  "test_patch": "diff --git a/sklearn/ensemble/tests/test_stacking.py b/sklearn/ensemble/tests/test_stacking.py\n--- a/sklearn/ensemble/tests/test_stacking.py\n+++ b/sklearn/ensemble/tests/test_stacking.py\n@@ -5,6 +5,7 @@\n \n import pytest\n import numpy as np\n+import scipy.sparse as sparse\n \n from sklearn.base import BaseEstimator\n from sklearn.base import ClassifierMixin\n@@ -38,6 +39,7 @@\n from sklearn.model_selection import KFold\n \n from sklearn.utils._testing import assert_allclose\n+from sklearn.utils._testing import assert_allclose_dense_sparse\n from sklearn.utils._testing import ignore_warnings\n from sklearn.utils.estimator_checks import check_estimator\n from sklearn.utils.estimator_checks import check_no_attributes_set_in_init\n@@ -52,7 +54,8 @@\n @pytest.mark.parametrize(\n     \"final_estimator\", [None, RandomForestClassifier(random_state=42)]\n )\n-def test_stacking_classifier_iris(cv, final_estimator):\n+@pytest.mark.parametrize(\"passthrough\", [False, True])\n+def test_stacking_classifier_iris(cv, final_estimator, passthrough):\n     # prescale the data to avoid convergence warning without using a pipeline\n     # for later assert\n     X_train, X_test, y_train, y_test = train_test_split(\n@@ -60,7 +63,8 @@ def test_stacking_classifier_iris(cv, final_estimator):\n     )\n     estimators = [('lr', LogisticRegression()), ('svc', LinearSVC())]\n     clf = StackingClassifier(\n-        estimators=estimators, final_estimator=final_estimator, cv=cv\n+        estimators=estimators, final_estimator=final_estimator, cv=cv,\n+        passthrough=passthrough\n     )\n     clf.fit(X_train, y_train)\n     clf.predict(X_test)\n@@ -68,7 +72,10 @@ def test_stacking_classifier_iris(cv, final_estimator):\n     assert clf.score(X_test, y_test) > 0.8\n \n     X_trans = clf.transform(X_test)\n-    assert X_trans.shape[1] == 6\n+    expected_column_count = 10 if passthrough else 6\n+    assert X_trans.shape[1] == expected_column_count\n+    if passthrough:\n+        assert_allclose(X_test, X_trans[:, -4:])\n \n     clf.set_params(lr='drop')\n     clf.fit(X_train, y_train)\n@@ -79,7 +86,10 @@ def test_stacking_classifier_iris(cv, final_estimator):\n         clf.decision_function(X_test)\n \n     X_trans = clf.transform(X_test)\n-    assert X_trans.shape[1] == 3\n+    expected_column_count_drop = 7 if passthrough else 3\n+    assert X_trans.shape[1] == expected_column_count_drop\n+    if passthrough:\n+        assert_allclose(X_test, X_trans[:, -4:])\n \n \n def test_stacking_classifier_drop_column_binary_classification():\n@@ -161,7 +171,9 @@ def test_stacking_regressor_drop_estimator():\n      (RandomForestRegressor(random_state=42), {}),\n      (DummyRegressor(), {'return_std': True})]\n )\n-def test_stacking_regressor_diabetes(cv, final_estimator, predict_params):\n+@pytest.mark.parametrize(\"passthrough\", [False, True])\n+def test_stacking_regressor_diabetes(cv, final_estimator, predict_params,\n+                                     passthrough):\n     # prescale the data to avoid convergence warning without using a pipeline\n     # for later assert\n     X_train, X_test, y_train, _ = train_test_split(\n@@ -169,7 +181,8 @@ def test_stacking_regressor_diabetes(cv, final_estimator, predict_params):\n     )\n     estimators = [('lr', LinearRegression()), ('svr', LinearSVR())]\n     reg = StackingRegressor(\n-        estimators=estimators, final_estimator=final_estimator, cv=cv\n+        estimators=estimators, final_estimator=final_estimator, cv=cv,\n+        passthrough=passthrough\n     )\n     reg.fit(X_train, y_train)\n     result = reg.predict(X_test, **predict_params)\n@@ -178,14 +191,58 @@ def test_stacking_regressor_diabetes(cv, final_estimator, predict_params):\n         assert len(result) == expected_result_length\n \n     X_trans = reg.transform(X_test)\n-    assert X_trans.shape[1] == 2\n+    expected_column_count = 12 if passthrough else 2\n+    assert X_trans.shape[1] == expected_column_count\n+    if passthrough:\n+        assert_allclose(X_test, X_trans[:, -10:])\n \n     reg.set_params(lr='drop')\n     reg.fit(X_train, y_train)\n     reg.predict(X_test)\n \n     X_trans = reg.transform(X_test)\n-    assert X_trans.shape[1] == 1\n+    expected_column_count_drop = 11 if passthrough else 1\n+    assert X_trans.shape[1] == expected_column_count_drop\n+    if passthrough:\n+        assert_allclose(X_test, X_trans[:, -10:])\n+\n+\n+@pytest.mark.parametrize('fmt', ['csc', 'csr', 'coo'])\n+def test_stacking_regressor_sparse_passthrough(fmt):\n+    # Check passthrough behavior on a sparse X matrix\n+    X_train, X_test, y_train, _ = train_test_split(\n+        sparse.coo_matrix(scale(X_diabetes)).asformat(fmt),\n+        y_diabetes, random_state=42\n+    )\n+    estimators = [('lr', LinearRegression()), ('svr', LinearSVR())]\n+    rf = RandomForestRegressor(n_estimators=10, random_state=42)\n+    clf = StackingRegressor(\n+        estimators=estimators, final_estimator=rf, cv=5, passthrough=True\n+    )\n+    clf.fit(X_train, y_train)\n+    X_trans = clf.transform(X_test)\n+    assert_allclose_dense_sparse(X_test, X_trans[:, -10:])\n+    assert sparse.issparse(X_trans)\n+    assert X_test.format == X_trans.format\n+\n+\n+@pytest.mark.parametrize('fmt', ['csc', 'csr', 'coo'])\n+def test_stacking_classifier_sparse_passthrough(fmt):\n+    # Check passthrough behavior on a sparse X matrix\n+    X_train, X_test, y_train, _ = train_test_split(\n+        sparse.coo_matrix(scale(X_iris)).asformat(fmt),\n+        y_iris, random_state=42\n+    )\n+    estimators = [('lr', LogisticRegression()), ('svc', LinearSVC())]\n+    rf = RandomForestClassifier(n_estimators=10, random_state=42)\n+    clf = StackingClassifier(\n+        estimators=estimators, final_estimator=rf, cv=5, passthrough=True\n+    )\n+    clf.fit(X_train, y_train)\n+    X_trans = clf.transform(X_test)\n+    assert_allclose_dense_sparse(X_test, X_trans[:, -4:])\n+    assert sparse.issparse(X_trans)\n+    assert X_test.format == X_trans.format\n \n \n def test_stacking_classifier_drop_binary_prob():\n",
  "problem_statement": "Stacking: add an option to use the original dataset when training final_estimator\nI think it will be readonable to add an option to use the original dataset when training final_estimator. This seems reasonable and has proved to be useful in some Kaggle competitions.\r\n\r\nReference: implementation from mlxtend\r\nhttp://rasbt.github.io/mlxtend/api_subpackages/mlxtend.classifier/#stackingcvclassifier\r\n\r\nuse_features_in_secondary : bool (default: False)\r\nIf True, the meta-classifier will be trained both on the predictions of the original classifiers and the original dataset. If False, the meta-classifier will be trained only on the predictions of the original classifiers.\n",
  "hints_text": "I think that I added this in the early stage of the PR and we ruled this out.\nI agree that it existed at one point. I think it can be considered now in\nany case.\n\n`use_feature_in_secondary` might be a long name. At that time, I named it `passthrough`.\nWould it be better.\n> I think that I added this in the early stage of the PR and we ruled this out.\r\n\r\nCould you please summarize the reason? thanks @glemaitre\nThe reason was to make the PR simpler from what I am reading now.\r\nSo I think that we can go ahead to make a new PR.\n> At that time, I named it passthrough.\r\n\r\nLet's use this name.\nHi all --  I'd be glad to take a stab at putting a PR in for this.\n@jcusick13 Go for it :)\n> `use_feature_in_secondary` might be a long name. At that time, I named it `passthrough`.\r\n\r\n@glemaitre  i think original name is better, `passthrough`  is hard to understand, pass through what? pass through original features. \r\nmaybe name it as `use_raw_features`?\n#response_container_BBPPID{font-family: initial; font-size:initial; color: initial;}IMO it is to long\nWe use \"passthrough\" with similar semantics in ColumnTransformer\n\n(albeit with different syntax and context: it's not a parameter there)\n",
  "created_at": "2019-10-05T13:41:54Z",
  "version": "0.22",
  "FAIL_TO_PASS": "[\"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_iris[False-None-3]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_iris[False-None-cv1]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_iris[False-final_estimator1-3]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_iris[False-final_estimator1-cv1]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_iris[True-None-3]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_iris[True-None-cv1]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_iris[True-final_estimator1-3]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_iris[True-final_estimator1-cv1]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_diabetes[False-None-predict_params0-3]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_diabetes[False-None-predict_params0-cv1]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_diabetes[False-final_estimator1-predict_params1-3]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_diabetes[False-final_estimator1-predict_params1-cv1]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_diabetes[False-final_estimator2-predict_params2-3]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_diabetes[False-final_estimator2-predict_params2-cv1]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_diabetes[True-None-predict_params0-3]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_diabetes[True-None-predict_params0-cv1]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_diabetes[True-final_estimator1-predict_params1-3]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_diabetes[True-final_estimator1-predict_params1-cv1]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_diabetes[True-final_estimator2-predict_params2-3]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_diabetes[True-final_estimator2-predict_params2-cv1]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_sparse_passthrough[csc]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_sparse_passthrough[csr]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_sparse_passthrough[coo]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_sparse_passthrough[csc]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_sparse_passthrough[csr]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_sparse_passthrough[coo]\"]",
  "PASS_TO_PASS": "[\"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_drop_column_binary_classification\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_drop_estimator\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_drop_estimator\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_drop_binary_prob\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_error[y0-params0-ValueError-Invalid\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_error[y1-params1-ValueError-Invalid\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_error[y2-params2-ValueError-should\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_error[y3-params3-ValueError-does\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_error[y4-params4-TypeError-does\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_error[y5-params5-TypeError-does\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_error[y6-params6-ValueError-All\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_error[y7-params7-ValueError-parameter\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_error[y0-params0-ValueError-Invalid\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_error[y1-params1-ValueError-Invalid\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_error[y2-params2-ValueError-should\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_error[y3-params3-TypeError-does\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_error[y4-params4-TypeError-does\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_error[y5-params5-ValueError-All\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_regressor_error[y6-params6-ValueError-parameter\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_named_estimators[stacking_estimator0]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_named_estimators[stacking_estimator1]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_named_estimators_dropped[stacking_estimator0]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_named_estimators_dropped[stacking_estimator1]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_set_get_params[stacking_estimator0]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_set_get_params[stacking_estimator1]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_randomness[StackingClassifier]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_randomness[StackingRegressor]\", \"sklearn/ensemble/tests/test_stacking.py::test_check_estimators_stacking_estimator[StackingClassifier]\", \"sklearn/ensemble/tests/test_stacking.py::test_check_estimators_stacking_estimator[StackingRegressor]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_stratify_default\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_with_sample_weight[StackingClassifier]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_with_sample_weight[StackingRegressor]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_cv_influence[StackingClassifier]\", \"sklearn/ensemble/tests/test_stacking.py::test_stacking_cv_influence[StackingRegressor]\"]",
  "environment_setup_commit": "7e85a6d1f038bbb932b36f18d75df6be937ed00d",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:31.012413",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}