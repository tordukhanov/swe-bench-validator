{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-13704",
  "base_commit": "57726672b52421aca17123cc313136a340344d0d",
  "patch": "diff --git a/sklearn/feature_selection/variance_threshold.py b/sklearn/feature_selection/variance_threshold.py\n--- a/sklearn/feature_selection/variance_threshold.py\n+++ b/sklearn/feature_selection/variance_threshold.py\n@@ -5,7 +5,7 @@\n from ..base import BaseEstimator\n from .base import SelectorMixin\n from ..utils import check_array\n-from ..utils.sparsefuncs import mean_variance_axis\n+from ..utils.sparsefuncs import mean_variance_axis, min_max_axis\n from ..utils.validation import check_is_fitted\n \n \n@@ -65,8 +65,18 @@ def fit(self, X, y=None):\n \n         if hasattr(X, \"toarray\"):   # sparse matrix\n             _, self.variances_ = mean_variance_axis(X, axis=0)\n+            if self.threshold == 0:\n+                mins, maxes = min_max_axis(X, axis=0)\n+                peak_to_peaks = maxes - mins\n         else:\n             self.variances_ = np.var(X, axis=0)\n+            if self.threshold == 0:\n+                peak_to_peaks = np.ptp(X, axis=0)\n+\n+        if self.threshold == 0:\n+            # Use peak-to-peak to avoid numeric precision issues\n+            # for constant features\n+            self.variances_ = np.minimum(self.variances_, peak_to_peaks)\n \n         if np.all(self.variances_ <= self.threshold):\n             msg = \"No feature in X meets the variance threshold {0:.5f}\"\n",
  "test_patch": "diff --git a/sklearn/feature_selection/tests/test_variance_threshold.py b/sklearn/feature_selection/tests/test_variance_threshold.py\n--- a/sklearn/feature_selection/tests/test_variance_threshold.py\n+++ b/sklearn/feature_selection/tests/test_variance_threshold.py\n@@ -1,3 +1,6 @@\n+import numpy as np\n+import pytest\n+\n from sklearn.utils.testing import (assert_array_equal, assert_equal,\n                                    assert_raises)\n \n@@ -26,3 +29,17 @@ def test_variance_threshold():\n     for X in [data, csr_matrix(data)]:\n         X = VarianceThreshold(threshold=.4).fit_transform(X)\n         assert_equal((len(data), 1), X.shape)\n+\n+\n+def test_zero_variance_floating_point_error():\n+    # Test that VarianceThreshold(0.0).fit eliminates features that have\n+    # the same value in every sample, even when floating point errors\n+    # cause np.var not to be 0 for the feature.\n+    # See #13691\n+\n+    data = [[-0.13725701]] * 10\n+    assert np.var(data) != 0\n+    for X in [data, csr_matrix(data), csc_matrix(data), bsr_matrix(data)]:\n+        msg = \"No feature in X meets the variance threshold 0.00000\"\n+        with pytest.raises(ValueError, match=msg):\n+            VarianceThreshold().fit(X)\n",
  "problem_statement": "VarianceThreshold doesn't remove feature with zero variance\n#### Description\r\nWhen calling VarianceThreshold().fit_transform() on certain inputs, it fails to remove a column that has only one unique value.\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nimport numpy as np\r\nfrom sklearn.feature_selection import VarianceThreshold\r\n\r\nworks_correctly = np.array([[-0.13725701,  7.        ],\r\n                            [-0.13725701, -0.09853293],\r\n                            [-0.13725701, -0.09853293],\r\n                            [-0.13725701, -0.09853293],\r\n                            [-0.13725701, -0.09853293],\r\n                            [-0.13725701, -0.09853293],\r\n                            [-0.13725701, -0.09853293],\r\n                            [-0.13725701, -0.09853293],\r\n                            [-0.13725701, -0.09853293]])\r\n\r\nbroken = np.array([[-0.13725701,  7.        ],\r\n                   [-0.13725701, -0.09853293],\r\n                   [-0.13725701, -0.09853293],\r\n                   [-0.13725701, -0.09853293],\r\n                   [-0.13725701, -0.09853293],\r\n                   [-0.13725701, -0.09853293],\r\n                   [-0.13725701, -0.09853293],\r\n                   [-0.13725701, -0.09853293],\r\n                   [-0.13725701, -0.09853293],\r\n                   [-0.13725701, -0.09853293]])\r\n\r\nselector = VarianceThreshold()\r\nprint(selector.fit_transform(works_correctly))\r\n\r\nselector = VarianceThreshold()\r\nprint(selector.fit_transform(broken))\r\nprint(set(broken[:, 0]))\r\n```\r\n\r\n#### Expected Results\r\nThe Variance threshold should produce\r\n```\r\n[[ 7.        ]\r\n [-0.09853293]\r\n [-0.09853293]\r\n [-0.09853293]\r\n [-0.09853293]\r\n [-0.09853293]\r\n [-0.09853293]\r\n [-0.09853293]\r\n [-0.09853293]]\r\n```\r\n#### Actual Results\r\n```\r\n[[ 7.        ]\r\n [-0.09853293]\r\n [-0.09853293]\r\n [-0.09853293]\r\n [-0.09853293]\r\n [-0.09853293]\r\n [-0.09853293]\r\n [-0.09853293]\r\n [-0.09853293]]\r\n[[-0.13725701  7.        ]\r\n [-0.13725701 -0.09853293]\r\n [-0.13725701 -0.09853293]\r\n [-0.13725701 -0.09853293]\r\n [-0.13725701 -0.09853293]\r\n [-0.13725701 -0.09853293]\r\n [-0.13725701 -0.09853293]\r\n [-0.13725701 -0.09853293]\r\n [-0.13725701 -0.09853293]\r\n [-0.13725701 -0.09853293]]\r\n{-0.13725701}\r\n```\r\nThis issue arose when I was using VarianceThreshold on a real dataset (of which this is a subset). It appears to work correctly in other situations (for instance I can't reproduce this behaviour if I replace the first column with 1's).\r\n\r\n#### Versions\r\nSystem\r\n------\r\n    python: 3.5.6 |Anaconda, Inc.| (default, Aug 26 2018, 16:30:03)  [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\r\n   machine: Darwin-18.2.0-x86_64-i386-64bit\r\nexecutable: /anaconda3/envs/tensorflow/bin/python3\r\n\r\nBLAS\r\n----\r\n    macros: NO_ATLAS_INFO=3, HAVE_CBLAS=None\r\n  lib_dirs:\r\ncblas_libs: cblas\r\n\r\nPython deps\r\n-----------\r\nsetuptools: 40.2.0\r\n     numpy: 1.15.4\r\n   sklearn: 0.20.0\r\n    Cython: None\r\n     scipy: 1.1.0\r\n    pandas: 0.24.0\r\n       pip: 19.0.1\r\n\n",
  "hints_text": "On closer inspection this is just caused by floating point error in calculating the variance, and therefore not a bug with sklearn. It is resolvable by setting the variance threshold to e.g. 1e-33 rather than 0.\nWe should probably avoid 0 as a default. I'd be happy to deprecate the\ncurrent default and change it to np.finfo(X.dtype) or something smaller by\ndefault.\n\nAnother option would be to use np.ptp rather than np.var when the threshold is 0.",
  "created_at": "2019-04-23T15:57:53Z",
  "version": "0.22",
  "FAIL_TO_PASS": "[\"sklearn/feature_selection/tests/test_variance_threshold.py::test_zero_variance_floating_point_error\"]",
  "PASS_TO_PASS": "[\"sklearn/feature_selection/tests/test_variance_threshold.py::test_zero_variance\", \"sklearn/feature_selection/tests/test_variance_threshold.py::test_variance_threshold\"]",
  "environment_setup_commit": "7e85a6d1f038bbb932b36f18d75df6be937ed00d",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.998213",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}