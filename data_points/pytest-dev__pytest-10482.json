{
  "repo": "pytest-dev/pytest",
  "instance_id": "pytest-dev__pytest-10482",
  "base_commit": "54d5a63d1485110015665ece1065982407394517",
  "patch": "diff --git a/src/_pytest/fixtures.py b/src/_pytest/fixtures.py\n--- a/src/_pytest/fixtures.py\n+++ b/src/_pytest/fixtures.py\n@@ -58,6 +58,7 @@\n from _pytest.mark import ParameterSet\n from _pytest.mark.structures import MarkDecorator\n from _pytest.outcomes import fail\n+from _pytest.outcomes import skip\n from _pytest.outcomes import TEST_OUTCOME\n from _pytest.pathlib import absolutepath\n from _pytest.pathlib import bestrelpath\n@@ -1129,6 +1130,10 @@ def pytest_fixture_setup(\n     except TEST_OUTCOME:\n         exc_info = sys.exc_info()\n         assert exc_info[0] is not None\n+        if isinstance(\n+            exc_info[1], skip.Exception\n+        ) and not fixturefunc.__name__.startswith(\"xunit_setup\"):\n+            exc_info[1]._use_item_location = True  # type: ignore[attr-defined]\n         fixturedef.cached_result = (None, my_cache_key, exc_info)\n         raise\n     fixturedef.cached_result = (result, my_cache_key, None)\n",
  "test_patch": "diff --git a/testing/test_skipping.py b/testing/test_skipping.py\n--- a/testing/test_skipping.py\n+++ b/testing/test_skipping.py\n@@ -1439,6 +1439,27 @@ def test_pass():\n     )\n \n \n+def test_skip_from_fixture(pytester: Pytester) -> None:\n+    pytester.makepyfile(\n+        **{\n+            \"tests/test_1.py\": \"\"\"\n+        import pytest\n+        def test_pass(arg):\n+            pass\n+        @pytest.fixture\n+        def arg():\n+            condition = True\n+            if condition:\n+                pytest.skip(\"Fixture conditional skip\")\n+            \"\"\",\n+        }\n+    )\n+    result = pytester.runpytest(\"-rs\", \"tests/test_1.py\", \"--rootdir=tests\")\n+    result.stdout.fnmatch_lines(\n+        [\"SKIPPED [[]1[]] tests/test_1.py:2: Fixture conditional skip\"]\n+    )\n+\n+\n def test_skip_using_reason_works_ok(pytester: Pytester) -> None:\n     p = pytester.makepyfile(\n         \"\"\"\n",
  "problem_statement": "Short test summary doesn't show the test name when skipping from a fixture\nI'm using Pytest 7.0.1on Ubuntu 18.04 with Python 3.6.9.\r\n\r\nConsider a test:\r\n```python\r\ndef test_0(bar):\r\n    assert 0\r\n```\r\n\r\nand a fixture defined in `conftest.py` that will skip a test based on some conditional check.\r\n```python\r\nimport pytest\r\n\r\n@pytest.fixture\r\ndef bar():\r\n    if some_condition:\r\n        pytest.skip(\"Skipping\")\r\n```\r\n\r\nThen running the test with pytest shows something like:\r\n```bash\r\n$ pytest . -rs\r\n================================== test session starts ==================================\r\nplatform linux -- Python 3.6.9, pytest-7.0.1, pluggy-1.0.0\r\nrootdir: /tmp/foo\r\nplugins: cpp-2.1.2\r\ncollected 1 item                                                                        \r\n\r\ntest_foo.py s                                                                     [100%]\r\n\r\n================================ short test summary info ================================\r\nSKIPPED [1] conftest.py:6: Skipping\r\n================================== 1 skipped in 0.01s ===================================\r\n```\r\n\r\nThe summary shows that some test was skipped but there's no indication which test was skipped. Instead, it should show the test name rather than the location in the fixture where the `pytest.skip` was called from. If there are multiple tests that are skipped from various locations, matching a test with its skip condition becomes impossible.\r\n\r\nThere are some similar issues in #114, #748, #760 which may be related.\n",
  "hints_text": "Also reproduces with pytest 7.2.",
  "created_at": "2022-11-08T12:07:04Z",
  "version": "7.2",
  "FAIL_TO_PASS": "[\"testing/test_skipping.py::test_skip_from_fixture\"]",
  "PASS_TO_PASS": "[\"testing/test_skipping.py::test_importorskip\", \"testing/test_skipping.py::TestEvaluation::test_no_marker\", \"testing/test_skipping.py::TestEvaluation::test_marked_xfail_no_args\", \"testing/test_skipping.py::TestEvaluation::test_marked_skipif_no_args\", \"testing/test_skipping.py::TestEvaluation::test_marked_one_arg\", \"testing/test_skipping.py::TestEvaluation::test_marked_one_arg_with_reason\", \"testing/test_skipping.py::TestEvaluation::test_marked_one_arg_twice\", \"testing/test_skipping.py::TestEvaluation::test_marked_one_arg_twice2\", \"testing/test_skipping.py::TestEvaluation::test_marked_skipif_with_boolean_without_reason\", \"testing/test_skipping.py::TestEvaluation::test_marked_skipif_with_invalid_boolean\", \"testing/test_skipping.py::TestEvaluation::test_skipif_class\", \"testing/test_skipping.py::TestEvaluation::test_skipif_markeval_namespace\", \"testing/test_skipping.py::TestEvaluation::test_skipif_markeval_namespace_multiple\", \"testing/test_skipping.py::TestEvaluation::test_skipif_markeval_namespace_ValueError\", \"testing/test_skipping.py::TestXFail::test_xfail_simple[True]\", \"testing/test_skipping.py::TestXFail::test_xfail_simple[False]\", \"testing/test_skipping.py::TestXFail::test_xfail_xpassed\", \"testing/test_skipping.py::TestXFail::test_xfail_using_platform\", \"testing/test_skipping.py::TestXFail::test_xfail_xpassed_strict\", \"testing/test_skipping.py::TestXFail::test_xfail_run_anyway\", \"testing/test_skipping.py::TestXFail::test_xfail_run_with_skip_mark[test_input0-expected0]\", \"testing/test_skipping.py::TestXFail::test_xfail_run_with_skip_mark[test_input1-expected1]\", \"testing/test_skipping.py::TestXFail::test_xfail_evalfalse_but_fails\", \"testing/test_skipping.py::TestXFail::test_xfail_not_report_default\", \"testing/test_skipping.py::TestXFail::test_xfail_not_run_xfail_reporting\", \"testing/test_skipping.py::TestXFail::test_xfail_not_run_no_setup_run\", \"testing/test_skipping.py::TestXFail::test_xfail_xpass\", \"testing/test_skipping.py::TestXFail::test_xfail_imperative\", \"testing/test_skipping.py::TestXFail::test_xfail_imperative_in_setup_function\", \"testing/test_skipping.py::TestXFail::test_dynamic_xfail_no_run\", \"testing/test_skipping.py::TestXFail::test_dynamic_xfail_set_during_funcarg_setup\", \"testing/test_skipping.py::TestXFail::test_dynamic_xfail_set_during_runtest_failed\", \"testing/test_skipping.py::TestXFail::test_dynamic_xfail_set_during_runtest_passed_strict\", \"testing/test_skipping.py::TestXFail::test_xfail_raises[TypeError-TypeError-*1\", \"testing/test_skipping.py::TestXFail::test_xfail_raises[(AttributeError,\", \"testing/test_skipping.py::TestXFail::test_xfail_raises[TypeError-IndexError-*1\", \"testing/test_skipping.py::TestXFail::test_strict_sanity\", \"testing/test_skipping.py::TestXFail::test_strict_xfail[True]\", \"testing/test_skipping.py::TestXFail::test_strict_xfail[False]\", \"testing/test_skipping.py::TestXFail::test_strict_xfail_condition[True]\", \"testing/test_skipping.py::TestXFail::test_strict_xfail_condition[False]\", \"testing/test_skipping.py::TestXFail::test_xfail_condition_keyword[True]\", \"testing/test_skipping.py::TestXFail::test_xfail_condition_keyword[False]\", \"testing/test_skipping.py::TestXFail::test_strict_xfail_default_from_file[true]\", \"testing/test_skipping.py::TestXFail::test_strict_xfail_default_from_file[false]\", \"testing/test_skipping.py::TestXFail::test_xfail_markeval_namespace\", \"testing/test_skipping.py::TestXFailwithSetupTeardown::test_failing_setup_issue9\", \"testing/test_skipping.py::TestXFailwithSetupTeardown::test_failing_teardown_issue9\", \"testing/test_skipping.py::TestSkip::test_skip_class\", \"testing/test_skipping.py::TestSkip::test_skips_on_false_string\", \"testing/test_skipping.py::TestSkip::test_arg_as_reason\", \"testing/test_skipping.py::TestSkip::test_skip_no_reason\", \"testing/test_skipping.py::TestSkip::test_skip_with_reason\", \"testing/test_skipping.py::TestSkip::test_only_skips_marked_test\", \"testing/test_skipping.py::TestSkip::test_strict_and_skip\", \"testing/test_skipping.py::TestSkip::test_wrong_skip_usage\", \"testing/test_skipping.py::TestSkipif::test_skipif_conditional\", \"testing/test_skipping.py::TestSkipif::test_skipif_reporting[\\\"hasattr(sys,\", \"testing/test_skipping.py::TestSkipif::test_skipif_reporting[True,\", \"testing/test_skipping.py::TestSkipif::test_skipif_using_platform\", \"testing/test_skipping.py::TestSkipif::test_skipif_reporting_multiple[skipif-SKIP-skipped]\", \"testing/test_skipping.py::TestSkipif::test_skipif_reporting_multiple[xfail-XPASS-xpassed]\", \"testing/test_skipping.py::test_skip_not_report_default\", \"testing/test_skipping.py::test_skipif_class\", \"testing/test_skipping.py::test_skipped_reasons_functional\", \"testing/test_skipping.py::test_skipped_folding\", \"testing/test_skipping.py::test_reportchars\", \"testing/test_skipping.py::test_reportchars_error\", \"testing/test_skipping.py::test_reportchars_all\", \"testing/test_skipping.py::test_reportchars_all_error\", \"testing/test_skipping.py::test_errors_in_xfail_skip_expressions\", \"testing/test_skipping.py::test_xfail_skipif_with_globals\", \"testing/test_skipping.py::test_default_markers\", \"testing/test_skipping.py::test_xfail_test_setup_exception\", \"testing/test_skipping.py::test_imperativeskip_on_xfail_test\", \"testing/test_skipping.py::TestBooleanCondition::test_skipif\", \"testing/test_skipping.py::TestBooleanCondition::test_skipif_noreason\", \"testing/test_skipping.py::TestBooleanCondition::test_xfail\", \"testing/test_skipping.py::test_xfail_item\", \"testing/test_skipping.py::test_module_level_skip_error\", \"testing/test_skipping.py::test_module_level_skip_with_allow_module_level\", \"testing/test_skipping.py::test_invalid_skip_keyword_parameter\", \"testing/test_skipping.py::test_mark_xfail_item\", \"testing/test_skipping.py::test_summary_list_after_errors\", \"testing/test_skipping.py::test_relpath_rootdir\", \"testing/test_skipping.py::test_skip_using_reason_works_ok\", \"testing/test_skipping.py::test_fail_using_reason_works_ok\", \"testing/test_skipping.py::test_fail_fails_with_msg_and_reason\", \"testing/test_skipping.py::test_skip_fails_with_msg_and_reason\", \"testing/test_skipping.py::test_exit_with_msg_and_reason_fails\", \"testing/test_skipping.py::test_exit_with_reason_works_ok\"]",
  "environment_setup_commit": "572b5657d7ca557593418ce0319fabff88800c73",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.922486",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}