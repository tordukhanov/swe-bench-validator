{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-10443",
  "base_commit": "48f3303bfc0be26136b98e9aa95dc3b3f916daff",
  "patch": "diff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py\n--- a/sklearn/feature_extraction/text.py\n+++ b/sklearn/feature_extraction/text.py\n@@ -11,7 +11,7 @@\n The :mod:`sklearn.feature_extraction.text` submodule gathers utilities to\n build feature vectors from text documents.\n \"\"\"\n-from __future__ import unicode_literals\n+from __future__ import unicode_literals, division\n \n import array\n from collections import Mapping, defaultdict\n@@ -19,6 +19,7 @@\n from operator import itemgetter\n import re\n import unicodedata\n+import warnings\n \n import numpy as np\n import scipy.sparse as sp\n@@ -29,7 +30,7 @@\n from ..preprocessing import normalize\n from .hashing import FeatureHasher\n from .stop_words import ENGLISH_STOP_WORDS\n-from ..utils.validation import check_is_fitted\n+from ..utils.validation import check_is_fitted, check_array, FLOAT_DTYPES\n from ..utils.fixes import sp_version\n \n __all__ = ['CountVectorizer',\n@@ -573,7 +574,7 @@ def _document_frequency(X):\n     if sp.isspmatrix_csr(X):\n         return np.bincount(X.indices, minlength=X.shape[1])\n     else:\n-        return np.diff(sp.csc_matrix(X, copy=False).indptr)\n+        return np.diff(X.indptr)\n \n \n class CountVectorizer(BaseEstimator, VectorizerMixin):\n@@ -1117,11 +1118,14 @@ def fit(self, X, y=None):\n         X : sparse matrix, [n_samples, n_features]\n             a matrix of term/token counts\n         \"\"\"\n+        X = check_array(X, accept_sparse=('csr', 'csc'))\n         if not sp.issparse(X):\n-            X = sp.csc_matrix(X)\n+            X = sp.csr_matrix(X)\n+        dtype = X.dtype if X.dtype in FLOAT_DTYPES else np.float64\n+\n         if self.use_idf:\n             n_samples, n_features = X.shape\n-            df = _document_frequency(X)\n+            df = _document_frequency(X).astype(dtype)\n \n             # perform idf smoothing if required\n             df += int(self.smooth_idf)\n@@ -1129,9 +1133,11 @@ def fit(self, X, y=None):\n \n             # log+1 instead of log makes sure terms with zero idf don't get\n             # suppressed entirely.\n-            idf = np.log(float(n_samples) / df) + 1.0\n-            self._idf_diag = sp.spdiags(idf, diags=0, m=n_features,\n-                                        n=n_features, format='csr')\n+            idf = np.log(n_samples / df) + 1\n+            self._idf_diag = sp.diags(idf, offsets=0,\n+                                      shape=(n_features, n_features),\n+                                      format='csr',\n+                                      dtype=dtype)\n \n         return self\n \n@@ -1151,12 +1157,9 @@ def transform(self, X, copy=True):\n         -------\n         vectors : sparse matrix, [n_samples, n_features]\n         \"\"\"\n-        if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.floating):\n-            # preserve float family dtype\n-            X = sp.csr_matrix(X, copy=copy)\n-        else:\n-            # convert counts or binary occurrences to floats\n-            X = sp.csr_matrix(X, dtype=np.float64, copy=copy)\n+        X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, copy=copy)\n+        if not sp.issparse(X):\n+            X = sp.csr_matrix(X, dtype=np.float64)\n \n         n_samples, n_features = X.shape\n \n@@ -1367,7 +1370,7 @@ def __init__(self, input='content', encoding='utf-8',\n                  stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n                  ngram_range=(1, 1), max_df=1.0, min_df=1,\n                  max_features=None, vocabulary=None, binary=False,\n-                 dtype=np.int64, norm='l2', use_idf=True, smooth_idf=True,\n+                 dtype=np.float64, norm='l2', use_idf=True, smooth_idf=True,\n                  sublinear_tf=False):\n \n         super(TfidfVectorizer, self).__init__(\n@@ -1432,6 +1435,13 @@ def idf_(self, value):\n                                  (len(value), len(self.vocabulary)))\n         self._tfidf.idf_ = value\n \n+    def _check_params(self):\n+        if self.dtype not in FLOAT_DTYPES:\n+            warnings.warn(\"Only {} 'dtype' should be used. {} 'dtype' will \"\n+                          \"be converted to np.float64.\"\n+                          .format(FLOAT_DTYPES, self.dtype),\n+                          UserWarning)\n+\n     def fit(self, raw_documents, y=None):\n         \"\"\"Learn vocabulary and idf from training set.\n \n@@ -1444,6 +1454,7 @@ def fit(self, raw_documents, y=None):\n         -------\n         self : TfidfVectorizer\n         \"\"\"\n+        self._check_params()\n         X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n         self._tfidf.fit(X)\n         return self\n@@ -1464,6 +1475,7 @@ def fit_transform(self, raw_documents, y=None):\n         X : sparse matrix, [n_samples, n_features]\n             Tf-idf-weighted document-term matrix.\n         \"\"\"\n+        self._check_params()\n         X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n         self._tfidf.fit(X)\n         # X is already a transformed view of raw_documents so\n",
  "test_patch": "diff --git a/sklearn/feature_extraction/tests/test_text.py b/sklearn/feature_extraction/tests/test_text.py\n--- a/sklearn/feature_extraction/tests/test_text.py\n+++ b/sklearn/feature_extraction/tests/test_text.py\n@@ -1,6 +1,9 @@\n from __future__ import unicode_literals\n import warnings\n \n+import pytest\n+from scipy import sparse\n+\n from sklearn.feature_extraction.text import strip_tags\n from sklearn.feature_extraction.text import strip_accents_unicode\n from sklearn.feature_extraction.text import strip_accents_ascii\n@@ -28,15 +31,14 @@\n                                    assert_in, assert_less, assert_greater,\n                                    assert_warns_message, assert_raise_message,\n                                    clean_warning_registry, ignore_warnings,\n-                                   SkipTest, assert_raises)\n+                                   SkipTest, assert_raises,\n+                                   assert_allclose_dense_sparse)\n \n from collections import defaultdict, Mapping\n from functools import partial\n import pickle\n from io import StringIO\n \n-import pytest\n-\n JUNK_FOOD_DOCS = (\n     \"the pizza pizza beer copyright\",\n     \"the pizza burger beer copyright\",\n@@ -1042,6 +1044,42 @@ def test_vectorizer_string_object_as_input():\n             ValueError, message, vec.transform, \"hello world!\")\n \n \n+@pytest.mark.parametrize(\"X_dtype\", [np.float32, np.float64])\n+def test_tfidf_transformer_type(X_dtype):\n+    X = sparse.rand(10, 20000, dtype=X_dtype, random_state=42)\n+    X_trans = TfidfTransformer().fit_transform(X)\n+    assert X_trans.dtype == X.dtype\n+\n+\n+def test_tfidf_transformer_sparse():\n+    X = sparse.rand(10, 20000, dtype=np.float64, random_state=42)\n+    X_csc = sparse.csc_matrix(X)\n+    X_csr = sparse.csr_matrix(X)\n+\n+    X_trans_csc = TfidfTransformer().fit_transform(X_csc)\n+    X_trans_csr = TfidfTransformer().fit_transform(X_csr)\n+    assert_allclose_dense_sparse(X_trans_csc, X_trans_csr)\n+    assert X_trans_csc.format == X_trans_csr.format\n+\n+\n+@pytest.mark.parametrize(\n+    \"vectorizer_dtype, output_dtype, expected_warning, msg_warning\",\n+    [(np.int32, np.float64, UserWarning, \"'dtype' should be used.\"),\n+     (np.int64, np.float64, UserWarning, \"'dtype' should be used.\"),\n+     (np.float32, np.float32, None, None),\n+     (np.float64, np.float64, None, None)]\n+)\n+def test_tfidf_vectorizer_type(vectorizer_dtype, output_dtype,\n+                               expected_warning, msg_warning):\n+    X = np.array([\"numpy\", \"scipy\", \"sklearn\"])\n+    vectorizer = TfidfVectorizer(dtype=vectorizer_dtype)\n+    with pytest.warns(expected_warning, match=msg_warning) as record:\n+            X_idf = vectorizer.fit_transform(X)\n+    if expected_warning is None:\n+        assert len(record) == 0\n+    assert X_idf.dtype == output_dtype\n+\n+\n @pytest.mark.parametrize(\"vec\", [\n         HashingVectorizer(ngram_range=(2, 1)),\n         CountVectorizer(ngram_range=(2, 1)),\n",
  "problem_statement": "TfidfVectorizer dtype argument ignored\n#### Description\r\nTfidfVectorizer's fit/fit_transform output is always np.float64 instead of the specified dtype\r\n\r\n#### Steps/Code to Reproduce\r\n```py\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\ntest = TfidfVectorizer(dtype=np.float32)\r\nprint(test.fit_transform([\"Help I have a bug\"]).dtype)\r\n```\r\n\r\n#### Expected Results\r\n```py\r\ndtype('float32')\r\n```\r\n\r\n#### Actual Results\r\n```py\r\ndtype('float64')\r\n```\r\n\r\n#### Versions\r\n```\r\nDarwin-17.2.0-x86_64-i386-64bit\r\nPython 3.6.1 |Anaconda 4.4.0 (x86_64)| (default, May 11 2017, 13:04:09) \r\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]\r\nNumPy 1.13.3\r\nSciPy 1.0.0\r\nScikit-Learn 0.19.0\r\n```\r\n  \n",
  "hints_text": "",
  "created_at": "2018-01-10T04:02:32Z",
  "version": "0.20",
  "FAIL_TO_PASS": "[\"sklearn/feature_extraction/tests/test_text.py::test_tfidf_transformer_type[float32]\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_type[int32-float64-UserWarning-'dtype'\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_type[int64-float64-UserWarning-'dtype'\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_type[float32-float32-None-None]\"]",
  "PASS_TO_PASS": "[\"sklearn/feature_extraction/tests/test_text.py::test_strip_accents\", \"sklearn/feature_extraction/tests/test_text.py::test_to_ascii\", \"sklearn/feature_extraction/tests/test_text.py::test_word_analyzer_unigrams\", \"sklearn/feature_extraction/tests/test_text.py::test_word_analyzer_unigrams_and_bigrams\", \"sklearn/feature_extraction/tests/test_text.py::test_unicode_decode_error\", \"sklearn/feature_extraction/tests/test_text.py::test_char_ngram_analyzer\", \"sklearn/feature_extraction/tests/test_text.py::test_char_wb_ngram_analyzer\", \"sklearn/feature_extraction/tests/test_text.py::test_word_ngram_analyzer\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_custom_vocabulary\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_custom_vocabulary_pipeline\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_custom_vocabulary_repeated_indices\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_custom_vocabulary_gap_index\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_stop_words\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_empty_vocabulary\", \"sklearn/feature_extraction/tests/test_text.py::test_fit_countvectorizer_twice\", \"sklearn/feature_extraction/tests/test_text.py::test_tf_idf_smoothing\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_no_smoothing\", \"sklearn/feature_extraction/tests/test_text.py::test_sublinear_tf\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_setters\", \"sklearn/feature_extraction/tests/test_text.py::test_hashing_vectorizer\", \"sklearn/feature_extraction/tests/test_text.py::test_feature_names\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_max_features\", \"sklearn/feature_extraction/tests/test_text.py::test_count_vectorizer_max_features\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_max_df\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_min_df\", \"sklearn/feature_extraction/tests/test_text.py::test_count_binary_occurrences\", \"sklearn/feature_extraction/tests/test_text.py::test_hashed_binary_occurrences\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_inverse_transform\", \"sklearn/feature_extraction/tests/test_text.py::test_count_vectorizer_pipeline_grid_selection\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_pipeline_grid_selection\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_pipeline_cross_validation\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_unicode\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_with_fixed_vocabulary\", \"sklearn/feature_extraction/tests/test_text.py::test_pickling_vectorizer\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_vocab_sets_when_pickling\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_vocab_dicts_when_pickling\", \"sklearn/feature_extraction/tests/test_text.py::test_stop_words_removal\", \"sklearn/feature_extraction/tests/test_text.py::test_pickling_transformer\", \"sklearn/feature_extraction/tests/test_text.py::test_transformer_idf_setter\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_setter\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidfvectorizer_invalid_idf_attr\", \"sklearn/feature_extraction/tests/test_text.py::test_non_unique_vocab\", \"sklearn/feature_extraction/tests/test_text.py::test_hashingvectorizer_nan_in_docs\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidfvectorizer_binary\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidfvectorizer_export_idf\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_vocab_clone\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_string_object_as_input\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_transformer_type[float64]\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_transformer_sparse\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_type[float64-float64-None-None]\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizers_invalid_ngram_range[vec0]\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizers_invalid_ngram_range[vec1]\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizers_invalid_ngram_range[vec2]\"]",
  "environment_setup_commit": "55bf5d93e5674f13a1134d93a11fd0cd11aabcd1",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.950743",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}