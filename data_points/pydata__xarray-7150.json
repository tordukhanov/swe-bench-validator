{
  "repo": "pydata/xarray",
  "instance_id": "pydata__xarray-7150",
  "base_commit": "f93b467db5e35ca94fefa518c32ee9bf93232475",
  "patch": "diff --git a/xarray/backends/api.py b/xarray/backends/api.py\n--- a/xarray/backends/api.py\n+++ b/xarray/backends/api.py\n@@ -234,7 +234,7 @@ def _get_mtime(filename_or_obj):\n \n def _protect_dataset_variables_inplace(dataset, cache):\n     for name, variable in dataset.variables.items():\n-        if name not in variable.dims:\n+        if name not in dataset._indexes:\n             # no need to protect IndexVariable objects\n             data = indexing.CopyOnWriteArray(variable._data)\n             if cache:\n",
  "test_patch": "diff --git a/xarray/tests/test_backends_api.py b/xarray/tests/test_backends_api.py\n--- a/xarray/tests/test_backends_api.py\n+++ b/xarray/tests/test_backends_api.py\n@@ -48,6 +48,25 @@ def open_dataset(\n     assert_identical(expected, actual)\n \n \n+def test_multiindex() -> None:\n+    # GH7139\n+    # Check that we properly handle backends that change index variables\n+    dataset = xr.Dataset(coords={\"coord1\": [\"A\", \"B\"], \"coord2\": [1, 2]})\n+    dataset = dataset.stack(z=[\"coord1\", \"coord2\"])\n+\n+    class MultiindexBackend(xr.backends.BackendEntrypoint):\n+        def open_dataset(\n+            self,\n+            filename_or_obj,\n+            drop_variables=None,\n+            **kwargs,\n+        ) -> xr.Dataset:\n+            return dataset.copy(deep=True)\n+\n+    loaded = xr.open_dataset(\"fake_filename\", engine=MultiindexBackend)\n+    assert_identical(dataset, loaded)\n+\n+\n class PassThroughBackendEntrypoint(xr.backends.BackendEntrypoint):\n     \"\"\"Access an object passed to the `open_dataset` method.\"\"\"\n \n",
  "problem_statement": "xarray.open_dataset has issues if the dataset returned by the backend contains a multiindex\n### What happened?\n\nAs a follow up of this comment: https://github.com/pydata/xarray/issues/6752#issuecomment-1236756285 I'm currently trying to implement a custom `NetCDF4` backend that allows me to also handle multiindices when loading a NetCDF dataset using `xr.open_dataset`. \r\n\r\nI'm using the following two functions to convert the dataset to a NetCDF compatible version and back again:\r\nhttps://github.com/pydata/xarray/issues/1077#issuecomment-1101505074.\r\n\r\nHere is a small code example:\r\n\r\n### Creating the dataset\r\n```python\r\nimport xarray as xr\r\nimport pandas\r\n\r\ndef create_multiindex(**kwargs):\r\n    return pandas.MultiIndex.from_arrays(list(kwargs.values()), names=kwargs.keys())\r\n\r\ndataset = xr.Dataset()\r\ndataset.coords[\"observation\"] = [\"A\", \"B\"]\r\ndataset.coords[\"wavelength\"] = [0.4, 0.5, 0.6, 0.7]\r\ndataset.coords[\"stokes\"] = [\"I\", \"Q\"]\r\ndataset[\"measurement\"] = create_multiindex(\r\n    observation=[\"A\", \"A\", \"B\", \"B\"],\r\n    wavelength=[0.4, 0.5, 0.6, 0.7],\r\n    stokes=[\"I\", \"Q\", \"I\", \"I\"],\r\n)\r\n```\r\n\r\n### Saving as NetCDF\r\n```python\r\nfrom cf_xarray import encode_multi_index_as_compress\r\npatched = encode_multi_index_as_compress(dataset)\r\npatched.to_netcdf(\"multiindex.nc\")\r\n```\r\n\r\n### And loading again\r\n```python\r\nfrom cf_xarray import decode_compress_to_multi_index\r\nloaded = xr.open_dataset(\"multiindex.nc\")\r\nloaded = decode_compress_to_multiindex(loaded)\r\nassert loaded.equals(dataset)  # works\r\n```\r\n\r\n### Custom Backend\r\nWhile the manual patching for saving is currently still required, I tried to at least work around the added function call in `open_dataset` by creating a custom NetCDF Backend:\r\n\r\n```python\r\n# registered as netcdf4-multiindex backend in setup.py\r\nclass MultiindexNetCDF4BackendEntrypoint(NetCDF4BackendEntrypoint):\r\n    def open_dataset(self, *args, handle_multiindex=True, **kwargs):\r\n        ds = super().open_dataset(*args, **kwargs)\r\n\r\n        if handle_multiindex:  # here is where the restore operation happens:\r\n            ds = decode_compress_to_multiindex(ds)\r\n\r\n        return ds\r\n```\r\n\r\n### The error\r\n```python\r\n>>> loaded = xr.open_dataset(\"multiindex.nc\", engine=\"netcdf4-multiindex\", handle_multiindex=True)  # fails\r\n\r\nFile ~/.local/share/virtualenvs/test-oePfdNug/lib/python3.8/site-packages/xarray/core/variable.py:2795, in IndexVariable.data(self, data)\r\n   2793 @Variable.data.setter  # type: ignore[attr-defined]\r\n   2794 def data(self, data):\r\n-> 2795     raise ValueError(\r\n   2796         f\"Cannot assign to the .data attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. \"\r\n   2797         f\"Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate.\"\r\n   2798     )\r\n\r\nValueError: Cannot assign to the .data attribute of dimension coordinate a.k.a IndexVariable 'measurement'. Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate.\r\n```\r\n\r\nbut this works:\r\n```python\r\n>>> loaded = xr.open_dataset(\"multiindex.nc\", engine=\"netcdf4-multiindex\", handle_multiindex=False)\r\n>>> loaded = decode_compress_to_multiindex(loaded)\r\n>>> assert loaded.equals(dataset)\r\n```\r\n\r\nSo I'm guessing `xarray` is performing some operation on the dataset returned by the backend, and one of those leads to a failure if there is a multiindex already contained.\n\n### What did you expect to happen?\n\nI expected that it doesn't matter wheter `decode_compress_to_multi_index` is called inside the backend or afterwards, and the same dataset will be returned each time.\n\n### Minimal Complete Verifiable Example\n\n```Python\nSee above.\n```\n\n\n### MVCE confirmation\n\n- [X] Minimal example — the example is as focused as reasonably possible to demonstrate the underlying issue in xarray.\n- [X] Complete example — the example is self-contained, including all data and the text of any traceback.\n- [X] Verifiable example — the example copy & pastes into an IPython prompt or [Binder notebook](https://mybinder.org/v2/gh/pydata/xarray/main?urlpath=lab/tree/doc/examples/blank_template.ipynb), returning the result.\n- [X] New issue — a search of GitHub Issues suggests this is not a duplicate.\n\n### Relevant log output\n\n_No response_\n\n### Anything else we need to know?\n\nI'm also open to other suggestions how I could simplify the usage of multiindices, maybe there is an approach that doesn't require a custom backend at all?\r\n\r\n\n\n### Environment\n\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.8.10 (default, Jan 28 2022, 09:41:12) \r\n[GCC 9.3.0]\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 5.10.102.1-microsoft-standard-WSL2\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: C.UTF-8\r\nLOCALE: ('en_US', 'UTF-8')\r\nlibhdf5: 1.10.5\r\nlibnetcdf: 4.6.3\r\n\r\nxarray: 2022.9.0\r\npandas: 1.5.0\r\nnumpy: 1.23.3\r\nscipy: 1.9.1\r\nnetCDF4: 1.5.4\r\npydap: None\r\nh5netcdf: None\r\nh5py: 3.7.0\r\nNio: None\r\nzarr: None\r\ncftime: 1.6.2\r\nnc_time_axis: None\r\nPseudoNetCDF: None\r\nrasterio: 1.3.2\r\ncfgrib: None\r\niris: None\r\nbottleneck: None\r\ndask: None\r\ndistributed: None\r\nmatplotlib: 3.6.0\r\ncartopy: 0.19.0.post1\r\nseaborn: None\r\nnumbagg: None\r\nfsspec: None\r\ncupy: None\r\npint: None\r\nsparse: 0.13.0\r\nflox: None\r\nnumpy_groupies: None\r\nsetuptools: 65.3.0\r\npip: 22.2.2\r\nconda: None\r\npytest: 7.1.3\r\nIPython: 8.5.0\r\nsphinx: 4.5.0\r\n</details>\r\n\n",
  "hints_text": "Hi @lukasbindreiter, could you add the whole error traceback please?\nI can see this type of decoding breaking some assumption in the file reading process. A full traceback would help identify where.\r\n\r\nI think the real solution is actually #4490, so you could explicitly provide a coder.\nHere is the full stacktrace:\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In [12], line 7\r\n----> 7 loaded = xr.open_dataset(\"multiindex.nc\", engine=\"netcdf4-multiindex\", handle_multiindex=True)\r\n      8 print(loaded)\r\n\r\nFile ~/.local/share/virtualenvs/test-oePfdNug/lib/python3.8/site-packages/xarray/backends/api.py:537, in open_dataset(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)\r\n    530 overwrite_encoded_chunks = kwargs.pop(\"overwrite_encoded_chunks\", None)\r\n    531 backend_ds = backend.open_dataset(\r\n    532     filename_or_obj,\r\n    533     drop_variables=drop_variables,\r\n    534     **decoders,\r\n    535     **kwargs,\r\n    536 )\r\n--> 537 ds = _dataset_from_backend_dataset(\r\n    538     backend_ds,\r\n    539     filename_or_obj,\r\n    540     engine,\r\n    541     chunks,\r\n    542     cache,\r\n    543     overwrite_encoded_chunks,\r\n    544     inline_array,\r\n    545     drop_variables=drop_variables,\r\n    546     **decoders,\r\n    547     **kwargs,\r\n    548 )\r\n    549 return ds\r\n\r\nFile ~/.local/share/virtualenvs/test-oePfdNug/lib/python3.8/site-packages/xarray/backends/api.py:345, in _dataset_from_backend_dataset(backend_ds, filename_or_obj, engine, chunks, cache, overwrite_encoded_chunks, inline_array, **extra_tokens)\r\n    340 if not isinstance(chunks, (int, dict)) and chunks not in {None, \"auto\"}:\r\n    341     raise ValueError(\r\n    342         f\"chunks must be an int, dict, 'auto', or None. Instead found {chunks}.\"\r\n    343     )\r\n--> 345 _protect_dataset_variables_inplace(backend_ds, cache)\r\n    346 if chunks is None:\r\n    347     ds = backend_ds\r\n\r\nFile ~/.local/share/virtualenvs/test-oePfdNug/lib/python3.8/site-packages/xarray/backends/api.py:239, in _protect_dataset_variables_inplace(dataset, cache)\r\n    237 if cache:\r\n    238     data = indexing.MemoryCachedArray(data)\r\n--> 239 variable.data = data\r\n\r\nFile ~/.local/share/virtualenvs/test-oePfdNug/lib/python3.8/site-packages/xarray/core/variable.py:2795, in IndexVariable.data(self, data)\r\n   2793 @Variable.data.setter  # type: ignore[attr-defined]\r\n   2794 def data(self, data):\r\n-> 2795     raise ValueError(\r\n   2796         f\"Cannot assign to the .data attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. \"\r\n   2797         f\"Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate.\"\r\n   2798     )\r\n\r\nValueError: Cannot assign to the .data attribute of dimension coordinate a.k.a IndexVariable 'measurement'. Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate.\r\n```\nLooks like the backend logic needs some updates to make it compatible with the new xarray data model with explicit indexes (i.e., possible indexed coordinates with name != dimension like for multi-index levels now), e.g., here:\r\n\r\nhttps://github.com/pydata/xarray/blob/8eea8bb67bad0b5ac367c082125dd2b2519d4f52/xarray/backends/api.py#L234-L241\r\n\r\n",
  "created_at": "2022-10-10T13:03:26Z",
  "version": "2022.09",
  "FAIL_TO_PASS": "[\"xarray/tests/test_backends_api.py::test_multiindex\"]",
  "PASS_TO_PASS": "[\"xarray/tests/test_backends_api.py::test__get_default_engine\", \"xarray/tests/test_backends_api.py::test_custom_engine\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_honor_chunks[False-shape0-pref_chunks0]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_honor_chunks[False-shape1-pref_chunks1]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_honor_chunks[False-shape2-pref_chunks2]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_honor_chunks[False-shape3-pref_chunks3]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_honor_chunks[True-shape0-pref_chunks0]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_honor_chunks[True-shape1-pref_chunks1]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_honor_chunks[True-shape2-pref_chunks2]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_honor_chunks[True-shape3-pref_chunks3]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_split_chunks[shape0-pref_chunks0-req_chunks0]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_split_chunks[shape1-pref_chunks1-req_chunks1]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_split_chunks[shape2-pref_chunks2-req_chunks2]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_split_chunks[shape3-pref_chunks3-req_chunks3]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_split_chunks[shape4-pref_chunks4-req_chunks4]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_join_chunks[shape0-pref_chunks0-req_chunks0]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_join_chunks[shape1-pref_chunks1-req_chunks1]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_join_chunks[shape2-pref_chunks2-req_chunks2]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_join_chunks[shape3-pref_chunks3-req_chunks3]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_join_chunks[shape4-pref_chunks4-req_chunks4]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_join_chunks[shape5-pref_chunks5-req_chunks5]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_join_chunks[shape6-pref_chunks6-req_chunks6]\", \"xarray/tests/test_backends_api.py::TestPreferredChunks::test_join_chunks[shape7-pref_chunks7-req_chunks7]\"]",
  "environment_setup_commit": "087ebbb78668bdf5d2d41c3b2553e3f29ce75be1",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.903557",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}