{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-12486",
  "base_commit": "02dc9ed680e7f53f1b0d410dcdd37341c7958eb1",
  "patch": "diff --git a/sklearn/metrics/scorer.py b/sklearn/metrics/scorer.py\n--- a/sklearn/metrics/scorer.py\n+++ b/sklearn/metrics/scorer.py\n@@ -126,7 +126,13 @@ def __call__(self, clf, X, y, sample_weight=None):\n         y_type = type_of_target(y)\n         y_pred = clf.predict_proba(X)\n         if y_type == \"binary\":\n-            y_pred = y_pred[:, 1]\n+            if y_pred.shape[1] == 2:\n+                y_pred = y_pred[:, 1]\n+            else:\n+                raise ValueError('got predict_proba of shape {},'\n+                                 ' but need classifier with two'\n+                                 ' classes for {} scoring'.format(\n+                                     y_pred.shape, self._score_func.__name__))\n         if sample_weight is not None:\n             return self._sign * self._score_func(y, y_pred,\n                                                  sample_weight=sample_weight,\n@@ -183,7 +189,14 @@ def __call__(self, clf, X, y, sample_weight=None):\n                 y_pred = clf.predict_proba(X)\n \n                 if y_type == \"binary\":\n-                    y_pred = y_pred[:, 1]\n+                    if y_pred.shape[1] == 2:\n+                        y_pred = y_pred[:, 1]\n+                    else:\n+                        raise ValueError('got predict_proba of shape {},'\n+                                         ' but need classifier with two'\n+                                         ' classes for {} scoring'.format(\n+                                             y_pred.shape,\n+                                             self._score_func.__name__))\n                 elif isinstance(y_pred, list):\n                     y_pred = np.vstack([p[:, -1] for p in y_pred]).T\n \n",
  "test_patch": "diff --git a/sklearn/metrics/tests/test_score_objects.py b/sklearn/metrics/tests/test_score_objects.py\n--- a/sklearn/metrics/tests/test_score_objects.py\n+++ b/sklearn/metrics/tests/test_score_objects.py\n@@ -186,10 +186,11 @@ def check_scoring_validator_for_single_metric_usecases(scoring_validator):\n \n \n def check_multimetric_scoring_single_metric_wrapper(*args, **kwargs):\n-    # This wraps the _check_multimetric_scoring to take in single metric\n-    # scoring parameter so we can run the tests that we will run for\n-    # check_scoring, for check_multimetric_scoring too for single-metric\n-    # usecases\n+    # This wraps the _check_multimetric_scoring to take in\n+    # single metric scoring parameter so we can run the tests\n+    # that we will run for check_scoring, for check_multimetric_scoring\n+    # too for single-metric usecases\n+\n     scorers, is_multi = _check_multimetric_scoring(*args, **kwargs)\n     # For all single metric use cases, it should register as not multimetric\n     assert_false(is_multi)\n@@ -370,7 +371,21 @@ def test_thresholded_scorers():\n     X, y = make_blobs(random_state=0, centers=3)\n     X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n     clf.fit(X_train, y_train)\n-    assert_raises(ValueError, get_scorer('roc_auc'), clf, X_test, y_test)\n+    with pytest.raises(ValueError, match=\"multiclass format is not supported\"):\n+        get_scorer('roc_auc')(clf, X_test, y_test)\n+\n+    # test error is raised with a single class present in model\n+    # (predict_proba shape is not suitable for binary auc)\n+    X, y = make_blobs(random_state=0, centers=2)\n+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n+    clf = DecisionTreeClassifier()\n+    clf.fit(X_train, np.zeros_like(y_train))\n+    with pytest.raises(ValueError, match=\"need classifier with two classes\"):\n+        get_scorer('roc_auc')(clf, X_test, y_test)\n+\n+    # for proba scorers\n+    with pytest.raises(ValueError, match=\"need classifier with two classes\"):\n+        get_scorer('neg_log_loss')(clf, X_test, y_test)\n \n \n def test_thresholded_scorers_multilabel_indicator_data():\n",
  "problem_statement": "ck estimator is classifier & num_classes>=2 in score.py\n<!--\r\nThanks for contributing a pull request! Please ensure you have taken a look at\r\nthe contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist\r\n-->\r\n\r\n#### Reference Issues/PRs\r\n<!--\r\nExample: Fixes #1234. See also #3456.\r\nPlease use keywords (e.g., Fixes) to create link to the issues or pull requests\r\nyou resolved, so that they will automatically be closed when your pull request\r\nis merged. See https://github.com/blog/1506-closing-issues-via-pull-requests\r\n-->\r\n\r\n\r\n#### What does this implement/fix? Explain your changes.\r\nWe are fixing this issue: https://github.com/scikit-learn/scikit-learn/issues/7598\r\nWe added a test in the scorer.py file that raises a ValueError if the user is either trying to use a non classifier model for a classification problem, or is using a dataset with only one class. \r\n\r\n#### Any other comments?\r\n\r\n\r\n<!--\r\nPlease be aware that we are a loose team of volunteers so patience is\r\nnecessary; assistance handling other issues is very welcome. We value\r\nall user contributions, no matter how minor they are. If we are slow to\r\nreview, either the pull request needs some benchmarking, tinkering,\r\nconvincing, etc. or more likely the reviewers are simply busy. In either\r\ncase, we ask for your understanding during the review process.\r\nFor more information, see our FAQ on this topic:\r\nhttp://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.\r\n\r\nThanks for contributing!\r\n-->\r\n@reshamas\nBUG: Using GridSearchCV with scoring='roc_auc' and GMM as classifier gives IndexError\nWhen performing grid search using GridSearchCV using ootb scoring method 'roc_auc' and ootb GMM classifier from sklearn.mixture.GMM I get an index error.\nCode to reproduce:\n\n```\nfrom sklearn import datasets\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.mixture import GMM\nX,y = datasets.make_classification(n_samples = 10000, n_features=10,n_classes=2)\n# Vanilla GMM_model\ngmm_model = GMM()\n# Standard param grid\nparam_grid = {'n_components' : [1,2,3,4],\n              'covariance_type': ['tied','full','spherical']}\ngrid_search = GridSearchCV(gmm_model, param_grid, scoring='roc_auc')\n# Fit GS with this data\ngrid_search.fit(X, y)\n```\n\nSorry if the format is incorrect. First time I am posting.\n\nERROR:\n  File \"*/python2.7/site-packages/sklearn/metrics/scorer.py\", line 175, in **call**\n    y_pred = y_pred[:, 1]\nIndexError: index 1 is out of bounds for axis 1 with size 1\n\n",
  "hints_text": "Looks good in general, but you need to add a regression test (you could use the GMM one or just a classification one with a single class maybe)\nThanks for your feedback, will do it soon.\n\nOn Sat 29 Sep 2018 at 17:36, Andreas Mueller <notifications@github.com>\nwrote:\n\n> Looks good in general, but you need to add a regression test (you could\n> use the GMM one or just a classification one with a single class maybe)\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/pull/12221#issuecomment-425677053>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AlCeRE4pCeU1nUuA-TSqV-sjZl8XJpiMks5uf-fagaJpZM4XAqk->\n> .\n>\n-- \n*Alice MARTIN*\nData Scientist - Paris, France\nadresse email pro: alice.martindonati.pro@gmail.com\nhttps://www.linkedin.com/in/alicemartindonati\nhttps://github.com/AMDonati\n\n@AMDonati I am happy to get on a zoom or google hangouts meeting so we can run the checks.  Let me know what works for you.  My email:  reshama@wimlds.org\r\n\nUpdated to 0.17.1 and issue persists ( Changing GMM to GaussianMixture)\n\nThe error is strange, but GMM is not a supervised model, so AUC doesn't really make sense.\nWe might want to raise a better error, though it's hard to detect what's going on here in a sense.\n\nDo you really mean updated to 0.17.1, not 0.18?\n\nOn 8 October 2016 at 03:28, Andreas Mueller notifications@github.com\nwrote:\n\n> The error is strange, but GMM is not a supervised model, so AUC doesn't\n> really make sense.\n> \n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/scikit-learn/scikit-learn/issues/7598#issuecomment-252298229,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAEz6_dNOBwzUCFpb4N1bNKGAjC02ZEDks5qxnMlgaJpZM4KRC_m\n> .\n\nGetting the same error with\n\n```\n    cv = GridSearchCV(\n        estimator=DecisionTreeClassifier(),\n        param_grid={\n            'max_depth': [20],\n            'class_weight': ['auto'],\n            'min_samples_split': [100],\n            'min_samples_leaf': [30],\n            'criterion': ['gini']\n        },\n        scoring='roc_auc',\n        n_jobs=-1,\n    )\n```\n\nLog:\n\n```\n__call__(self=make_scorer(roc_auc_score, needs_threshold=True), clf=DecisionTreeClassifier(class_weight='auto', crit...resort=False, random_state=None, splitter='best'), X=memmap([[ 2.14686672e-01, 0.00000000e+00, 0...000000e+00, 1.00000000e+00, 0.00000000e+00]]), y=memmap([0, 0, 0, ..., 0, 0, 0]), sample_weight=None) \n170 \n171 except (NotImplementedError, AttributeError): \n172 y_pred = clf.predict_proba(X) \n173 \n174 if y_type == \"binary\": \n--> 175 y_pred = y_pred[:, 1] \ny_pred = array([[ 1.], \n[ 1.], \n[ 1.], \n..., \n[ 1.], \n[ 1.], \n[ 1.]]) \n176 elif isinstance(y_pred, list): \n177 y_pred = np.vstack([p[:, -1] for p in y_pred]).T \n178 \n179 if sample_weight is not None: \n\nIndexError: index 1 is out of bounds for axis 1 with size 1 \n```\n\nIt looks there like you might have been training your `DecisionTreeClassifier` on a single class... what does the `y` you pass to `GridSearchCV.fit` look like?\n\nYes, this error message is not very helpful.\n\nI ran into this error and you are correct @amueller about the single class explanation. Here's what my data looks like.\r\n\r\n```\r\nX_test.shape: (750, 34)\r\ny_test.shape: (750,)\r\ny_test value_counts: True    750\r\n```\r\nSo my data contains a single class: `True`.\r\n\r\nPerhaps, a more descriptive error message would help. Something along the line of your comment: `It looks like you might have a single class`. Looking at line 175 long enough may give that away too.\nCan someone help with this issue? I do not know how to fix it still.  Thank you!\nclf = ExtraTreesClassifier()\r\nmy_cv = TimeSeriesSplit(n_splits=5)  # time series split\r\n\r\nparam_grid = {\r\n              'n_estimators': [100, 300, 500, 700, 1000, 2000, 5000],\r\n              'min_samples_split': [2, 5, 10],\r\n              'min_samples_leaf': range(2,20,2),\r\n              'bootstrap': [True, False]\r\n             }\r\n\r\nclf = GridSearchCV(estimator=clf, param_grid=param_grid, cv=my_cv, n_jobs=-1, scoring='roc_auc', return_train_score=False)\r\nclf.fit(X, y)\n@liuwanfei you likely have just one class in ```y```, at least looks like it was an issue for most people in this thread. It should be an error message with a clear text instead of the exception.\nYes, what the people above have mentioned is correct - if you train with one class you _will_ get this error.\r\n\r\nHowever, if you have a look at my code, I generated a dataset which has 2 classes so that was not the case with me. What _was_ the causing the issue is that my param grid was set up with a subtle error.  Remember the \"roc_auc\" scorer is using probabilities as inputs to create the ROC curve, and in my example above, my parameter space for `n_components:` was `[1,2,3,4]`. \r\n\r\nIf you think about it, a GMM with one component will output only one probability. Thus, the output of `model.predict_proba` will be a one dimensional array which is why an `IndexError` occurs on `pred[:, 1]`.\r\n\r\nSo, for your case, see if one of the parameter combinations might not result in the classifier being constrained to predicting a single class. \r\n\r\nP.S I only realised this _now_, almost 2 years after the post. lol\nI and @reshamas are working on it",
  "created_at": "2018-10-30T01:08:40Z",
  "version": "0.21",
  "FAIL_TO_PASS": "[\"sklearn/metrics/tests/test_score_objects.py::test_thresholded_scorers\"]",
  "PASS_TO_PASS": "[\"sklearn/metrics/tests/test_score_objects.py::test_all_scorers_repr\", \"sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring\", \"sklearn/metrics/tests/test_score_objects.py::test_check_scoring_gridsearchcv\", \"sklearn/metrics/tests/test_score_objects.py::test_make_scorer\", \"sklearn/metrics/tests/test_score_objects.py::test_classification_scores\", \"sklearn/metrics/tests/test_score_objects.py::test_regression_scorers\", \"sklearn/metrics/tests/test_score_objects.py::test_thresholded_scorers_multilabel_indicator_data\", \"sklearn/metrics/tests/test_score_objects.py::test_supervised_cluster_scorers\", \"sklearn/metrics/tests/test_score_objects.py::test_raises_on_score_list\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_sample_weight\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[explained_variance]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[r2]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[max_error]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_median_absolute_error]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_absolute_error]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_squared_error]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_squared_log_error]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[accuracy]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[balanced_accuracy]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[average_precision]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_log_loss]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[brier_score_loss]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[adjusted_rand_score]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[homogeneity_score]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[completeness_score]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[v_measure_score]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[mutual_info_score]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[adjusted_mutual_info_score]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[normalized_mutual_info_score]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[fowlkes_mallows_score]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_macro]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_micro]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_samples]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_weighted]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_macro]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_micro]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_samples]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_weighted]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_macro]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_micro]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_samples]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_weighted]\", \"sklearn/metrics/tests/test_score_objects.py::test_scoring_is_not_metric\"]",
  "environment_setup_commit": "7813f7efb5b2012412888b69e73d76f2df2b50b6",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.964733",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}