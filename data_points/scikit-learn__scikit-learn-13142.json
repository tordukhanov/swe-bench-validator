{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-13142",
  "base_commit": "1c8668b0a021832386470ddf740d834e02c66f69",
  "patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -257,11 +257,6 @@ def fit_predict(self, X, y=None):\n                 best_params = self._get_parameters()\n                 best_n_iter = n_iter\n \n-        # Always do a final e-step to guarantee that the labels returned by\n-        # fit_predict(X) are always consistent with fit(X).predict(X)\n-        # for any value of max_iter and tol (and any random_state).\n-        _, log_resp = self._e_step(X)\n-\n         if not self.converged_:\n             warnings.warn('Initialization %d did not converge. '\n                           'Try different init parameters, '\n@@ -273,6 +268,11 @@ def fit_predict(self, X, y=None):\n         self.n_iter_ = best_n_iter\n         self.lower_bound_ = max_lower_bound\n \n+        # Always do a final e-step to guarantee that the labels returned by\n+        # fit_predict(X) are always consistent with fit(X).predict(X)\n+        # for any value of max_iter and tol (and any random_state).\n+        _, log_resp = self._e_step(X)\n+\n         return log_resp.argmax(axis=1)\n \n     def _e_step(self, X):\n",
  "test_patch": "diff --git a/sklearn/mixture/tests/test_bayesian_mixture.py b/sklearn/mixture/tests/test_bayesian_mixture.py\n--- a/sklearn/mixture/tests/test_bayesian_mixture.py\n+++ b/sklearn/mixture/tests/test_bayesian_mixture.py\n@@ -451,6 +451,15 @@ def test_bayesian_mixture_fit_predict(seed, max_iter, tol):\n         assert_array_equal(Y_pred1, Y_pred2)\n \n \n+def test_bayesian_mixture_fit_predict_n_init():\n+    # Check that fit_predict is equivalent to fit.predict, when n_init > 1\n+    X = np.random.RandomState(0).randn(1000, 5)\n+    gm = BayesianGaussianMixture(n_components=5, n_init=10, random_state=0)\n+    y_pred1 = gm.fit_predict(X)\n+    y_pred2 = gm.predict(X)\n+    assert_array_equal(y_pred1, y_pred2)\n+\n+\n def test_bayesian_mixture_predict_predict_proba():\n     # this is the same test as test_gaussian_mixture_predict_predict_proba()\n     rng = np.random.RandomState(0)\ndiff --git a/sklearn/mixture/tests/test_gaussian_mixture.py b/sklearn/mixture/tests/test_gaussian_mixture.py\n--- a/sklearn/mixture/tests/test_gaussian_mixture.py\n+++ b/sklearn/mixture/tests/test_gaussian_mixture.py\n@@ -598,6 +598,15 @@ def test_gaussian_mixture_fit_predict(seed, max_iter, tol):\n         assert_greater(adjusted_rand_score(Y, Y_pred2), .95)\n \n \n+def test_gaussian_mixture_fit_predict_n_init():\n+    # Check that fit_predict is equivalent to fit.predict, when n_init > 1\n+    X = np.random.RandomState(0).randn(1000, 5)\n+    gm = GaussianMixture(n_components=5, n_init=5, random_state=0)\n+    y_pred1 = gm.fit_predict(X)\n+    y_pred2 = gm.predict(X)\n+    assert_array_equal(y_pred1, y_pred2)\n+\n+\n def test_gaussian_mixture_fit():\n     # recover the ground truth\n     rng = np.random.RandomState(0)\n",
  "problem_statement": "GaussianMixture predict and fit_predict disagree when n_init>1\n#### Description\r\nWhen `n_init` is specified in GaussianMixture, the results of fit_predict(X) and predict(X) are often different.  The `test_gaussian_mixture_fit_predict` unit test doesn't catch this because it does not set `n_init`.\r\n\r\n#### Steps/Code to Reproduce\r\n```\r\npython\r\nfrom sklearn.mixture import GaussianMixture\r\nfrom sklearn.utils.testing import assert_array_equal\r\nimport numpy\r\nX = numpy.random.randn(1000,5)\r\nprint 'no n_init'\r\ngm = GaussianMixture(n_components=5)\r\nc1 = gm.fit_predict(X)\r\nc2 = gm.predict(X)\r\nassert_array_equal(c1,c2)\r\nprint 'n_init=5'\r\ngm = GaussianMixture(n_components=5, n_init=5)\r\nc1 = gm.fit_predict(X)\r\nc2 = gm.predict(X)\r\nassert_array_equal(c1,c2)\r\n```\r\n\r\n#### Expected Results\r\n```\r\nno n_init\r\nn_init=5\r\n```\r\nNo exceptions.\r\n\r\n#### Actual Results\r\n```\r\nno n_init\r\nn_init=5\r\nTraceback (most recent call last):\r\n  File \"test_gm.py\", line 17, in <module>\r\n    assert_array_equal(c1,c2)\r\n  File \"/home/scott/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py\", line 872, in assert_array_equal\r\n    verbose=verbose, header='Arrays are not equal')\r\n  File \"/home/scott/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py\", line 796, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not equal\r\n\r\n(mismatch 88.6%)\r\n x: array([4, 0, 1, 1, 1, 3, 3, 4, 4, 2, 0, 0, 1, 2, 0, 2, 0, 1, 3, 1, 1, 3,\r\n       2, 1, 0, 2, 1, 0, 2, 0, 3, 1, 2, 3, 3, 1, 0, 2, 2, 0, 3, 0, 2, 0,\r\n       4, 2, 3, 0, 4, 2, 4, 1, 0, 2, 2, 1, 3, 2, 1, 4, 0, 2, 2, 1, 1, 2,...\r\n y: array([4, 1, 0, 2, 2, 1, 1, 4, 4, 0, 4, 1, 0, 3, 1, 0, 2, 2, 1, 2, 0, 0,\r\n       1, 0, 4, 1, 0, 4, 0, 1, 1, 2, 3, 1, 4, 0, 1, 4, 4, 4, 0, 1, 0, 2,\r\n       4, 1, 1, 2, 4, 3, 4, 0, 2, 3, 2, 3, 0, 0, 2, 3, 3, 3, 3, 0, 3, 2,...\r\n```\r\n\r\n#### Versions\r\n```\r\nSystem:\r\n    python: 2.7.15rc1 (default, Nov 12 2018, 14:31:15)  [GCC 7.3.0]\r\n   machine: Linux-4.15.0-43-generic-x86_64-with-Ubuntu-18.04-bionic\r\nexecutable: /usr/bin/python\r\n\r\nBLAS:\r\n    macros: HAVE_CBLAS=None, NO_ATLAS_INFO=-1\r\ncblas_libs: cblas\r\n  lib_dirs: /usr/lib/x86_64-linux-gnu\r\n\r\nPython deps:\r\n    Cython: 0.28.5\r\n     scipy: 1.2.0\r\nsetuptools: 39.0.1\r\n       pip: 19.0.1\r\n     numpy: 1.16.0\r\n    pandas: 0.23.1\r\n   sklearn: 0.20.2\r\n```\n",
  "hints_text": "Indeed the code in fit_predict and the one in predict are not exactly consistent. This should be fixed but we would need to check the math to choose the correct variant, add a test and remove the other one.\nI don't think the math is wrong or inconsistent.  I think it's a matter of `fit_predict` returning the fit from the last of `n_iter` iterations, when it should be returning the fit from the _best_ of the iterations.  That is, the last call to `self._e_step()` (base.py:263) should be moved to just before the return, after `self._set_parameters(best_params)` restores the best solution.\nSeems good indeed. When looking quickly you can miss the fact that `_e_step` uses the parameters even if not passed as arguments because they are attributes of the estimator. That's what happened to me :)\r\n\r\n Would you submit a PR ?",
  "created_at": "2019-02-12T14:32:37Z",
  "version": "0.21",
  "FAIL_TO_PASS": "[\"sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_fit_predict_n_init\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_predict_n_init\"]",
  "PASS_TO_PASS": "[\"sklearn/mixture/tests/test_bayesian_mixture.py::test_log_dirichlet_norm\", \"sklearn/mixture/tests/test_bayesian_mixture.py::test_log_wishart_norm\", \"sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_covariance_type\", \"sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_weight_concentration_prior_type\", \"sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_weights_prior_initialisation\", \"sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_mean_prior_initialisation\", \"sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_precisions_prior_initialisation\", \"sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_check_is_fitted\", \"sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_weights\", \"sklearn/mixture/tests/test_bayesian_mixture.py::test_monotonic_likelihood\", \"sklearn/mixture/tests/test_bayesian_mixture.py::test_compare_covar_type\", \"sklearn/mixture/tests/test_bayesian_mixture.py::test_check_covariance_precision\", \"sklearn/mixture/tests/test_bayesian_mixture.py::test_invariant_translation\", \"sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_fit_predict[0-2-1e-07]\", \"sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_fit_predict[1-2-0.1]\", \"sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_fit_predict[3-300-1e-07]\", \"sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_fit_predict[4-300-0.1]\", \"sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_predict_predict_proba\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_attributes\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_check_X\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_check_weights\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_check_means\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_check_precisions\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_suffstat_sk_full\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_suffstat_sk_tied\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_suffstat_sk_diag\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_suffstat_sk_spherical\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_compute_log_det_cholesky\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_log_probabilities\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_estimate_log_prob_resp\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_predict_predict_proba\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_predict[0-2-1e-07]\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_predict[1-2-0.1]\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_predict[3-300-1e-07]\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_predict[4-300-0.1]\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_best_params\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_convergence_warning\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_multiple_init\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_n_parameters\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_bic_1d_1component\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_aic_bic\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_verbose\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_warm_start[0]\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_warm_start[1]\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_warm_start[2]\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_convergence_detected_with_warm_start\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_score\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_score_samples\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_monotonic_likelihood\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_regularisation\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_property\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_sample\", \"sklearn/mixture/tests/test_gaussian_mixture.py::test_init\"]",
  "environment_setup_commit": "7813f7efb5b2012412888b69e73d76f2df2b50b6",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.989682",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}