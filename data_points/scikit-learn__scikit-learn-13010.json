{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-13010",
  "base_commit": "365c1b2071b4020cdce1cb81da1cba43a004e370",
  "patch": "diff --git a/sklearn/utils/extmath.py b/sklearn/utils/extmath.py\n--- a/sklearn/utils/extmath.py\n+++ b/sklearn/utils/extmath.py\n@@ -658,6 +658,38 @@ def make_nonnegative(X, min_value=0):\n     return X\n \n \n+# Use at least float64 for the accumulating functions to avoid precision issue\n+# see https://github.com/numpy/numpy/issues/9393. The float64 is also retained\n+# as it is in case the float overflows\n+def _safe_accumulator_op(op, x, *args, **kwargs):\n+    \"\"\"\n+    This function provides numpy accumulator functions with a float64 dtype\n+    when used on a floating point input. This prevents accumulator overflow on\n+    smaller floating point dtypes.\n+\n+    Parameters\n+    ----------\n+    op : function\n+        A numpy accumulator function such as np.mean or np.sum\n+    x : numpy array\n+        A numpy array to apply the accumulator function\n+    *args : positional arguments\n+        Positional arguments passed to the accumulator function after the\n+        input x\n+    **kwargs : keyword arguments\n+        Keyword arguments passed to the accumulator function\n+\n+    Returns\n+    -------\n+    result : The output of the accumulator function passed to this function\n+    \"\"\"\n+    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:\n+        result = op(x, *args, **kwargs, dtype=np.float64)\n+    else:\n+        result = op(x, *args, **kwargs)\n+    return result\n+\n+\n def _incremental_mean_and_var(X, last_mean, last_variance, last_sample_count):\n     \"\"\"Calculate mean update and a Youngs and Cramer variance update.\n \n@@ -708,12 +740,7 @@ def _incremental_mean_and_var(X, last_mean, last_variance, last_sample_count):\n     # new = the current increment\n     # updated = the aggregated stats\n     last_sum = last_mean * last_sample_count\n-    if np.issubdtype(X.dtype, np.floating) and X.dtype.itemsize < 8:\n-        # Use at least float64 for the accumulator to avoid precision issues;\n-        # see https://github.com/numpy/numpy/issues/9393\n-        new_sum = np.nansum(X, axis=0, dtype=np.float64).astype(X.dtype)\n-    else:\n-        new_sum = np.nansum(X, axis=0)\n+    new_sum = _safe_accumulator_op(np.nansum, X, axis=0)\n \n     new_sample_count = np.sum(~np.isnan(X), axis=0)\n     updated_sample_count = last_sample_count + new_sample_count\n@@ -723,7 +750,8 @@ def _incremental_mean_and_var(X, last_mean, last_variance, last_sample_count):\n     if last_variance is None:\n         updated_variance = None\n     else:\n-        new_unnormalized_variance = np.nanvar(X, axis=0) * new_sample_count\n+        new_unnormalized_variance = (\n+            _safe_accumulator_op(np.nanvar, X, axis=0) * new_sample_count)\n         last_unnormalized_variance = last_variance * last_sample_count\n \n         with np.errstate(divide='ignore', invalid='ignore'):\ndiff --git a/sklearn/utils/validation.py b/sklearn/utils/validation.py\n--- a/sklearn/utils/validation.py\n+++ b/sklearn/utils/validation.py\n@@ -34,14 +34,18 @@\n \n def _assert_all_finite(X, allow_nan=False):\n     \"\"\"Like assert_all_finite, but only for ndarray.\"\"\"\n+    # validation is also imported in extmath\n+    from .extmath import _safe_accumulator_op\n+\n     if _get_config()['assume_finite']:\n         return\n     X = np.asanyarray(X)\n     # First try an O(n) time, O(1) space solution for the common case that\n     # everything is finite; fall back to O(n) space np.isfinite to prevent\n-    # false positives from overflow in sum method.\n+    # false positives from overflow in sum method. The sum is also calculated\n+    # safely to reduce dtype induced overflows.\n     is_float = X.dtype.kind in 'fc'\n-    if is_float and np.isfinite(X.sum()):\n+    if is_float and (np.isfinite(_safe_accumulator_op(np.sum, X))):\n         pass\n     elif is_float:\n         msg_err = \"Input contains {} or a value too large for {!r}.\"\n",
  "test_patch": "diff --git a/sklearn/preprocessing/tests/test_data.py b/sklearn/preprocessing/tests/test_data.py\n--- a/sklearn/preprocessing/tests/test_data.py\n+++ b/sklearn/preprocessing/tests/test_data.py\n@@ -450,6 +450,31 @@ def test_scaler_2d_arrays():\n     assert X_scaled is not X\n \n \n+def test_scaler_float16_overflow():\n+    # Test if the scaler will not overflow on float16 numpy arrays\n+    rng = np.random.RandomState(0)\n+    # float16 has a maximum of 65500.0. On the worst case 5 * 200000 is 100000\n+    # which is enough to overflow the data type\n+    X = rng.uniform(5, 10, [200000, 1]).astype(np.float16)\n+\n+    with np.errstate(over='raise'):\n+        scaler = StandardScaler().fit(X)\n+        X_scaled = scaler.transform(X)\n+\n+    # Calculate the float64 equivalent to verify result\n+    X_scaled_f64 = StandardScaler().fit_transform(X.astype(np.float64))\n+\n+    # Overflow calculations may cause -inf, inf, or nan. Since there is no nan\n+    # input, all of the outputs should be finite. This may be redundant since a\n+    # FloatingPointError exception will be thrown on overflow above.\n+    assert np.all(np.isfinite(X_scaled))\n+\n+    # The normal distribution is very unlikely to go above 4. At 4.0-8.0 the\n+    # float16 precision is 2^-8 which is around 0.004. Thus only 2 decimals are\n+    # checked to account for precision differences.\n+    assert_array_almost_equal(X_scaled, X_scaled_f64, decimal=2)\n+\n+\n def test_handle_zeros_in_scale():\n     s1 = np.array([0, 1, 2, 3])\n     s2 = _handle_zeros_in_scale(s1, copy=True)\n",
  "problem_statement": "StandardScaler fit overflows on float16\n#### Description\r\n\r\nWhen using StandardScaler on a large float16 numpy array the mean and std calculation overflows. I can convert the array to a larger precision but when working with a larger dataset the memory saved by using float16 on smaller numbers kind of matter. The error is mostly on numpy. Adding the dtype on the mean/std calculation does it but I'm not sure if that how people here would like to do it.\r\n\r\n#### Steps/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\nsample = np.full([10_000_000, 1], 10.0, dtype=np.float16)\r\nStandardScaler().fit_transform(sample)\r\n```\r\n\r\n#### Expected Results\r\n\r\nThe normalized array\r\n\r\n#### Actual Results\r\n\r\n```\r\n/opt/conda/lib/python3.6/site-packages/numpy/core/_methods.py:36: RuntimeWarning: overflow encountered in reduce\r\n  return umr_sum(a, axis, dtype, out, keepdims, initial)\r\n/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\r\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\n/opt/conda/lib/python3.6/site-packages/numpy/core/_methods.py:36: RuntimeWarning: overflow encountered in reduce\r\n  return umr_sum(a, axis, dtype, out, keepdims, initial)\r\n/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:765: RuntimeWarning: invalid value encountered in true_divide\r\n  X /= self.scale_\r\n\r\narray([[nan],\r\n       [nan],\r\n       [nan],\r\n       ...,\r\n       [nan],\r\n       [nan],\r\n       [nan]], dtype=float16)\r\n```\r\n\r\n#### Versions\r\n\r\n```\r\nSystem:\r\n    python: 3.6.6 |Anaconda, Inc.| (default, Oct  9 2018, 12:34:16)  [GCC 7.3.0]\r\nexecutable: /opt/conda/bin/python\r\n   machine: Linux-4.9.0-5-amd64-x86_64-with-debian-9.4\r\n\r\nBLAS:\r\n    macros: SCIPY_MKL_H=None, HAVE_CBLAS=None\r\n  lib_dirs: /opt/conda/lib\r\ncblas_libs: mkl_rt, pthread\r\n\r\nPython deps:\r\n       pip: 18.1\r\nsetuptools: 39.1.0\r\n   sklearn: 0.20.2\r\n     numpy: 1.16.0\r\n     scipy: 1.1.0\r\n    Cython: 0.29.2\r\n    pandas: 0.23.4\r\n```\r\n\n",
  "hints_text": "If adding dtype on the mean calculation is sufficient, that's probably a\ngood idea. Pull request?\n",
  "created_at": "2019-01-18T07:14:27Z",
  "version": "0.21",
  "FAIL_TO_PASS": "[\"sklearn/preprocessing/tests/test_data.py::test_scaler_float16_overflow\"]",
  "PASS_TO_PASS": "[\"sklearn/preprocessing/tests/test_data.py::test_polynomial_features\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_feature_names\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_feature_array_order\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[1-True-False-int]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[2-True-False-int]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[2-True-False-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[2-True-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[3-False-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[3-False-True-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[4-False-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[4-False-True-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X[1-True-False-int]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X[2-True-False-int]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X[2-True-False-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X[2-True-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X[3-False-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X[3-False-True-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_floats[2-True-False-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_floats[2-True-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_floats[3-False-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_floats[3-False-True-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[0-2-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[1-2-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[2-2-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[0-3-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[1-3-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[2-3-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[0-2-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[1-2-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[2-2-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[0-3-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[1-3-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[2-3-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_degree_4[True-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_degree_4[True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_degree_4[False-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_degree_4[False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[2-1-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[2-2-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[3-1-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[3-2-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[3-3-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[2-1-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[2-2-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[3-1-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[3-2-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[3-3-False]\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_1d\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_dtype\", \"sklearn/preprocessing/tests/test_data.py::test_scale_1d\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_numerical_stability\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_2d_arrays\", \"sklearn/preprocessing/tests/test_data.py::test_handle_zeros_in_scale\", \"sklearn/preprocessing/tests/test_data.py::test_minmax_scaler_partial_fit\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_partial_fit\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_partial_fit_numerical_stability\", \"sklearn/preprocessing/tests/test_data.py::test_partial_fit_sparse_input\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_trasform_with_partial_fit\", \"sklearn/preprocessing/tests/test_data.py::test_min_max_scaler_iris\", \"sklearn/preprocessing/tests/test_data.py::test_min_max_scaler_zero_variance_features\", \"sklearn/preprocessing/tests/test_data.py::test_minmax_scale_axis1\", \"sklearn/preprocessing/tests/test_data.py::test_min_max_scaler_1d\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_without_centering\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[asarray-True-True]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[asarray-True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[asarray-False-True]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[asarray-False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[csc_matrix-True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[csc_matrix-False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[csr_matrix-True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[csr_matrix-False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_return_identity\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_int\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_without_copy\", \"sklearn/preprocessing/tests/test_data.py::test_scale_sparse_with_mean_raise_exception\", \"sklearn/preprocessing/tests/test_data.py::test_scale_input_finiteness_validation\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_error_sparse\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_attributes[X0-True-True]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_attributes[X0-True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_attributes[X0-False-True]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_attributes[X0-False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_attributes[X1-True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_attributes[X1-False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_col_zero_sparse\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_2d_arrays\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[positive-0]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[positive-0.05]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[positive-0.1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[positive-0.5]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[positive-1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[negative-0]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[negative-0.05]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[negative-0.1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[negative-0.5]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[negative-1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[zeros-0]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[zeros-0.05]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[zeros-0.1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[zeros-0.5]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[zeros-1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[None-0]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[None-0.05]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[None-0.1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[None-0.5]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[None-1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_transform_one_row_csr\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_iris\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_iris_quantiles\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_iris\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_check_error\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_sparse_ignore_zeros\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_dense_toy\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_subsampling\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_sparse_toy\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_axis1\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_bounds\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_and_inverse\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_nan\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_invalid_range\", \"sklearn/preprocessing/tests/test_data.py::test_scale_function_without_centering\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scale_axis1\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scale_1d_array\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_zero_variance_features\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_zero_variance_features\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_large_negative_value\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_transform_one_row_csr\", \"sklearn/preprocessing/tests/test_data.py::test_warning_scaling_integers\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_1d\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_partial_fit\", \"sklearn/preprocessing/tests/test_data.py::test_normalizer_l1\", \"sklearn/preprocessing/tests/test_data.py::test_normalizer_l2\", \"sklearn/preprocessing/tests/test_data.py::test_normalizer_max\", \"sklearn/preprocessing/tests/test_data.py::test_normalize\", \"sklearn/preprocessing/tests/test_data.py::test_binarizer\", \"sklearn/preprocessing/tests/test_data.py::test_center_kernel\", \"sklearn/preprocessing/tests/test_data.py::test_cv_pipeline_precomputed\", \"sklearn/preprocessing/tests/test_data.py::test_fit_transform\", \"sklearn/preprocessing/tests/test_data.py::test_add_dummy_feature\", \"sklearn/preprocessing/tests/test_data.py::test_add_dummy_feature_coo\", \"sklearn/preprocessing/tests/test_data.py::test_add_dummy_feature_csc\", \"sklearn/preprocessing/tests/test_data.py::test_add_dummy_feature_csr\", \"sklearn/preprocessing/tests/test_data.py::test_fit_cold_start\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_valid_axis\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_notfitted[box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_notfitted[yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X0-True-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X0-True-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X0-False-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X0-False-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X1-True-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X1-True-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X1-False-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X1-False-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_1d\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_2d\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_boxcox_strictly_positive_exception\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_yeojohnson_any_input[X0]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_yeojohnson_any_input[X1]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_yeojohnson_any_input[X2]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_yeojohnson_any_input[X3]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_shape_exception[box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_shape_exception[yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_method_exception\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_lambda_zero\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_lambda_one\", \"sklearn/preprocessing/tests/test_data.py::test_optimization_power_transformer[box-cox-0.1]\", \"sklearn/preprocessing/tests/test_data.py::test_optimization_power_transformer[box-cox-0.5]\", \"sklearn/preprocessing/tests/test_data.py::test_optimization_power_transformer[yeo-johnson-0.1]\", \"sklearn/preprocessing/tests/test_data.py::test_optimization_power_transformer[yeo-johnson-0.5]\", \"sklearn/preprocessing/tests/test_data.py::test_optimization_power_transformer[yeo-johnson-1.0]\", \"sklearn/preprocessing/tests/test_data.py::test_yeo_johnson_darwin_example\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_fit_transform[True-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_fit_transform[True-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_fit_transform[False-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_fit_transform[False-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_True[True-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_True[True-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_True[False-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_True[False-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_False[True-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_False[True-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_False[False-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_False[False-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transform_default_method\"]",
  "environment_setup_commit": "7813f7efb5b2012412888b69e73d76f2df2b50b6",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.978979",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}