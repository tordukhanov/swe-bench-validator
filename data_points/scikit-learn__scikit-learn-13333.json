{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-13333",
  "base_commit": "04a5733b86bba57a48520b97b9c0a5cd325a1b9a",
  "patch": "diff --git a/sklearn/preprocessing/data.py b/sklearn/preprocessing/data.py\n--- a/sklearn/preprocessing/data.py\n+++ b/sklearn/preprocessing/data.py\n@@ -424,7 +424,7 @@ def minmax_scale(X, feature_range=(0, 1), axis=0, copy=True):\n         X_scaled = X_std * (max - min) + min\n \n     where min, max = feature_range.\n- \n+\n     The transformation is calculated as (when ``axis=0``)::\n \n        X_scaled = scale * X + min - X.min(axis=0) * scale\n@@ -592,7 +592,7 @@ class StandardScaler(BaseEstimator, TransformerMixin):\n     -----\n     NaNs are treated as missing values: disregarded in fit, and maintained in\n     transform.\n-    \n+\n     We use a biased estimator for the standard deviation, equivalent to\n     `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n     affect model performance.\n@@ -2041,9 +2041,13 @@ class QuantileTransformer(BaseEstimator, TransformerMixin):\n \n     Parameters\n     ----------\n-    n_quantiles : int, optional (default=1000)\n+    n_quantiles : int, optional (default=1000 or n_samples)\n         Number of quantiles to be computed. It corresponds to the number\n         of landmarks used to discretize the cumulative distribution function.\n+        If n_quantiles is larger than the number of samples, n_quantiles is set\n+        to the number of samples as a larger number of quantiles does not give\n+        a better approximation of the cumulative distribution function\n+        estimator.\n \n     output_distribution : str, optional (default='uniform')\n         Marginal distribution for the transformed data. The choices are\n@@ -2072,6 +2076,10 @@ class QuantileTransformer(BaseEstimator, TransformerMixin):\n \n     Attributes\n     ----------\n+    n_quantiles_ : integer\n+        The actual number of quantiles used to discretize the cumulative\n+        distribution function.\n+\n     quantiles_ : ndarray, shape (n_quantiles, n_features)\n         The values corresponding the quantiles of reference.\n \n@@ -2218,10 +2226,19 @@ def fit(self, X, y=None):\n                                                        self.subsample))\n \n         X = self._check_inputs(X)\n+        n_samples = X.shape[0]\n+\n+        if self.n_quantiles > n_samples:\n+            warnings.warn(\"n_quantiles (%s) is greater than the total number \"\n+                          \"of samples (%s). n_quantiles is set to \"\n+                          \"n_samples.\"\n+                          % (self.n_quantiles, n_samples))\n+        self.n_quantiles_ = max(1, min(self.n_quantiles, n_samples))\n+\n         rng = check_random_state(self.random_state)\n \n         # Create the quantiles of reference\n-        self.references_ = np.linspace(0, 1, self.n_quantiles,\n+        self.references_ = np.linspace(0, 1, self.n_quantiles_,\n                                        endpoint=True)\n         if sparse.issparse(X):\n             self._sparse_fit(X, rng)\n@@ -2443,9 +2460,13 @@ def quantile_transform(X, axis=0, n_quantiles=1000,\n         Axis used to compute the means and standard deviations along. If 0,\n         transform each feature, otherwise (if 1) transform each sample.\n \n-    n_quantiles : int, optional (default=1000)\n+    n_quantiles : int, optional (default=1000 or n_samples)\n         Number of quantiles to be computed. It corresponds to the number\n         of landmarks used to discretize the cumulative distribution function.\n+        If n_quantiles is larger than the number of samples, n_quantiles is set\n+        to the number of samples as a larger number of quantiles does not give\n+        a better approximation of the cumulative distribution function\n+        estimator.\n \n     output_distribution : str, optional (default='uniform')\n         Marginal distribution for the transformed data. The choices are\n",
  "test_patch": "diff --git a/sklearn/preprocessing/tests/test_data.py b/sklearn/preprocessing/tests/test_data.py\n--- a/sklearn/preprocessing/tests/test_data.py\n+++ b/sklearn/preprocessing/tests/test_data.py\n@@ -1260,6 +1260,13 @@ def test_quantile_transform_check_error():\n     assert_raise_message(ValueError,\n                          'Expected 2D array, got scalar array instead',\n                          transformer.transform, 10)\n+    # check that a warning is raised is n_quantiles > n_samples\n+    transformer = QuantileTransformer(n_quantiles=100)\n+    warn_msg = \"n_quantiles is set to n_samples\"\n+    with pytest.warns(UserWarning, match=warn_msg) as record:\n+        transformer.fit(X)\n+    assert len(record) == 1\n+    assert transformer.n_quantiles_ == X.shape[0]\n \n \n def test_quantile_transform_sparse_ignore_zeros():\n",
  "problem_statement": "DOC Improve doc of n_quantiles in QuantileTransformer \n#### Description\r\nThe `QuantileTransformer` uses numpy.percentile(X_train, .) as the estimator of the quantile function of the training data. To know this function perfectly we just need to take `n_quantiles=n_samples`. Then it is just a linear interpolation (which is done in the code afterwards). Therefore I don't think we should be able to choose `n_quantiles > n_samples` and we should prevent users from thinking that the higher `n_quantiles` the better the transformation. As mentioned by @GaelVaroquaux IRL it is however true that it can be relevant to choose `n_quantiles < n_samples` when `n_samples` is very large.\r\n\r\nI suggest to add more information on the impact of `n_quantiles` in the doc which currently reads:\r\n```python\r\nNumber of quantiles to be computed. It corresponds to the number of\r\nlandmarks used to discretize the cumulative distribution function.\r\n```\r\n\r\nFor example using 100 times more landmarks result in the same transformation\r\n```python\r\nimport numpy as np\r\nfrom sklearn.preprocessing import QuantileTransformer\r\nfrom sklearn.utils.testing import assert_allclose\r\n\r\nn_samples = 100\r\nX_train = np.random.randn(n_samples, 2)\r\nX_test = np.random.randn(1000, 2)\r\n\r\nqf_1 = QuantileTransformer(n_quantiles=n_samples)\r\nqf_1.fit(X_train)\r\nX_trans_1 = qf_1.transform(X_test)\r\n\r\nqf_2 = QuantileTransformer(n_quantiles=10000)\r\nqf_2.fit(X_train)\r\nX_trans_2 = qf_2.transform(X_test)\r\n\r\nassert_allclose(X_trans_1, X_trans_2)\r\n```\r\n\r\nInterestingly if you do not choose `n_quantiles > n_samples` correctly, the linear interpolation done afterwards does not correspond to the numpy.percentile(X_train, .) estimator. This is not \"wrong\" as these are only estimators of the true quantile function/cdf but I think it is confusing and would be better to stick with the original estimator. For instance, the following raises an AssertionError.\r\n```python\r\nimport numpy as np\r\nfrom sklearn.preprocessing import QuantileTransformer\r\nfrom sklearn.utils.testing import assert_allclose\r\n\r\nn_samples = 100\r\nX_train = np.random.randn(n_samples, 2)\r\nX_test = np.random.randn(1000, 2)\r\n\r\nqf_1 = QuantileTransformer(n_quantiles=n_samples)\r\nqf_1.fit(X_train)\r\nX_trans_1 = qf_1.transform(X_test)\r\n\r\nqf_2 = QuantileTransformer(n_quantiles=200)\r\nqf_2.fit(X_train)\r\nX_trans_2 = qf_2.transform(X_test)\r\n\r\nassert_allclose(X_trans_1, X_trans_2)\r\n```\n",
  "hints_text": "When you say prevent, do you mean that we should raise an error if\nn_quantiles > n_samples, or that we should adjust n_quantiles to\nmin(n_quantiles, n_samples)? I'd be in favour of the latter, perhaps with a\nwarning. And yes, improved documentation is always good (albeit often\nignored).\n\nI was only talking about the documentation but yes we should also change the behavior when n_quantiles > n_samples, which will require a deprecation cycle... Ideally the default of n_quantiles should be n_samples. And if too slow users can choose a n_quantiles value smaller than n_samples.\nI don't think the second behavior (when `n_quantiles=200`, which leads to a linear interpolation that does not correspond to the numpy.percentile(X_train, .) estimator) is the intended behavior. Unless someone tells me there is a valid reason behind it.\n> Therefore I don't think we should be able to choose n_quantiles > n_samples and we should prevent users from thinking that the higher n_quantiles the better the transformation.\n\n+1 for dynamically downgrading n_quantiles to \"self.n_quantiles_ = min(n_quantiles, n_samples)\" maybe with a warning.\n\nHowever, -1 for raising an error: people might not know in advance what the sample is.\n\n\nSounds good! I will open a PR.",
  "created_at": "2019-02-28T15:01:19Z",
  "version": "0.21",
  "FAIL_TO_PASS": "[\"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_check_error\"]",
  "PASS_TO_PASS": "[\"sklearn/preprocessing/tests/test_data.py::test_polynomial_features\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_feature_names\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_feature_array_order\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[1-True-False-int]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[2-True-False-int]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[2-True-False-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[2-True-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[3-False-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[3-False-True-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[4-False-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csc_X[4-False-True-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X[1-True-False-int]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X[2-True-False-int]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X[2-True-False-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X[2-True-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X[3-False-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X[3-False-True-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_floats[2-True-False-float32]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_floats[2-True-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_floats[3-False-False-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_floats[3-False-True-float64]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[0-2-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[1-2-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[2-2-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[0-3-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[1-3-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[2-3-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[0-2-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[1-2-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[2-2-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[0-3-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[1-3-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_zero_row[2-3-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_degree_4[True-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_degree_4[True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_degree_4[False-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_degree_4[False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[2-1-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[2-2-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[3-1-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[3-2-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[3-3-True]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[2-1-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[2-2-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[3-1-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[3-2-False]\", \"sklearn/preprocessing/tests/test_data.py::test_polynomial_features_csr_X_dim_edges[3-3-False]\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_1d\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_dtype\", \"sklearn/preprocessing/tests/test_data.py::test_scale_1d\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_numerical_stability\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_2d_arrays\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_float16_overflow\", \"sklearn/preprocessing/tests/test_data.py::test_handle_zeros_in_scale\", \"sklearn/preprocessing/tests/test_data.py::test_minmax_scaler_partial_fit\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_partial_fit\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_partial_fit_numerical_stability\", \"sklearn/preprocessing/tests/test_data.py::test_partial_fit_sparse_input\", \"sklearn/preprocessing/tests/test_data.py::test_standard_scaler_trasform_with_partial_fit\", \"sklearn/preprocessing/tests/test_data.py::test_min_max_scaler_iris\", \"sklearn/preprocessing/tests/test_data.py::test_min_max_scaler_zero_variance_features\", \"sklearn/preprocessing/tests/test_data.py::test_minmax_scale_axis1\", \"sklearn/preprocessing/tests/test_data.py::test_min_max_scaler_1d\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_without_centering\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[asarray-True-True]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[asarray-True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[asarray-False-True]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[asarray-False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[csc_matrix-True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[csc_matrix-False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[csr_matrix-True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_n_samples_seen_with_nan[csr_matrix-False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_return_identity\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_int\", \"sklearn/preprocessing/tests/test_data.py::test_scaler_without_copy\", \"sklearn/preprocessing/tests/test_data.py::test_scale_sparse_with_mean_raise_exception\", \"sklearn/preprocessing/tests/test_data.py::test_scale_input_finiteness_validation\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_error_sparse\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_attributes[X0-True-True]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_attributes[X0-True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_attributes[X0-False-True]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_attributes[X0-False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_attributes[X1-True-False]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_attributes[X1-False-False]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_col_zero_sparse\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_2d_arrays\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[positive-0]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[positive-0.05]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[positive-0.1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[positive-0.5]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[positive-1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[negative-0]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[negative-0.05]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[negative-0.1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[negative-0.5]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[negative-1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[zeros-0]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[zeros-0.05]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[zeros-0.1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[zeros-0.5]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[zeros-1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[None-0]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[None-0.05]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[None-0.1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[None-0.5]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_equivalence_dense_sparse[None-1]\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_transform_one_row_csr\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_iris\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_iris_quantiles\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_iris\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_sparse_ignore_zeros\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_dense_toy\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_subsampling\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_sparse_toy\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_axis1\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_bounds\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_and_inverse\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_nan\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_invalid_range\", \"sklearn/preprocessing/tests/test_data.py::test_scale_function_without_centering\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scale_axis1\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scale_1d_array\", \"sklearn/preprocessing/tests/test_data.py::test_robust_scaler_zero_variance_features\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_zero_variance_features\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_large_negative_value\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_transform_one_row_csr\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_1d\", \"sklearn/preprocessing/tests/test_data.py::test_maxabs_scaler_partial_fit\", \"sklearn/preprocessing/tests/test_data.py::test_normalizer_l1\", \"sklearn/preprocessing/tests/test_data.py::test_normalizer_l2\", \"sklearn/preprocessing/tests/test_data.py::test_normalizer_max\", \"sklearn/preprocessing/tests/test_data.py::test_normalize\", \"sklearn/preprocessing/tests/test_data.py::test_binarizer\", \"sklearn/preprocessing/tests/test_data.py::test_center_kernel\", \"sklearn/preprocessing/tests/test_data.py::test_cv_pipeline_precomputed\", \"sklearn/preprocessing/tests/test_data.py::test_fit_transform\", \"sklearn/preprocessing/tests/test_data.py::test_add_dummy_feature\", \"sklearn/preprocessing/tests/test_data.py::test_add_dummy_feature_coo\", \"sklearn/preprocessing/tests/test_data.py::test_add_dummy_feature_csc\", \"sklearn/preprocessing/tests/test_data.py::test_add_dummy_feature_csr\", \"sklearn/preprocessing/tests/test_data.py::test_fit_cold_start\", \"sklearn/preprocessing/tests/test_data.py::test_quantile_transform_valid_axis\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_notfitted[box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_notfitted[yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X0-True-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X0-True-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X0-False-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X0-False-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X1-True-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X1-True-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X1-False-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_inverse[X1-False-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_1d\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_2d\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_boxcox_strictly_positive_exception\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_yeojohnson_any_input[X0]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_yeojohnson_any_input[X1]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_yeojohnson_any_input[X2]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_yeojohnson_any_input[X3]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_shape_exception[box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_shape_exception[yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_method_exception\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_lambda_zero\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_lambda_one\", \"sklearn/preprocessing/tests/test_data.py::test_optimization_power_transformer[box-cox-0.1]\", \"sklearn/preprocessing/tests/test_data.py::test_optimization_power_transformer[box-cox-0.5]\", \"sklearn/preprocessing/tests/test_data.py::test_optimization_power_transformer[yeo-johnson-0.1]\", \"sklearn/preprocessing/tests/test_data.py::test_optimization_power_transformer[yeo-johnson-0.5]\", \"sklearn/preprocessing/tests/test_data.py::test_optimization_power_transformer[yeo-johnson-1.0]\", \"sklearn/preprocessing/tests/test_data.py::test_yeo_johnson_darwin_example\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_nans[yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_fit_transform[True-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_fit_transform[True-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_fit_transform[False-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_fit_transform[False-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_True[True-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_True[True-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_True[False-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_True[False-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_False[True-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_False[True-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_False[False-box-cox]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transformer_copy_False[False-yeo-johnson]\", \"sklearn/preprocessing/tests/test_data.py::test_power_transform_default_method\"]",
  "environment_setup_commit": "7813f7efb5b2012412888b69e73d76f2df2b50b6",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.992741",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}