{
  "repo": "pytest-dev/pytest",
  "instance_id": "pytest-dev__pytest-9133",
  "base_commit": "7720154ca023da23581d87244a31acf5b14979f2",
  "patch": "diff --git a/src/_pytest/pytester.py b/src/_pytest/pytester.py\n--- a/src/_pytest/pytester.py\n+++ b/src/_pytest/pytester.py\n@@ -589,6 +589,7 @@ def assert_outcomes(\n         xpassed: int = 0,\n         xfailed: int = 0,\n         warnings: int = 0,\n+        deselected: int = 0,\n     ) -> None:\n         \"\"\"Assert that the specified outcomes appear with the respective\n         numbers (0 means it didn't occur) in the text output from a test run.\"\"\"\n@@ -605,6 +606,7 @@ def assert_outcomes(\n             xpassed=xpassed,\n             xfailed=xfailed,\n             warnings=warnings,\n+            deselected=deselected,\n         )\n \n \ndiff --git a/src/_pytest/pytester_assertions.py b/src/_pytest/pytester_assertions.py\n--- a/src/_pytest/pytester_assertions.py\n+++ b/src/_pytest/pytester_assertions.py\n@@ -43,6 +43,7 @@ def assert_outcomes(\n     xpassed: int = 0,\n     xfailed: int = 0,\n     warnings: int = 0,\n+    deselected: int = 0,\n ) -> None:\n     \"\"\"Assert that the specified outcomes appear with the respective\n     numbers (0 means it didn't occur) in the text output from a test run.\"\"\"\n@@ -56,6 +57,7 @@ def assert_outcomes(\n         \"xpassed\": outcomes.get(\"xpassed\", 0),\n         \"xfailed\": outcomes.get(\"xfailed\", 0),\n         \"warnings\": outcomes.get(\"warnings\", 0),\n+        \"deselected\": outcomes.get(\"deselected\", 0),\n     }\n     expected = {\n         \"passed\": passed,\n@@ -65,5 +67,6 @@ def assert_outcomes(\n         \"xpassed\": xpassed,\n         \"xfailed\": xfailed,\n         \"warnings\": warnings,\n+        \"deselected\": deselected,\n     }\n     assert obtained == expected\n",
  "test_patch": "diff --git a/testing/test_pytester.py b/testing/test_pytester.py\n--- a/testing/test_pytester.py\n+++ b/testing/test_pytester.py\n@@ -861,3 +861,17 @@ def test_with_warning():\n     )\n     result = pytester.runpytest()\n     result.assert_outcomes(passed=1, warnings=1)\n+\n+\n+def test_pytester_outcomes_deselected(pytester: Pytester) -> None:\n+    pytester.makepyfile(\n+        \"\"\"\n+        def test_one():\n+            pass\n+\n+        def test_two():\n+            pass\n+        \"\"\"\n+    )\n+    result = pytester.runpytest(\"-k\", \"test_one\")\n+    result.assert_outcomes(passed=1, deselected=1)\n",
  "problem_statement": "Add a `deselected` parameter to `assert_outcomes()`\n<!--\r\nThanks for suggesting a feature!\r\n\r\nQuick check-list while suggesting features:\r\n-->\r\n\r\n#### What's the problem this feature will solve?\r\n<!-- What are you trying to do, that you are unable to achieve with pytest as it currently stands? -->\r\nI'd like to be able to use `pytester.RunResult.assert_outcomes()` to check deselected count.\r\n\r\n#### Describe the solution you'd like\r\n<!-- A clear and concise description of what you want to happen. -->\r\nAdd a `deselected` parameter to `pytester.RunResult.assert_outcomes()`\r\n\r\n<!-- Provide examples of real-world use cases that this would enable and how it solves the problem described above. -->\r\nPlugins that use `pytest_collection_modifyitems` to change the `items` and add change the deselected items need to be tested. Using `assert_outcomes()` to check the deselected count would be helpful.\r\n\r\n#### Alternative Solutions\r\n<!-- Have you tried to workaround the problem using a pytest plugin or other tools? Or a different approach to solving this issue? Please elaborate here. -->\r\nUse `parseoutcomes()` instead of `assert_outcomes()`. `parseoutcomes()` returns a dictionary that includes `deselected`, if there are any.\r\nHowever, if we have a series of tests, some that care about deselected, and some that don't, then we may have some tests using `assert_outcomes()` and some using `parseoutcomes()`, which is slightly annoying.\r\n\r\n#### Additional context\r\n<!-- Add any other context, links, etc. about the feature here. -->\r\n\n",
  "hints_text": "Sounds reasonable. üëç \nHi! I would like to work on this proposal. I went ahead and modified `pytester.RunResult.assert_outcomes()` to also compare the `deselected` count to that returned by `parseoutcomes()`. I also modified `pytester_assertions.assert_outcomes()` called by `pytester.RunResult.assert_outcomes()`.\r\n\r\nWhile all tests pass after the above changes, I think none of the tests presently would be using `assert_outcomes()` with deselected as a parameter since it's a new feature, so should I also write a test for the same?  Can you suggest how I should go about doing the same? I am a bit confused as there are many `test_` files.\r\n\r\nI am new here, so any help/feedback will be highly appreciated. Thanks!\nLooking at the [pull request 8952](https://github.com/pytest-dev/pytest/pull/8952/files) would be a good place to start.\r\nThat PR involved adding `warnings` to `assert_outcomes()`, so it will also show you where all you need to make sure to have changes.\r\nIt also includes a test. \r\nThe test for `deselected` will have to be different.\r\nUsing `-k` or `-m` will be effective in creating deselected tests.\r\nOne option is to have two tests,  `test_one` and `test_two`, for example, and call `pytester.runpytest(\"-k\", \"two\")`.\r\nThat should produce one passed and one deselected.\nOne exception to the necessary changes. I don't think `test_nose.py` mods would be necessary. Of course, I'd double check with @nicoddemus.\ndidn't mean to close",
  "created_at": "2021-09-29T14:28:54Z",
  "version": "7.0",
  "FAIL_TO_PASS": "[\"testing/test_pytester.py::test_pytester_outcomes_deselected\"]",
  "PASS_TO_PASS": "[\"testing/test_pytester.py::test_hookrecorder_basic[apiclass]\", \"testing/test_pytester.py::test_hookrecorder_basic[api]\", \"testing/test_pytester.py::TestSysModulesSnapshot::test_remove_added\", \"testing/test_pytester.py::TestSysModulesSnapshot::test_add_removed\", \"testing/test_pytester.py::TestSysModulesSnapshot::test_restore_reloaded\", \"testing/test_pytester.py::TestSysModulesSnapshot::test_preserve_modules\", \"testing/test_pytester.py::TestSysModulesSnapshot::test_preserve_container\", \"testing/test_pytester.py::TestSysPathsSnapshot::test_restore[path]\", \"testing/test_pytester.py::TestSysPathsSnapshot::test_restore[meta_path]\", \"testing/test_pytester.py::TestSysPathsSnapshot::test_preserve_container[path]\", \"testing/test_pytester.py::TestSysPathsSnapshot::test_preserve_container[meta_path]\", \"testing/test_pytester.py::test_linematcher_with_nonlist\", \"testing/test_pytester.py::test_linematcher_match_failure\", \"testing/test_pytester.py::test_linematcher_consecutive\", \"testing/test_pytester.py::test_linematcher_no_matching[no_fnmatch_line]\", \"testing/test_pytester.py::test_linematcher_no_matching[no_re_match_line]\", \"testing/test_pytester.py::test_linematcher_no_matching_after_match\", \"testing/test_pytester.py::test_linematcher_string_api\", \"testing/test_pytester.py::test_pytest_addopts_before_pytester\", \"testing/test_pytester.py::test_run_result_repr\", \"testing/test_pytester.py::test_parse_summary_line_always_plural\", \"testing/test_pytester.py::test_parseconfig\", \"testing/test_pytester.py::test_pytester_runs_with_plugin\", \"testing/test_pytester.py::test_pytester_with_doctest\", \"testing/test_pytester.py::test_runresult_assertion_on_xfail\", \"testing/test_pytester.py::test_runresult_assertion_on_xpassed\", \"testing/test_pytester.py::test_xpassed_with_strict_is_considered_a_failure\", \"testing/test_pytester.py::test_makepyfile_unicode\", \"testing/test_pytester.py::test_makepyfile_utf8\", \"testing/test_pytester.py::TestInlineRunModulesCleanup::test_inline_run_test_module_not_cleaned_up\", \"testing/test_pytester.py::TestInlineRunModulesCleanup::test_inline_run_taking_and_restoring_a_sys_modules_snapshot\", \"testing/test_pytester.py::TestInlineRunModulesCleanup::test_inline_run_sys_modules_snapshot_restore_preserving_modules\", \"testing/test_pytester.py::TestInlineRunModulesCleanup::test_external_test_module_imports_not_cleaned_up\", \"testing/test_pytester.py::test_assert_outcomes_after_pytest_error\", \"testing/test_pytester.py::test_cwd_snapshot\", \"testing/test_pytester.py::test_pytester_subprocess_via_runpytest_arg\", \"testing/test_pytester.py::test_unicode_args\", \"testing/test_pytester.py::test_run_stdin\", \"testing/test_pytester.py::test_popen_stdin_pipe\", \"testing/test_pytester.py::test_popen_stdin_bytes\", \"testing/test_pytester.py::test_popen_default_stdin_stderr_and_stdin_None\", \"testing/test_pytester.py::test_pytester_outcomes_with_multiple_errors\", \"testing/test_pytester.py::test_makefile_joins_absolute_path\", \"testing/test_pytester.py::test_testtmproot\", \"testing/test_pytester.py::test_testdir_makefile_dot_prefixes_extension_silently\", \"testing/test_pytester.py::test_pytester_makefile_dot_prefixes_extension_with_warning\", \"testing/test_pytester.py::test_testdir_makefile_ext_none_raises_type_error\", \"testing/test_pytester.py::test_testdir_makefile_ext_empty_string_makes_file\", \"testing/test_pytester.py::test_pytester_assert_outcomes_warnings\", \"testing/test_pytester.py::test_pytester_subprocess\", \"testing/test_pytester.py::test_pytester_run_no_timeout\", \"testing/test_pytester.py::test_pytester_run_with_timeout\", \"testing/test_pytester.py::test_pytester_run_timeout_expires\"]",
  "environment_setup_commit": "e2ee3144ed6e241dea8d96215fcdca18b3892551",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.945985",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}