{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-14898",
  "base_commit": "d2476fb679f05e80c56e8b151ff0f6d7a470e4ae",
  "patch": "diff --git a/sklearn/metrics/classification.py b/sklearn/metrics/classification.py\n--- a/sklearn/metrics/classification.py\n+++ b/sklearn/metrics/classification.py\n@@ -2369,7 +2369,7 @@ def brier_score_loss(y_true, y_prob, sample_weight=None, pos_label=None):\n         raise ValueError(\"y_prob contains values less than 0.\")\n \n     # if pos_label=None, when y_true is in {-1, 1} or {0, 1},\n-    # pos_labe is set to 1 (consistent with precision_recall_curve/roc_curve),\n+    # pos_label is set to 1 (consistent with precision_recall_curve/roc_curve),\n     # otherwise pos_label is set to the greater label\n     # (different from precision_recall_curve/roc_curve,\n     # the purpose is to keep backward compatibility).\ndiff --git a/sklearn/metrics/scorer.py b/sklearn/metrics/scorer.py\n--- a/sklearn/metrics/scorer.py\n+++ b/sklearn/metrics/scorer.py\n@@ -21,6 +21,7 @@\n from collections.abc import Iterable\n from functools import partial\n from collections import Counter\n+import warnings\n \n import numpy as np\n \n@@ -125,6 +126,9 @@ def __init__(self, score_func, sign, kwargs):\n         self._kwargs = kwargs\n         self._score_func = score_func\n         self._sign = sign\n+        # XXX After removing the deprecated scorers (v0.24) remove the\n+        # XXX deprecation_msg property again and remove __call__'s body again\n+        self._deprecation_msg = None\n \n     def __repr__(self):\n         kwargs_string = \"\".join([\", %s=%s\" % (str(k), str(v))\n@@ -157,6 +161,10 @@ def __call__(self, estimator, X, y_true, sample_weight=None):\n         score : float\n             Score function applied to prediction of estimator on X.\n         \"\"\"\n+        if self._deprecation_msg is not None:\n+            warnings.warn(self._deprecation_msg,\n+                          category=DeprecationWarning,\n+                          stacklevel=2)\n         return self._score(partial(_cached_call, None), estimator, X, y_true,\n                            sample_weight=sample_weight)\n \n@@ -193,6 +201,7 @@ def _score(self, method_caller, estimator, X, y_true, sample_weight=None):\n         score : float\n             Score function applied to prediction of estimator on X.\n         \"\"\"\n+\n         y_pred = method_caller(estimator, \"predict\", X)\n         if sample_weight is not None:\n             return self._sign * self._score_func(y_true, y_pred,\n@@ -232,6 +241,7 @@ def _score(self, method_caller, clf, X, y, sample_weight=None):\n         score : float\n             Score function applied to prediction of estimator on X.\n         \"\"\"\n+\n         y_type = type_of_target(y)\n         y_pred = method_caller(clf, \"predict_proba\", X)\n         if y_type == \"binary\":\n@@ -284,6 +294,7 @@ def _score(self, method_caller, clf, X, y, sample_weight=None):\n         score : float\n             Score function applied to prediction of estimator on X.\n         \"\"\"\n+\n         y_type = type_of_target(y)\n         if y_type not in (\"binary\", \"multilabel-indicator\"):\n             raise ValueError(\"{0} format is not supported\".format(y_type))\n@@ -339,11 +350,15 @@ def get_scorer(scoring):\n     \"\"\"\n     if isinstance(scoring, str):\n         try:\n-            scorer = SCORERS[scoring]\n+            if scoring == 'brier_score_loss':\n+                # deprecated\n+                scorer = brier_score_loss_scorer\n+            else:\n+                scorer = SCORERS[scoring]\n         except KeyError:\n             raise ValueError('%r is not a valid scoring value. '\n                              'Use sorted(sklearn.metrics.SCORERS.keys()) '\n-                             'to get valid options.' % (scoring))\n+                             'to get valid options.' % scoring)\n     else:\n         scorer = scoring\n     return scorer\n@@ -642,9 +657,16 @@ def make_scorer(score_func, greater_is_better=True, needs_proba=False,\n # Score function for probabilistic classification\n neg_log_loss_scorer = make_scorer(log_loss, greater_is_better=False,\n                                   needs_proba=True)\n+neg_brier_score_scorer = make_scorer(brier_score_loss,\n+                                     greater_is_better=False,\n+                                     needs_proba=True)\n brier_score_loss_scorer = make_scorer(brier_score_loss,\n                                       greater_is_better=False,\n                                       needs_proba=True)\n+deprecation_msg = ('Scoring method brier_score_loss was renamed to '\n+                   'neg_brier_score in version 0.22 and will '\n+                   'be removed in 0.24.')\n+brier_score_loss_scorer._deprecation_msg = deprecation_msg\n \n \n # Clustering scores\n@@ -676,7 +698,7 @@ def make_scorer(score_func, greater_is_better=True, needs_proba=False,\n                balanced_accuracy=balanced_accuracy_scorer,\n                average_precision=average_precision_scorer,\n                neg_log_loss=neg_log_loss_scorer,\n-               brier_score_loss=brier_score_loss_scorer,\n+               neg_brier_score=neg_brier_score_scorer,\n                # Cluster metrics that use supervised evaluation\n                adjusted_rand_score=adjusted_rand_scorer,\n                homogeneity_score=homogeneity_scorer,\n",
  "test_patch": "diff --git a/sklearn/metrics/tests/test_score_objects.py b/sklearn/metrics/tests/test_score_objects.py\n--- a/sklearn/metrics/tests/test_score_objects.py\n+++ b/sklearn/metrics/tests/test_score_objects.py\n@@ -54,7 +54,7 @@\n                'roc_auc', 'average_precision', 'precision',\n                'precision_weighted', 'precision_macro', 'precision_micro',\n                'recall', 'recall_weighted', 'recall_macro', 'recall_micro',\n-               'neg_log_loss', 'log_loss', 'brier_score_loss',\n+               'neg_log_loss', 'log_loss', 'neg_brier_score',\n                'jaccard', 'jaccard_weighted', 'jaccard_macro',\n                'jaccard_micro', 'roc_auc_ovr', 'roc_auc_ovo',\n                'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted']\n@@ -551,6 +551,17 @@ def test_scoring_is_not_metric():\n         check_scoring(KMeans(), cluster_module.adjusted_rand_score)\n \n \n+def test_deprecated_scorer():\n+    X, y = make_blobs(random_state=0, centers=2)\n+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n+    clf = DecisionTreeClassifier()\n+    clf.fit(X_train, y_train)\n+\n+    deprecated_scorer = get_scorer('brier_score_loss')\n+    with pytest.warns(DeprecationWarning):\n+        deprecated_scorer(clf, X_test, y_test)\n+\n+\n @pytest.mark.parametrize(\n     (\"scorers,expected_predict_count,\"\n      \"expected_predict_proba_count,expected_decision_func_count\"),\n",
  "problem_statement": "Documentation section 3.3.1.1 has incorrect description of brier_score_loss\nIn the documentation, section 3.3.1.1. \"Common cases: predefined values\" includes the remark\r\n\r\n> All scorer objects follow the convention that higher return values are better than lower return values. \r\n\r\nAs far as I can tell, this is true for all of the listed metrics, **except** the `brier_score_loss`. In the case of `brier_score_loss`, a _lower loss value is better._ This is because `brier_score_loss` measures the mean-square difference between a predicted probability and a categorical outcome; the Brier score is _minimized_ at 0.0 because all summands are either `(0 - 0) ^ 2=0` or `(1 -1) ^ 2=0` when the model is making perfect predictions. On the other hand, the Brier score is _maximized_ at 1.0 when all predictions are **opposite** the correct label, as all summands are either `(0 - 1)^2=1` or `(1 - 0)^2=1`.\r\n\r\nTherefore, the definition of the `brier_score_loss` is not consistent with the quotation from section 3.3.1.1. \r\n\r\nI suggest making 2 changes to relieve this confusion.\r\n\r\n1. Implement a function `neg_brier_score_loss` which simply negates the value of `brier_score_loss`; this is a direct analogy to what is done in the case of `neg_log_loss`. A better model has a lower value of log-loss (categorical cross-entropy loss), therefore a larger value of the _negative_ log-loss implies a better model. Naturally, the same is true for Brier score, where it is also the case that a better model is assigned a lower loss.\r\n\r\n2. Remove reference to `brier_score_loss` from section 3.3.1.1. Brier score is useful in lots of ways; however, because it does not have the property that a larger value implies a better model, it seems confusing to mention it in the context of section 3.3.1.1. References to `brier_score_loss` can be replaced with `neg_brier_score_loss`, which has the property that better models have large values, just like accuracy, ROC AUC and the rest of the listed metrics.\n",
  "hints_text": "Indeed this is probably the right course of action. Please feel free to open a PR if your wish.\n@Sycor4x  I'll gladly work on it if you're not already doing it \n@qdeffense Thank you. I had planned to start these revisions if this suggestion were well-received; however, I've just come down with a cold and won't be able to write coherent code at the moment. If you want to take a stab at this, I support your diligence. \r\n\r\nIt occurred to me after I wrote this that it is possible for the verbal description in 3.3.1.1 to be incorrect while the _behavior_ of the scorer objects called via the strings in 3.3.1.1 might work correctly in the sense that internally, `brier_score_loss` behaves in the same manner as `neg_log_loss` and therefore is consistent with the statement\r\n\r\n> All scorer objects follow the convention that higher return values are better than lower return values.\r\n\r\nIf this is the case, then the _documentation_ is the only thing that needs to be tweaked: just make it explicit that some kind of reversal is applied to `brier_score_loss` such that the block quote is true.\r\n\r\nI haven't been able to check -- I'm basically incapacitated right now.",
  "created_at": "2019-09-06T00:32:56Z",
  "version": "0.22",
  "FAIL_TO_PASS": "[\"sklearn/metrics/tests/test_score_objects.py::test_scorer_sample_weight\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_brier_score]\", \"sklearn/metrics/tests/test_score_objects.py::test_deprecated_scorer\"]",
  "PASS_TO_PASS": "[\"sklearn/metrics/tests/test_score_objects.py::test_all_scorers_repr\", \"sklearn/metrics/tests/test_score_objects.py::test_check_scoring_and_check_multimetric_scoring\", \"sklearn/metrics/tests/test_score_objects.py::test_check_scoring_gridsearchcv\", \"sklearn/metrics/tests/test_score_objects.py::test_make_scorer\", \"sklearn/metrics/tests/test_score_objects.py::test_classification_scores\", \"sklearn/metrics/tests/test_score_objects.py::test_regression_scorers\", \"sklearn/metrics/tests/test_score_objects.py::test_thresholded_scorers\", \"sklearn/metrics/tests/test_score_objects.py::test_thresholded_scorers_multilabel_indicator_data\", \"sklearn/metrics/tests/test_score_objects.py::test_supervised_cluster_scorers\", \"sklearn/metrics/tests/test_score_objects.py::test_raises_on_score_list\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[explained_variance]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[r2]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[max_error]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_median_absolute_error]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_absolute_error]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_squared_error]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_squared_log_error]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_root_mean_squared_error]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_poisson_deviance]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_mean_gamma_deviance]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[accuracy]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc_ovr]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc_ovo]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc_ovr_weighted]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[roc_auc_ovo_weighted]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[balanced_accuracy]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[average_precision]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[neg_log_loss]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[adjusted_rand_score]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[homogeneity_score]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[completeness_score]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[v_measure_score]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[mutual_info_score]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[adjusted_mutual_info_score]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[normalized_mutual_info_score]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[fowlkes_mallows_score]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_macro]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_micro]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_samples]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[precision_weighted]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_macro]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_micro]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_samples]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[recall_weighted]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_macro]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_micro]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_samples]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[f1_weighted]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_macro]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_micro]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_samples]\", \"sklearn/metrics/tests/test_score_objects.py::test_scorer_memmap_input[jaccard_weighted]\", \"sklearn/metrics/tests/test_score_objects.py::test_scoring_is_not_metric\", \"sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers0-1-1-1]\", \"sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers1-1-0-1]\", \"sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once[scorers2-1-1-0]\", \"sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_classifier_no_decision\", \"sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_calls_method_once_regressor_threshold\", \"sklearn/metrics/tests/test_score_objects.py::test_multimetric_scorer_sanity_check\"]",
  "environment_setup_commit": "7e85a6d1f038bbb932b36f18d75df6be937ed00d",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:31.009391",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}