{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-11346",
  "base_commit": "93382cc41fb95abbbf534aed4c4cf2405c38d601",
  "patch": "diff --git a/sklearn/linear_model/omp.py b/sklearn/linear_model/omp.py\n--- a/sklearn/linear_model/omp.py\n+++ b/sklearn/linear_model/omp.py\n@@ -191,7 +191,7 @@ def _gram_omp(Gram, Xy, n_nonzero_coefs, tol_0=None, tol=None,\n     \"\"\"\n     Gram = Gram.copy('F') if copy_Gram else np.asfortranarray(Gram)\n \n-    if copy_Xy:\n+    if copy_Xy or not Xy.flags.writeable:\n         Xy = Xy.copy()\n \n     min_float = np.finfo(Gram.dtype).eps\n@@ -491,6 +491,9 @@ def orthogonal_mp_gram(Gram, Xy, n_nonzero_coefs=None, tol=None,\n         Xy = Xy[:, np.newaxis]\n         if tol is not None:\n             norms_squared = [norms_squared]\n+    if copy_Xy or not Xy.flags.writeable:\n+        # Make the copy once instead of many times in _gram_omp itself.\n+        Xy = Xy.copy()\n \n     if n_nonzero_coefs is None and tol is None:\n         n_nonzero_coefs = int(0.1 * len(Gram))\n@@ -515,7 +518,7 @@ def orthogonal_mp_gram(Gram, Xy, n_nonzero_coefs=None, tol=None,\n         out = _gram_omp(\n             Gram, Xy[:, k], n_nonzero_coefs,\n             norms_squared[k] if tol is not None else None, tol,\n-            copy_Gram=copy_Gram, copy_Xy=copy_Xy,\n+            copy_Gram=copy_Gram, copy_Xy=False,\n             return_path=return_path)\n         if return_path:\n             _, idx, coefs, n_iter = out\n",
  "test_patch": "diff --git a/sklearn/decomposition/tests/test_dict_learning.py b/sklearn/decomposition/tests/test_dict_learning.py\n--- a/sklearn/decomposition/tests/test_dict_learning.py\n+++ b/sklearn/decomposition/tests/test_dict_learning.py\n@@ -1,3 +1,4 @@\n+from __future__ import division\n import pytest\n \n import numpy as np\n@@ -366,3 +367,22 @@ def test_sparse_coder_estimator():\n                        transform_alpha=0.001).transform(X)\n     assert_true(not np.all(code == 0))\n     assert_less(np.sqrt(np.sum((np.dot(code, V) - X) ** 2)), 0.1)\n+\n+\n+def test_sparse_coder_parallel_mmap():\n+    # Non-regression test for:\n+    # https://github.com/scikit-learn/scikit-learn/issues/5956\n+    # Test that SparseCoder does not error by passing reading only\n+    # arrays to child processes\n+\n+    rng = np.random.RandomState(777)\n+    n_components, n_features = 40, 64\n+    init_dict = rng.rand(n_components, n_features)\n+    # Ensure that `data` is >2M. Joblib memory maps arrays\n+    # if they are larger than 1MB. The 4 accounts for float32\n+    # data type\n+    n_samples = int(2e6) // (4 * n_features)\n+    data = np.random.rand(n_samples, n_features).astype(np.float32)\n+\n+    sc = SparseCoder(init_dict, transform_algorithm='omp', n_jobs=2)\n+    sc.fit_transform(data)\ndiff --git a/sklearn/linear_model/tests/test_omp.py b/sklearn/linear_model/tests/test_omp.py\n--- a/sklearn/linear_model/tests/test_omp.py\n+++ b/sklearn/linear_model/tests/test_omp.py\n@@ -104,6 +104,20 @@ def test_perfect_signal_recovery():\n     assert_array_almost_equal(gamma[:, 0], gamma_gram, decimal=2)\n \n \n+def test_orthogonal_mp_gram_readonly():\n+    # Non-regression test for:\n+    # https://github.com/scikit-learn/scikit-learn/issues/5956\n+    idx, = gamma[:, 0].nonzero()\n+    G_readonly = G.copy()\n+    G_readonly.setflags(write=False)\n+    Xy_readonly = Xy.copy()\n+    Xy_readonly.setflags(write=False)\n+    gamma_gram = orthogonal_mp_gram(G_readonly, Xy_readonly[:, 0], 5,\n+                                    copy_Gram=False, copy_Xy=False)\n+    assert_array_equal(idx, np.flatnonzero(gamma_gram))\n+    assert_array_almost_equal(gamma[:, 0], gamma_gram, decimal=2)\n+\n+\n def test_estimator():\n     omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)\n     omp.fit(X, y[:, 0])\n",
  "problem_statement": "ValueError: assignment destination is read-only, when paralleling with n_jobs > 1\nWhen I run `SparseCoder` with n_jobs > 1, there is a chance to raise exception `ValueError: assignment destination is read-only`. The code is shown as follow:\n\n```\nfrom sklearn.decomposition import SparseCoder\nimport numpy as np\n\ndata_dims = 4103\ninit_dict = np.random.rand(500, 64)\ndata = np.random.rand(data_dims, 64)\nc = SparseCoder(init_dict , transform_algorithm='omp', n_jobs=8).fit_transform(data)\n```\n\nThe bigger `data_dims` is, the higher chance get. When `data_dims` is small (lower than 2000, I verified), everything works fine. Once `data_dims` is bigger than 2000, there is a chance to get the exception. When `data_dims` is bigger than 5000, it is 100% raised.\n\nMy version infor:\n\nOS: OS X 10.11.1\npython: Python 2.7.10 |Anaconda 2.2.0\nnumpy: 1.10.1\nsklearn: 0.17\n\nThe full error information is shown as follow\n\n```\n---------------------------------------------------------------------------\nJoblibValueError                          Traceback (most recent call last)\n<ipython-input-24-d745e5de1eae> in <module>()\n----> 1 learned_dict = dict_learn(init_dict, patches)\n\n<ipython-input-23-50e8dab30ec4> in dict_learn(dictionary, data)\n      6         # Sparse coding stage\n      7         coder = SparseCoder(dictionary, transform_algorithm='omp', n_jobs=8, transform_n_nonzero_coefs=3)\n----> 8         code = coder.fit_transform(data)\n      9         #print iteration, ' ', linalg.norm(data - np.dot(code, dictionary)), ' +',\n     10         # update stage\n\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/sklearn/base.pyc in fit_transform(self, X, y, **fit_params)\n    453         if y is None:\n    454             # fit method of arity 1 (unsupervised transformation)\n--> 455             return self.fit(X, **fit_params).transform(X)\n    456         else:\n    457             # fit method of arity 2 (supervised transformation)\n\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/sklearn/decomposition/dict_learning.pyc in transform(self, X, y)\n    816             X, self.components_, algorithm=self.transform_algorithm,\n    817             n_nonzero_coefs=self.transform_n_nonzero_coefs,\n--> 818             alpha=self.transform_alpha, n_jobs=self.n_jobs)\n    819 \n    820         if self.split_sign:\n\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/sklearn/decomposition/dict_learning.pyc in sparse_encode(X, dictionary, gram, cov, algorithm, n_nonzero_coefs, alpha, copy_cov, init, max_iter, n_jobs, check_input, verbose)\n    298             max_iter=max_iter,\n    299             check_input=False)\n--> 300         for this_slice in slices)\n    301     for this_slice, this_view in zip(slices, code_views):\n    302         code[this_slice] = this_view\n\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in __call__(self, iterable)\n    810                 # consumption.\n    811                 self._iterating = False\n--> 812             self.retrieve()\n    813             # Make sure that we get a last message telling us we are done\n    814             elapsed_time = time.time() - self._start_time\n\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in retrieve(self)\n    760                         # a working pool as they expect.\n    761                         self._initialize_pool()\n--> 762                 raise exception\n    763 \n    764     def __call__(self, iterable):\n\nJoblibValueError: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/runpy.py in _run_module_as_main(mod_name='IPython.kernel.__main__', alter_argv=1)\n    157     pkg_name = mod_name.rpartition('.')[0]\n    158     main_globals = sys.modules[\"__main__\"].__dict__\n    159     if alter_argv:\n    160         sys.argv[0] = fname\n    161     return _run_code(code, main_globals, None,\n--> 162                      \"__main__\", fname, loader, pkg_name)\n        fname = '/Users/fengyuyao/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py'\n        loader = <pkgutil.ImpLoader instance>\n        pkg_name = 'IPython.kernel'\n    163 \n    164 def run_module(mod_name, init_globals=None,\n    165                run_name=None, alter_sys=False):\n    166     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/runpy.py in _run_code(code=<code object <module> at 0x10596bdb0, file \"/Use...ite-packages/IPython/kernel/__main__.py\", line 1>, run_globals={'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/Users/fengyuyao/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'IPython.kernel', 'app': <module 'IPython.kernel.zmq.kernelapp' from '/Us.../site-packages/IPython/kernel/zmq/kernelapp.pyc'>}, init_globals=None, mod_name='__main__', mod_fname='/Users/fengyuyao/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py', mod_loader=<pkgutil.ImpLoader instance>, pkg_name='IPython.kernel')\n     67         run_globals.update(init_globals)\n     68     run_globals.update(__name__ = mod_name,\n     69                        __file__ = mod_fname,\n     70                        __loader__ = mod_loader,\n     71                        __package__ = pkg_name)\n---> 72     exec code in run_globals\n        code = <code object <module> at 0x10596bdb0, file \"/Use...ite-packages/IPython/kernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/Users/fengyuyao/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'IPython.kernel', 'app': <module 'IPython.kernel.zmq.kernelapp' from '/Us.../site-packages/IPython/kernel/zmq/kernelapp.pyc'>}\n     73     return run_globals\n     74 \n     75 def _run_module_code(code, init_globals=None,\n     76                     mod_name=None, mod_fname=None,\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from IPython.kernel.zmq import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/IPython/config/application.py in launch_instance(cls=<class 'IPython.kernel.zmq.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    569         \n    570         If a global instance already exists, this reinitializes and starts it\n    571         \"\"\"\n    572         app = cls.instance(**kwargs)\n    573         app.initialize(argv)\n--> 574         app.start()\n        app.start = <bound method IPKernelApp.start of <IPython.kernel.zmq.kernelapp.IPKernelApp object>>\n    575 \n    576 #-----------------------------------------------------------------------------\n    577 # utility functions, for convenience\n    578 #-----------------------------------------------------------------------------\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/kernelapp.py in start(self=<IPython.kernel.zmq.kernelapp.IPKernelApp object>)\n    369     def start(self):\n    370         if self.poller is not None:\n    371             self.poller.start()\n    372         self.kernel.start()\n    373         try:\n--> 374             ioloop.IOLoop.instance().start()\n    375         except KeyboardInterrupt:\n    376             pass\n    377 \n    378 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    146             PollIOLoop.configure(ZMQIOLoop)\n    147         return PollIOLoop.instance()\n    148     \n    149     def start(self):\n    150         try:\n--> 151             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    152         except ZMQError as e:\n    153             if e.errno == ETERM:\n    154                 # quietly return on ETERM\n    155                 pass\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    835                 self._events.update(event_pairs)\n    836                 while self._events:\n    837                     fd, events = self._events.popitem()\n    838                     try:\n    839                         fd_obj, handler_func = self._handlers[fd]\n--> 840                         handler_func(fd_obj, events)\n        handler_func = <function null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    841                     except (OSError, IOError) as e:\n    842                         if errno_from_exception(e) == errno.EPIPE:\n    843                             # Happens when the client closes the connection\n    844                             pass\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    428             # dispatch events:\n    429             if events & IOLoop.ERROR:\n    430                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    431                 return\n    432             if events & IOLoop.READ:\n--> 433                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    434                 if not self.socket:\n    435                     return\n    436             if events & IOLoop.WRITE:\n    437                 self._handle_send()\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    460                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    461         else:\n    462             if self._recv_callback:\n    463                 callback = self._recv_callback\n    464                 # self._recv_callback = None\n--> 465                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    466                 \n    467         # self.update_state()\n    468         \n    469 \n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    402         close our socket.\"\"\"\n    403         try:\n    404             # Use a NullContext to ensure that all StackContexts are run\n    405             # inside our blanket exception handler rather than outside.\n    406             with stack_context.NullContext():\n--> 407                 callback(*args, **kwargs)\n        callback = <function null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    408         except:\n    409             gen_log.error(\"Uncaught exception, closing connection.\",\n    410                           exc_info=True)\n    411             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    247         if self.control_stream:\n    248             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    249 \n    250         def make_dispatcher(stream):\n    251             def dispatcher(msg):\n--> 252                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    253             return dispatcher\n    254 \n    255         for s in self.shell_streams:\n    256             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/kernelbase.py in dispatch_shell(self=<IPython.kernel.zmq.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {u'allow_stdin': True, u'code': u'learned_dict = dict_learn(init_dict, patches)', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {u'msg_id': u'D61C0C0F1F89441EB2C232BAE352E9B6', u'msg_type': u'execute_request', u'session': u'21C58290AD9A4368BCFCB05D17E87C41', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'D61C0C0F1F89441EB2C232BAE352E9B6', 'msg_type': u'execute_request', 'parent_header': {}})\n    208         else:\n    209             # ensure default_int_handler during handler call\n    210             sig = signal(SIGINT, default_int_handler)\n    211             self.log.debug(\"%s: %s\", msg_type, msg)\n    212             try:\n--> 213                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <IPython.kernel.zmq.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = ['21C58290AD9A4368BCFCB05D17E87C41']\n        msg = {'buffers': [], 'content': {u'allow_stdin': True, u'code': u'learned_dict = dict_learn(init_dict, patches)', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {u'msg_id': u'D61C0C0F1F89441EB2C232BAE352E9B6', u'msg_type': u'execute_request', u'session': u'21C58290AD9A4368BCFCB05D17E87C41', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'D61C0C0F1F89441EB2C232BAE352E9B6', 'msg_type': u'execute_request', 'parent_header': {}}\n    214             except Exception:\n    215                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    216             finally:\n    217                 signal(SIGINT, sig)\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/kernelbase.py in execute_request(self=<IPython.kernel.zmq.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=['21C58290AD9A4368BCFCB05D17E87C41'], parent={'buffers': [], 'content': {u'allow_stdin': True, u'code': u'learned_dict = dict_learn(init_dict, patches)', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {u'msg_id': u'D61C0C0F1F89441EB2C232BAE352E9B6', u'msg_type': u'execute_request', u'session': u'21C58290AD9A4368BCFCB05D17E87C41', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'D61C0C0F1F89441EB2C232BAE352E9B6', 'msg_type': u'execute_request', 'parent_header': {}})\n    357         if not silent:\n    358             self.execution_count += 1\n    359             self._publish_execute_input(code, parent, self.execution_count)\n    360         \n    361         reply_content = self.do_execute(code, silent, store_history,\n--> 362                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    363 \n    364         # Flush output before sending the reply.\n    365         sys.stdout.flush()\n    366         sys.stderr.flush()\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/IPython/kernel/zmq/ipkernel.py in do_execute(self=<IPython.kernel.zmq.ipkernel.IPythonKernel object>, code=u'learned_dict = dict_learn(init_dict, patches)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    176 \n    177         reply_content = {}\n    178         # FIXME: the shell calls the exception handler itself.\n    179         shell._reply_content = None\n    180         try:\n--> 181             shell.run_cell(code, store_history=store_history, silent=silent)\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <I....kernel.zmq.zmqshell.ZMQInteractiveShell object>>\n        code = u'learned_dict = dict_learn(init_dict, patches)'\n        store_history = True\n        silent = False\n    182         except:\n    183             status = u'error'\n    184             # FIXME: this code right now isn't being used yet by default,\n    185             # because the run_cell() call above directly fires off exception\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_cell(self=<IPython.kernel.zmq.zmqshell.ZMQInteractiveShell object>, raw_cell=u'learned_dict = dict_learn(init_dict, patches)', store_history=True, silent=False, shell_futures=True)\n   2863                 self.displayhook.exec_result = result\n   2864 \n   2865                 # Execute the user code\n   2866                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2867                 self.run_ast_nodes(code_ast.body, cell_name,\n-> 2868                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler instance>\n   2869 \n   2870                 # Reset this so later displayed values do not modify the\n   2871                 # ExecutionResult\n   2872                 self.displayhook.exec_result = None\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<IPython.kernel.zmq.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>], cell_name='<ipython-input-24-d745e5de1eae>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler instance>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   2967 \n   2968         try:\n   2969             for i, node in enumerate(to_run_exec):\n   2970                 mod = ast.Module([node])\n   2971                 code = compiler(mod, cell_name, \"exec\")\n-> 2972                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <I....kernel.zmq.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x10abcef30, file \"<ipython-input-24-d745e5de1eae>\", line 1>\n        result = <IPython.core.interactiveshell.ExecutionResult object>\n   2973                     return True\n   2974 \n   2975             for i, node in enumerate(to_run_interactive):\n   2976                 mod = ast.Interactive([node])\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_code(self=<IPython.kernel.zmq.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x10abcef30, file \"<ipython-input-24-d745e5de1eae>\", line 1>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   3027         outflag = 1  # happens in more places, so it's easier as default\n   3028         try:\n   3029             try:\n   3030                 self.hooks.pre_run_code_hook()\n   3031                 #rprint('Running code', repr(code_obj)) # dbg\n-> 3032                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x10abcef30, file \"<ipython-input-24-d745e5de1eae>\", line 1>\n        self.user_global_ns = {'In': ['', u'import skimage\\nimport skimage.data as data\\ni...klearn.preprocessing import normalize\\nimport os', u\"get_ipython().magic(u'matplotlib inline')\", u\"data_path = '/Users/fengyuyao/Research/experim...th) if '.png' in i])\\n\\ndata = data.mean(axis=3)\", u'img = data[0, ...]\\n#img = sktrans.resize(img, (150, 150))', u\"pimg = extract_patches_2d(img, (8,8))\\nnimg = ...#ccc =reconstruct_from_patches_2d(bbb, (50, 50))\", u'pimg = normalize(pimg)\\nnpimg = normalize(npimg)', u\"pimg = extract_patches_2d(img, (8,8))\\nnimg = ...#ccc =reconstruct_from_patches_2d(bbb, (50, 50))\", u'patches = np.array([extract_patches_2d(d, (8,8)) for d in data[:10,...]]).reshape(-1, 64)', u'init_dict = patches[np.random.choice(np.arange... = np.ones(64)\\ninit_dict = normalize(init_dict)', u\"def dict_learn(dictionary, data):\\n    diction...        #yield dictionary\\n    return dictionary\", u'learned_dict = dict_learn(init_dict, patches)', u\"def dict_learn(dictionary, data):\\n    diction...        #yield dictionary\\n    return dictionary\", u'init_dict = patches[np.random.choice(np.arange... = np.ones(64)\\ninit_dict = normalize(init_dict)', u'learned_dict = dict_learn(init_dict, patches)', u\"def dict_learn(dictionary, data):\\n    diction...        #yield dictionary\\n    return dictionary\", u'learned_dict = dict_learn(init_dict, patches)', u'patches = np.array([extract_patches_2d(d, (8,8)) for d in data[:10,...]]).reshape(-1, 64)', u'index = np.arange(patches.shape[0])\\nnp.random.shuffle(index)\\nindex = index[:20000]', u'patches = patches[index]', ...], 'Out': {20: (20000, 64)}, 'SparseCoder': <class 'sklearn.decomposition.dict_learning.SparseCoder'>, '_': (20000, 64), '_20': (20000, 64), '__': '', '___': '', '__builtin__': <module '__builtin__' (built-in)>, '__builtins__': <module '__builtin__' (built-in)>, '__doc__': 'Automatically created module for IPython interactive environment', ...}\n        self.user_ns = {'In': ['', u'import skimage\\nimport skimage.data as data\\ni...klearn.preprocessing import normalize\\nimport os', u\"get_ipython().magic(u'matplotlib inline')\", u\"data_path = '/Users/fengyuyao/Research/experim...th) if '.png' in i])\\n\\ndata = data.mean(axis=3)\", u'img = data[0, ...]\\n#img = sktrans.resize(img, (150, 150))', u\"pimg = extract_patches_2d(img, (8,8))\\nnimg = ...#ccc =reconstruct_from_patches_2d(bbb, (50, 50))\", u'pimg = normalize(pimg)\\nnpimg = normalize(npimg)', u\"pimg = extract_patches_2d(img, (8,8))\\nnimg = ...#ccc =reconstruct_from_patches_2d(bbb, (50, 50))\", u'patches = np.array([extract_patches_2d(d, (8,8)) for d in data[:10,...]]).reshape(-1, 64)', u'init_dict = patches[np.random.choice(np.arange... = np.ones(64)\\ninit_dict = normalize(init_dict)', u\"def dict_learn(dictionary, data):\\n    diction...        #yield dictionary\\n    return dictionary\", u'learned_dict = dict_learn(init_dict, patches)', u\"def dict_learn(dictionary, data):\\n    diction...        #yield dictionary\\n    return dictionary\", u'init_dict = patches[np.random.choice(np.arange... = np.ones(64)\\ninit_dict = normalize(init_dict)', u'learned_dict = dict_learn(init_dict, patches)', u\"def dict_learn(dictionary, data):\\n    diction...        #yield dictionary\\n    return dictionary\", u'learned_dict = dict_learn(init_dict, patches)', u'patches = np.array([extract_patches_2d(d, (8,8)) for d in data[:10,...]]).reshape(-1, 64)', u'index = np.arange(patches.shape[0])\\nnp.random.shuffle(index)\\nindex = index[:20000]', u'patches = patches[index]', ...], 'Out': {20: (20000, 64)}, 'SparseCoder': <class 'sklearn.decomposition.dict_learning.SparseCoder'>, '_': (20000, 64), '_20': (20000, 64), '__': '', '___': '', '__builtin__': <module '__builtin__' (built-in)>, '__builtins__': <module '__builtin__' (built-in)>, '__doc__': 'Automatically created module for IPython interactive environment', ...}\n   3033             finally:\n   3034                 # Reset our crash handler in place\n   3035                 sys.excepthook = old_excepthook\n   3036         except SystemExit as e:\n\n...........................................................................\n/Users/fengyuyao/Research/ppts/dictionary_learning_2015.11.25/code/<ipython-input-24-d745e5de1eae> in <module>()\n----> 1 \n      2 \n      3 \n      4 \n      5 \n      6 learned_dict = dict_learn(init_dict, patches)\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/Users/fengyuyao/Research/ppts/dictionary_learning_2015.11.25/code/<ipython-input-23-50e8dab30ec4> in dict_learn(dictionary=array([[ 0.125     ,  0.125     ,  0.125     , ....  0.10416518,\n         0.06896773,  0.0757119 ]]), data=array([[ 0.50559053,  0.49227671,  0.48265361, ....  0.15035063,\n         0.1782305 ,  0.19739984]]))\n      3     iteration = 0\n      4     last_iter_norm = 1e5\n      5     while True:\n      6         # Sparse coding stage\n      7         coder = SparseCoder(dictionary, transform_algorithm='omp', n_jobs=8, transform_n_nonzero_coefs=3)\n----> 8         code = coder.fit_transform(data)\n      9         #print iteration, ' ', linalg.norm(data - np.dot(code, dictionary)), ' +',\n     10         # update stage\n     11         for i in range(dictionary.shape[0]):\n     12             _dictionary = dictionary.copy()\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/sklearn/base.py in fit_transform(self=SparseCoder(dictionary=None, n_jobs=8, split_sig...rm_alpha=None,\n      transform_n_nonzero_coefs=3), X=array([[ 0.50559053,  0.49227671,  0.48265361, ....  0.15035063,\n         0.1782305 ,  0.19739984]]), y=None, **fit_params={})\n    450         \"\"\"\n    451         # non-optimized default implementation; override when a better\n    452         # method is possible for a given clustering algorithm\n    453         if y is None:\n    454             # fit method of arity 1 (unsupervised transformation)\n--> 455             return self.fit(X, **fit_params).transform(X)\n        self.fit = <bound method SparseCoder.fit of SparseCoder(dic...m_alpha=None,\n      transform_n_nonzero_coefs=3)>\n        X = array([[ 0.50559053,  0.49227671,  0.48265361, ....  0.15035063,\n         0.1782305 ,  0.19739984]])\n        fit_params.transform = undefined\n    456         else:\n    457             # fit method of arity 2 (supervised transformation)\n    458             return self.fit(X, y, **fit_params).transform(X)\n    459 \n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/sklearn/decomposition/dict_learning.py in transform(self=SparseCoder(dictionary=None, n_jobs=8, split_sig...rm_alpha=None,\n      transform_n_nonzero_coefs=3), X=array([[ 0.50559053,  0.49227671,  0.48265361, ....  0.15035063,\n         0.1782305 ,  0.19739984]]), y=None)\n    813         n_samples, n_features = X.shape\n    814 \n    815         code = sparse_encode(\n    816             X, self.components_, algorithm=self.transform_algorithm,\n    817             n_nonzero_coefs=self.transform_n_nonzero_coefs,\n--> 818             alpha=self.transform_alpha, n_jobs=self.n_jobs)\n        self.transform_alpha = None\n        self.n_jobs = 8\n    819 \n    820         if self.split_sign:\n    821             # feature vector is split into a positive and negative side\n    822             n_samples, n_features = code.shape\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/sklearn/decomposition/dict_learning.py in sparse_encode(X=array([[ 0.50559053,  0.49227671,  0.48265361, ....  0.15035063,\n         0.1782305 ,  0.19739984]]), dictionary=array([[ 0.125     ,  0.125     ,  0.125     , ....  0.10416518,\n         0.06896773,  0.0757119 ]]), gram=array([[ 1.        ,  0.99706708,  0.8669373 , ....  0.94511259,\n         0.93221472,  1.        ]]), cov=array([[ 3.49867539,  1.93651123,  2.05015994, ....  4.82561002,\n         0.62133361,  2.87358633]]), algorithm='omp', n_nonzero_coefs=3, alpha=None, copy_cov=False, init=None, max_iter=1000, n_jobs=8, check_input=True, verbose=0)\n    295             algorithm,\n    296             regularization=regularization, copy_cov=copy_cov,\n    297             init=init[this_slice] if init is not None else None,\n    298             max_iter=max_iter,\n    299             check_input=False)\n--> 300         for this_slice in slices)\n        this_slice = undefined\n        slices = [slice(0, 2500, None), slice(2500, 5000, None), slice(5000, 7500, None), slice(7500, 10000, None), slice(10000, 12500, None), slice(12500, 15000, None), slice(15000, 17500, None), slice(17500, 20000, None)]\n    301     for this_slice, this_view in zip(slices, code_views):\n    302         code[this_slice] = this_view\n    303     return code\n    304 \n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=8), iterable=<generator object <genexpr>>)\n    807             if pre_dispatch == \"all\" or n_jobs == 1:\n    808                 # The iterable was consumed all at once by the above for loop.\n    809                 # No need to wait for async callbacks to trigger to\n    810                 # consumption.\n    811                 self._iterating = False\n--> 812             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=8)>\n    813             # Make sure that we get a last message telling us we are done\n    814             elapsed_time = time.time() - self._start_time\n    815             self._print('Done %3i out of %3i | elapsed: %s finished',\n    816                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Fri Dec  4 10:21:33 2015\nPID: 35032              Python 2.7.10: /Users/fengyuyao/anaconda/bin/python\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/sklearn/decomposition/dict_learning.pyc in _sparse_encode(X=memmap([[ 0.50559053,  0.49227671,  0.48265361, ...  0.99596078,\n         0.99738562,  1.        ]]), dictionary=array([[ 0.125     ,  0.125     ,  0.125     , ....  0.10416518,\n         0.06896773,  0.0757119 ]]), gram=memmap([[ 1.        ,  0.99706708,  0.8669373 , ...  0.94511259,\n         0.93221472,  1.        ]]), cov=memmap([[ 3.49867539,  1.93651123,  2.05015994, ...  5.77883725,\n         3.55803798,  7.21968383]]), algorithm='omp', regularization=3, copy_cov=False, init=None, max_iter=1000, check_input=False, verbose=0)\n    147     elif algorithm == 'omp':\n    148         # TODO: Should verbose argument be passed to this?\n    149         new_code = orthogonal_mp_gram(\n    150             Gram=gram, Xy=cov, n_nonzero_coefs=int(regularization),\n    151             tol=None, norms_squared=row_norms(X, squared=True),\n--> 152             copy_Xy=copy_cov).T\n        algorithm = 'omp'\n        alpha = undefined\n    153     else:\n    154         raise ValueError('Sparse coding method must be \"lasso_lars\" '\n    155                          '\"lasso_cd\",  \"lasso\", \"threshold\" or \"omp\", got %s.'\n    156                          % algorithm)\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/sklearn/linear_model/omp.pyc in orthogonal_mp_gram(Gram=array([[ 1.        ,  0.99706708,  0.8669373 , ....  0.94511259,\n         0.93221472,  1.        ]]), Xy=array([[ 3.49867539,  1.93651123,  2.05015994, ....  5.77883725,\n         3.55803798,  7.21968383]]), n_nonzero_coefs=3, tol=None, norms_squared=array([ 12.37032493,   4.36747488,   4.2134112 ,... 37.00901994,\n        16.6505497 ,  58.97107498]), copy_Gram=True, copy_Xy=False, return_path=False, return_n_iter=False)\n    518     for k in range(Xy.shape[1]):\n    519         out = _gram_omp(\n    520             Gram, Xy[:, k], n_nonzero_coefs,\n    521             norms_squared[k] if tol is not None else None, tol,\n    522             copy_Gram=copy_Gram, copy_Xy=copy_Xy,\n--> 523             return_path=return_path)\n    524         if return_path:\n    525             _, idx, coefs, n_iter = out\n    526             coef = coef[:, :, :len(idx)]\n    527             for n_active, x in enumerate(coefs.T):\n\n...........................................................................\n/Users/fengyuyao/anaconda/lib/python2.7/site-packages/sklearn/linear_model/omp.pyc in _gram_omp(Gram=array([[ 1.        ,  0.99010866,  0.82197346, ....  0.94511259,\n         0.93221472,  1.        ]]), Xy=array([ 3.49867539,  3.48729003,  2.91977933,  3...4,  3.39029937,\n        3.45356109,  3.35550344]), n_nonzero_coefs=3, tol_0=None, tol=None, copy_Gram=True, copy_Xy=False, return_path=False)\n    240                 break\n    241             L[n_active, n_active] = np.sqrt(1 - v)\n    242         Gram[n_active], Gram[lam] = swap(Gram[n_active], Gram[lam])\n    243         Gram.T[n_active], Gram.T[lam] = swap(Gram.T[n_active], Gram.T[lam])\n    244         indices[n_active], indices[lam] = indices[lam], indices[n_active]\n--> 245         Xy[n_active], Xy[lam] = Xy[lam], Xy[n_active]\n        return_path = False\n    246         n_active += 1\n    247         # solves LL'x = y as a composition of two triangular systems\n    248         gamma, _ = potrs(L[:n_active, :n_active], Xy[:n_active], lower=True,\n    249                          overwrite_b=False)\n\nValueError: assignment destination is read-only\n___________________________________________________________________________\n\n```\n\n",
  "hints_text": "I am taking a look at this\n\nIs it not related to #5481, which seems more generic?\n\nIt is, but `SparseEncoder` is not an estimator\n\n> It is, but SparseEncoder is not an estimator\n\nNot that it matters but SparseCoder is an estimator:\n\n``` python\nfrom sklearn.base import BaseEstimator\nfrom sklearn.decomposition import SparseCoder\n\nissubclass(SparseCoder, BaseEstimator)  # True\n```\n\nI guess the error wasn't detected in #4807 as it is raised only when using `algorithm='omp'`. It should be raised when testing read only data on `OrthogonalMatchingPursuit` though.\r\n\nWas there a resolution to this bug? I've run into something similar while doing `n_jobs=-1` on RandomizedLogisticRegression, and didn't know whether I should open a new issue here. Here's the top of my stack:\n\n```\n/Users/ali/.pyenv/versions/mvenv/lib/python2.7/site-packages/sklearn/linear_model/randomized_l1.py in _randomized_logistic(X=memmap([[ -4.24636666e-03,  -5.10115749e-03,  -1...920913e-03,  -1.46599832e-03,   2.91083847e-03]]), y=array([1, 0, 0, ..., 0, 0, 0]), weights=array([ 0. ,  0.5,  0.5,  0. ,  0. ,  0. ,  0.5,... 0.5,  0.5,  0. ,  0. ,  0.5,  0. ,\n        0.5]), mask=array([ True,  True, False, ...,  True,  True,  True], dtype=bool), C=1.5, verbose=0, fit_intercept=True, tol=0.001)\n    351     if issparse(X):\n    352         size = len(weights)\n    353         weight_dia = sparse.dia_matrix((1 - weights, 0), (size, size))\n    354         X = X * weight_dia\n    355     else:\n--> 356         X *= (1 - weights)\n        X = memmap([[ -4.24636666e-03,  -5.10115749e-03,  -1...920913e-03,  -1.46599832e-03,   2.91083847e-03]])\n        weights = array([ 0. ,  0.5,  0.5,  0. ,  0. ,  0. ,  0.5,... 0.5,  0.5,  0. ,  0. ,  0.5,  0. ,\n        0.5])\n    357 \n    358     C = np.atleast_1d(np.asarray(C, dtype=np.float))\n    359     scores = np.zeros((X.shape[1], len(C)), dtype=np.bool)\n    360 \n\nValueError: output array is read-only\n```\n\nSomeone ran into the [same exact problem](http://stackoverflow.com/questions/27740804/scikit-learn-randomized-logistic-regression-gives-valueerror-output-array-is-r) on StackOverflow - `ValueError: output array is read-only`. Both provided solutions on SO are useless (the first one doesn't even bother solving the problem, and the second one is solving the problem by bypassing joblib completely).\n\n@alichaudry I just commented on a similar issue [here](https://github.com/scikit-learn/scikit-learn/issues/6614#issuecomment-208815649).\n\nI confirm that there is an error and it is floating in nature.\n\nsklearn.decomposition.SparseCoder(D, transform_algorithm = 'omp', n_jobs=64).transform(X) \n\nif X.shape[0] > 4000 it fails with ValueError: assignment destination is read-only\nIf X.shape[0] <100 it is ok.\n\nOS: Linux  3.10.0-327.13.1.el7.x86_64\n python: Python 2.7.5\n numpy: 1.10.1\n sklearn: 0.17\n\nHi there,\nI'm running into the same problem, using MiniBatchDictionaryLearning with jobs>1.\nI see a lot of referencing to other issues, but was there ever a solution to this? \nSorry in advance if a solution was mentioned and I missed it.\n\nOS: OSX\npython: 3.5\nnumpy: 1.10.1\nsklearn: 0.17\n\nThe problem is in modifying arrays in-place. @lesteve close as duplicate of #5481?\n\ncurrently I am still dealing with this issue and it is nearly a year since. this is still an open issue. \nIf you have a solution, please contribute it, @williamdjones \nhttps://github.com/scikit-learn/scikit-learn/pull/4807 is probably the more advanced effort to address this.\n@williamdjones I was not suggesting that it's solved, but that it's an issue that is reported at a different place, and having multiple issues related to the same problem makes keeping track of it harder.\nNot sure where to report this, or if it's related, but I get the `ValueError: output array is read-only` when using n_jobs > 1 with RandomizedLasso and other functions.\n@JGH1000 NOT A SOLUTION, but I would try using a random forest for feature selection instead since it is stable and has working joblib functionality.\nThanks @williamdjones, I used several different methods but found that RandomizedLasso works best for couple of particular datasets. In any case, it works but a bit slow. Not a deal breaker.\n@JGH1000 No problem. If you don't mind, I'm curious about the dimensionality of the datasets for which RLasso was useful versus those for which it was not. \n@williamdjones it was a small sample size (40-50), high-dimension (40,000-50,000) dataset. I would not say that other methods were bad, but RLasso provided results/ranking that were much more consistent with several univariate tests + domain knowledge. I guess this might not be the 'right' features but I had more trust in this method. Shame to hear it will be removed from scikit. \nThe problem still seems to exist on 24 core Ubuntu processor for RLasso with n_jobs = -1 and sklearn 0.19.1",
  "created_at": "2018-06-22T15:01:54Z",
  "version": "0.20",
  "FAIL_TO_PASS": "[\"sklearn/decomposition/tests/test_dict_learning.py::test_sparse_coder_parallel_mmap\", \"sklearn/linear_model/tests/test_omp.py::test_orthogonal_mp_gram_readonly\"]",
  "PASS_TO_PASS": "[\"sklearn/decomposition/tests/test_dict_learning.py::test_sparse_encode_shapes_omp\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_shapes\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_overcomplete\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_positivity[False-False-lasso_lars]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_positivity[False-False-lasso_cd]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_positivity[False-False-lars]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_positivity[False-False-threshold]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_positivity[False-True-lasso_lars]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_positivity[False-True-lasso_cd]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_positivity[False-True-lars]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_positivity[False-True-threshold]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_positivity[True-False-lasso_lars]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_positivity[True-False-lasso_cd]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_positivity[True-False-lars]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_positivity[True-False-threshold]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_positivity[True-True-lasso_lars]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_positivity[True-True-lasso_cd]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_positivity[True-True-lars]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_positivity[True-True-threshold]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_reconstruction\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_reconstruction_parallel\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_lassocd_readonly_data\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_nonzero_coefs\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_unknown_fit_algorithm\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_split\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_shapes\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_positivity[False-False-lasso_lars]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_positivity[False-False-lasso_cd]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_positivity[False-False-lars]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_positivity[False-False-threshold]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_positivity[False-True-lasso_lars]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_positivity[False-True-lasso_cd]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_positivity[False-True-lars]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_positivity[False-True-threshold]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_positivity[True-False-lasso_lars]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_positivity[True-False-lasso_cd]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_positivity[True-False-lars]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_positivity[True-False-threshold]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_positivity[True-True-lasso_lars]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_positivity[True-True-lasso_cd]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_positivity[True-True-lars]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_positivity[True-True-threshold]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_verbosity\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_estimator_shapes\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_overcomplete\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_initialization\", \"sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_partial_fit\", \"sklearn/decomposition/tests/test_dict_learning.py::test_sparse_encode_shapes\", \"sklearn/decomposition/tests/test_dict_learning.py::test_sparse_encode_positivity[False]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_sparse_encode_positivity[True]\", \"sklearn/decomposition/tests/test_dict_learning.py::test_sparse_encode_input\", \"sklearn/decomposition/tests/test_dict_learning.py::test_sparse_encode_error\", \"sklearn/decomposition/tests/test_dict_learning.py::test_sparse_encode_error_default_sparsity\", \"sklearn/decomposition/tests/test_dict_learning.py::test_unknown_method\", \"sklearn/decomposition/tests/test_dict_learning.py::test_sparse_coder_estimator\", \"sklearn/linear_model/tests/test_omp.py::test_correct_shapes\", \"sklearn/linear_model/tests/test_omp.py::test_correct_shapes_gram\", \"sklearn/linear_model/tests/test_omp.py::test_n_nonzero_coefs\", \"sklearn/linear_model/tests/test_omp.py::test_tol\", \"sklearn/linear_model/tests/test_omp.py::test_with_without_gram\", \"sklearn/linear_model/tests/test_omp.py::test_with_without_gram_tol\", \"sklearn/linear_model/tests/test_omp.py::test_unreachable_accuracy\", \"sklearn/linear_model/tests/test_omp.py::test_bad_input\", \"sklearn/linear_model/tests/test_omp.py::test_perfect_signal_recovery\", \"sklearn/linear_model/tests/test_omp.py::test_estimator\", \"sklearn/linear_model/tests/test_omp.py::test_identical_regressors\", \"sklearn/linear_model/tests/test_omp.py::test_swapped_regressors\", \"sklearn/linear_model/tests/test_omp.py::test_no_atoms\", \"sklearn/linear_model/tests/test_omp.py::test_omp_path\", \"sklearn/linear_model/tests/test_omp.py::test_omp_return_path_prop_with_gram\", \"sklearn/linear_model/tests/test_omp.py::test_omp_cv\", \"sklearn/linear_model/tests/test_omp.py::test_omp_reaches_least_squares\"]",
  "environment_setup_commit": "55bf5d93e5674f13a1134d93a11fd0cd11aabcd1",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.962010",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}