{
  "repo": "pylint-dev/pylint",
  "instance_id": "pylint-dev__pylint-4398",
  "base_commit": "43133c56d47bbc60e51a7f40433116b826eb18b7",
  "patch": "diff --git a/pylint/lint/pylinter.py b/pylint/lint/pylinter.py\n--- a/pylint/lint/pylinter.py\n+++ b/pylint/lint/pylinter.py\n@@ -264,6 +264,17 @@ def make_options():\n                     \"help\": \"Specify a score threshold to be exceeded before program exits with error.\",\n                 },\n             ),\n+            (\n+                \"fail-on\",\n+                {\n+                    \"default\": \"\",\n+                    \"type\": \"csv\",\n+                    \"metavar\": \"<msg ids>\",\n+                    \"help\": \"Return non-zero exit code if any of these messages/categories are detected,\"\n+                    \" even if score is above --fail-under value. Syntax same as enable.\"\n+                    \" Messages specified are enabled, while categories only check already-enabled messages.\",\n+                },\n+            ),\n             (\n                 \"confidence\",\n                 {\n@@ -450,6 +461,7 @@ def __init__(self, options=(), reporter=None, option_groups=(), pylintrc=None):\n         self.current_name = None\n         self.current_file = None\n         self.stats = None\n+        self.fail_on_symbols = []\n         # init options\n         self._external_opts = options\n         self.options = options + PyLinter.make_options()\n@@ -609,6 +621,40 @@ def register_checker(self, checker):\n         if not getattr(checker, \"enabled\", True):\n             self.disable(checker.name)\n \n+    def enable_fail_on_messages(self):\n+        \"\"\"enable 'fail on' msgs\n+\n+        Convert values in config.fail_on (which might be msg category, msg id,\n+        or symbol) to specific msgs, then enable and flag them for later.\n+        \"\"\"\n+        fail_on_vals = self.config.fail_on\n+        if not fail_on_vals:\n+            return\n+\n+        fail_on_cats = set()\n+        fail_on_msgs = set()\n+        for val in fail_on_vals:\n+            # If value is a cateogry, add category, else add message\n+            if val in MSG_TYPES:\n+                fail_on_cats.add(val)\n+            else:\n+                fail_on_msgs.add(val)\n+\n+        # For every message in every checker, if cat or msg flagged, enable check\n+        for all_checkers in self._checkers.values():\n+            for checker in all_checkers:\n+                for msg in checker.messages:\n+                    if msg.msgid in fail_on_msgs or msg.symbol in fail_on_msgs:\n+                        # message id/symbol matched, enable and flag it\n+                        self.enable(msg.msgid)\n+                        self.fail_on_symbols.append(msg.symbol)\n+                    elif msg.msgid[0] in fail_on_cats:\n+                        # message starts with a cateogry value, flag (but do not enable) it\n+                        self.fail_on_symbols.append(msg.symbol)\n+\n+    def any_fail_on_issues(self):\n+        return any(x in self.fail_on_symbols for x in self.stats[\"by_msg\"])\n+\n     def disable_noerror_messages(self):\n         for msgcat, msgids in self.msgs_store._msgs_by_category.items():\n             # enable only messages with 'error' severity and above ('fatal')\ndiff --git a/pylint/lint/run.py b/pylint/lint/run.py\n--- a/pylint/lint/run.py\n+++ b/pylint/lint/run.py\n@@ -368,6 +368,9 @@ def __init__(\n         # load plugin specific configuration.\n         linter.load_plugin_configuration()\n \n+        # Now that plugins are loaded, get list of all fail_on messages, and enable them\n+        linter.enable_fail_on_messages()\n+\n         if self._output:\n             try:\n                 with open(self._output, \"w\") as output:\n@@ -392,7 +395,12 @@ def __init__(\n             if linter.config.exit_zero:\n                 sys.exit(0)\n             else:\n-                if score_value and score_value >= linter.config.fail_under:\n+                if (\n+                    score_value\n+                    and score_value >= linter.config.fail_under\n+                    # detected messages flagged by --fail-on prevent non-zero exit code\n+                    and not linter.any_fail_on_issues()\n+                ):\n                     sys.exit(0)\n                 sys.exit(self.linter.msg_status)\n \n",
  "test_patch": "diff --git a/tests/test_self.py b/tests/test_self.py\n--- a/tests/test_self.py\n+++ b/tests/test_self.py\n@@ -741,6 +741,69 @@ def test_fail_under(self):\n             code=22,\n         )\n \n+    @pytest.mark.parametrize(\n+        \"fu_score,fo_msgs,fname,out\",\n+        [\n+            # Essentially same test cases as --fail-under, but run with/without a detected issue code\n+            # missing-function-docstring (C0116) is issue in both files\n+            # --fail-under should be irrelevant as missing-function-docstring is hit\n+            (-10, \"missing-function-docstring\", \"fail_under_plus7_5.py\", 16),\n+            (6, \"missing-function-docstring\", \"fail_under_plus7_5.py\", 16),\n+            (7.5, \"missing-function-docstring\", \"fail_under_plus7_5.py\", 16),\n+            (7.6, \"missing-function-docstring\", \"fail_under_plus7_5.py\", 16),\n+            (-11, \"missing-function-docstring\", \"fail_under_minus10.py\", 22),\n+            (-10, \"missing-function-docstring\", \"fail_under_minus10.py\", 22),\n+            (-9, \"missing-function-docstring\", \"fail_under_minus10.py\", 22),\n+            (-5, \"missing-function-docstring\", \"fail_under_minus10.py\", 22),\n+            # --fail-under should guide whether error code as missing-function-docstring is not hit\n+            (-10, \"broad-except\", \"fail_under_plus7_5.py\", 0),\n+            (6, \"broad-except\", \"fail_under_plus7_5.py\", 0),\n+            (7.5, \"broad-except\", \"fail_under_plus7_5.py\", 0),\n+            (7.6, \"broad-except\", \"fail_under_plus7_5.py\", 16),\n+            (-11, \"broad-except\", \"fail_under_minus10.py\", 0),\n+            (-10, \"broad-except\", \"fail_under_minus10.py\", 0),\n+            (-9, \"broad-except\", \"fail_under_minus10.py\", 22),\n+            (-5, \"broad-except\", \"fail_under_minus10.py\", 22),\n+            # Enable by message id\n+            (-10, \"C0116\", \"fail_under_plus7_5.py\", 16),\n+            # Enable by category\n+            (-10, \"C\", \"fail_under_plus7_5.py\", 16),\n+            (-10, \"fake1,C,fake2\", \"fail_under_plus7_5.py\", 16),\n+            # Ensure entire category not enabled by any msg id\n+            (-10, \"C0115\", \"fail_under_plus7_5.py\", 0),\n+        ],\n+    )\n+    def test_fail_on(self, fu_score, fo_msgs, fname, out):\n+        self._runtest(\n+            [\n+                \"--fail-under\",\n+                f\"{fu_score:f}\",\n+                f\"--fail-on={fo_msgs}\",\n+                \"--enable=all\",\n+                join(HERE, \"regrtest_data\", fname),\n+            ],\n+            code=out,\n+        )\n+\n+    @pytest.mark.parametrize(\n+        \"opts,out\",\n+        [\n+            # Special case to ensure that disabled items from category aren't enabled\n+            ([\"--disable=C0116\", \"--fail-on=C\"], 0),\n+            # Ensure order does not matter\n+            ([\"--fail-on=C\", \"--disable=C0116\"], 0),\n+            # Ensure --fail-on takes precedence over --disable\n+            ([\"--disable=C0116\", \"--fail-on=C0116\"], 16),\n+            # Ensure order does not matter\n+            ([\"--fail-on=C0116\", \"--disable=C0116\"], 16),\n+        ],\n+    )\n+    def test_fail_on_edge_case(self, opts, out):\n+        self._runtest(\n+            opts + [join(HERE, \"regrtest_data\", \"fail_under_plus7_5.py\")],\n+            code=out,\n+        )\n+\n     @staticmethod\n     def test_modify_sys_path() -> None:\n         @contextlib.contextmanager\n",
  "problem_statement": "Add --fail-on option to always return error code if specific issues (or issue types) are found\nWe're using pylint in CI with two primary goals:\r\n\r\n1. Ensure there are no error-category issues - this can be achieved by looking at the exit code\r\n2. Ensure the overall linting score doesn't deteriorate too much - this can be achieved by using `--fail-under=8`\r\n\r\nHowever if `--fail-under` is used, and the score is above that, then it passes even if there are error-category issues detected. Essentially it's acting as a \"only throw fail (non-zero) codes if under this\", instead of a \"fail if under this, otherwise change nothing\".\r\n\r\nTwo possible solutions I can think of here are:\r\n\r\n1. Have a configuration option to prevent `--fail-under` from consuming other exit statuses. I.e. a way to say \"if score is under N, return error code X, regardless of all else, otherwise change nothing\".\r\n2. Add a new option like `--fail-on=E,unused-import` which means that if there are any `E*` code (error-category) issues, or any `unused-error` issues, then fail, otherwise change nothing.\n",
  "hints_text": "@das-intensity thanks for this suggestion.\n@hippo91 so I'm quite interested in getting this done (ideally the second option, `--fail-on`), and happy to do the dev myself. I just don't want to get a PR in for this, only to be **then** told it doesn't fit the design of pylint/etc. Do you have any suggestion on how to proceed? Just do it and pray, or are there people I should discuss the specifics with first?\r\n\nI also like ``fail-on`` better, we already have a ``--error-only`` option so ``--fail-only-on`` would make more sense I think. @hippo91 and I are the most active admins right now, so consider this idea merge-able. Regarding what to do in the code the best thing to do would probably be to refactor the configuration handling to use arparse or click first (ðŸ˜„), because the configuration code in his current state might make it hard to make a simple reviewable patch. But feel free to read the code and come to your own conclusion.",
  "created_at": "2021-04-24T23:33:42Z",
  "version": "2.8",
  "FAIL_TO_PASS": "[\"tests/test_self.py::TestRunTC::test_fail_on[-10-missing-function-docstring-fail_under_plus7_5.py-16]\", \"tests/test_self.py::TestRunTC::test_fail_on[6-missing-function-docstring-fail_under_plus7_5.py-16]\", \"tests/test_self.py::TestRunTC::test_fail_on[7.5-missing-function-docstring-fail_under_plus7_5.py-16]\", \"tests/test_self.py::TestRunTC::test_fail_on[7.6-missing-function-docstring-fail_under_plus7_5.py-16]\", \"tests/test_self.py::TestRunTC::test_fail_on[-11-missing-function-docstring-fail_under_minus10.py-22]\", \"tests/test_self.py::TestRunTC::test_fail_on[-10-missing-function-docstring-fail_under_minus10.py-22]\", \"tests/test_self.py::TestRunTC::test_fail_on[-9-missing-function-docstring-fail_under_minus10.py-22]\", \"tests/test_self.py::TestRunTC::test_fail_on[-5-missing-function-docstring-fail_under_minus10.py-22]\", \"tests/test_self.py::TestRunTC::test_fail_on[-10-broad-except-fail_under_plus7_5.py-0]\", \"tests/test_self.py::TestRunTC::test_fail_on[6-broad-except-fail_under_plus7_5.py-0]\", \"tests/test_self.py::TestRunTC::test_fail_on[7.5-broad-except-fail_under_plus7_5.py-0]\", \"tests/test_self.py::TestRunTC::test_fail_on[7.6-broad-except-fail_under_plus7_5.py-16]\", \"tests/test_self.py::TestRunTC::test_fail_on[-11-broad-except-fail_under_minus10.py-0]\", \"tests/test_self.py::TestRunTC::test_fail_on[-10-broad-except-fail_under_minus10.py-0]\", \"tests/test_self.py::TestRunTC::test_fail_on[-9-broad-except-fail_under_minus10.py-22]\", \"tests/test_self.py::TestRunTC::test_fail_on[-5-broad-except-fail_under_minus10.py-22]\", \"tests/test_self.py::TestRunTC::test_fail_on[-10-C0116-fail_under_plus7_5.py-16]\", \"tests/test_self.py::TestRunTC::test_fail_on[-10-C-fail_under_plus7_5.py-16]\", \"tests/test_self.py::TestRunTC::test_fail_on[-10-fake1,C,fake2-fail_under_plus7_5.py-16]\", \"tests/test_self.py::TestRunTC::test_fail_on[-10-C0115-fail_under_plus7_5.py-0]\", \"tests/test_self.py::TestRunTC::test_fail_on_edge_case[opts0-0]\", \"tests/test_self.py::TestRunTC::test_fail_on_edge_case[opts1-0]\", \"tests/test_self.py::TestRunTC::test_fail_on_edge_case[opts2-16]\", \"tests/test_self.py::TestRunTC::test_fail_on_edge_case[opts3-16]\"]",
  "PASS_TO_PASS": "[\"tests/test_self.py::TestRunTC::test_pkginfo\", \"tests/test_self.py::TestRunTC::test_all\", \"tests/test_self.py::TestRunTC::test_no_ext_file\", \"tests/test_self.py::TestRunTC::test_w0704_ignored\", \"tests/test_self.py::TestRunTC::test_exit_zero\", \"tests/test_self.py::TestRunTC::test_generate_config_option\", \"tests/test_self.py::TestRunTC::test_generate_config_option_order\", \"tests/test_self.py::TestRunTC::test_generate_config_disable_symbolic_names\", \"tests/test_self.py::TestRunTC::test_generate_rcfile_no_obsolete_methods\", \"tests/test_self.py::TestRunTC::test_nonexistent_config_file\", \"tests/test_self.py::TestRunTC::test_help_message_option\", \"tests/test_self.py::TestRunTC::test_error_help_message_option\", \"tests/test_self.py::TestRunTC::test_error_missing_arguments\", \"tests/test_self.py::TestRunTC::test_no_out_encoding\", \"tests/test_self.py::TestRunTC::test_parallel_execution\", \"tests/test_self.py::TestRunTC::test_parallel_execution_bug_2674\", \"tests/test_self.py::TestRunTC::test_parallel_execution_missing_arguments\", \"tests/test_self.py::TestRunTC::test_py3k_option\", \"tests/test_self.py::TestRunTC::test_py3k_jobs_option\", \"tests/test_self.py::TestRunTC::test_abbreviations_are_not_supported\", \"tests/test_self.py::TestRunTC::test_enable_all_works\", \"tests/test_self.py::TestRunTC::test_wrong_import_position_when_others_disabled\", \"tests/test_self.py::TestRunTC::test_import_itself_not_accounted_for_relative_imports\", \"tests/test_self.py::TestRunTC::test_reject_empty_indent_strings\", \"tests/test_self.py::TestRunTC::test_json_report_when_file_has_syntax_error\", \"tests/test_self.py::TestRunTC::test_json_report_when_file_is_missing\", \"tests/test_self.py::TestRunTC::test_json_report_does_not_escape_quotes\", \"tests/test_self.py::TestRunTC::test_information_category_disabled_by_default\", \"tests/test_self.py::TestRunTC::test_error_mode_shows_no_score\", \"tests/test_self.py::TestRunTC::test_evaluation_score_shown_by_default\", \"tests/test_self.py::TestRunTC::test_confidence_levels\", \"tests/test_self.py::TestRunTC::test_bom_marker\", \"tests/test_self.py::TestRunTC::test_pylintrc_plugin_duplicate_options\", \"tests/test_self.py::TestRunTC::test_pylintrc_comments_in_values\", \"tests/test_self.py::TestRunTC::test_no_crash_with_formatting_regex_defaults\", \"tests/test_self.py::TestRunTC::test_getdefaultencoding_crashes_with_lc_ctype_utf8\", \"tests/test_self.py::TestRunTC::test_parseable_file_path\", \"tests/test_self.py::TestRunTC::test_stdin[/mymodule.py]\", \"tests/test_self.py::TestRunTC::test_stdin[mymodule.py-mymodule-mymodule.py]\", \"tests/test_self.py::TestRunTC::test_stdin_missing_modulename\", \"tests/test_self.py::TestRunTC::test_relative_imports[False]\", \"tests/test_self.py::TestRunTC::test_relative_imports[True]\", \"tests/test_self.py::TestRunTC::test_stdin_syntaxerror\", \"tests/test_self.py::TestRunTC::test_version\", \"tests/test_self.py::TestRunTC::test_fail_under\", \"tests/test_self.py::TestRunTC::test_modify_sys_path\", \"tests/test_self.py::TestRunTC::test_do_not_import_files_from_local_directory\", \"tests/test_self.py::TestRunTC::test_do_not_import_files_from_local_directory_with_pythonpath\", \"tests/test_self.py::TestRunTC::test_import_plugin_from_local_directory_if_pythonpath_cwd\", \"tests/test_self.py::TestRunTC::test_allow_import_of_files_found_in_modules_during_parallel_check\", \"tests/test_self.py::TestRunTC::test_jobs_score\", \"tests/test_self.py::TestRunTC::test_duplicate_code_raw_strings\", \"tests/test_self.py::TestRunTC::test_regression_parallel_mode_without_filepath\", \"tests/test_self.py::TestRunTC::test_output_file_valid_path\", \"tests/test_self.py::TestRunTC::test_output_file_invalid_path_exits_with_code_32\", \"tests/test_self.py::TestRunTC::test_output_file_can_be_combined_with_output_format_option[text-tests/regrtest_data/unused_variable.py:4:4:\", \"tests/test_self.py::TestRunTC::test_output_file_can_be_combined_with_output_format_option[parseable-tests/regrtest_data/unused_variable.py:4:\", \"tests/test_self.py::TestRunTC::test_output_file_can_be_combined_with_output_format_option[msvs-tests/regrtest_data/unused_variable.py(4):\", \"tests/test_self.py::TestRunTC::test_output_file_can_be_combined_with_output_format_option[colorized-tests/regrtest_data/unused_variable.py:4:4:\", \"tests/test_self.py::TestRunTC::test_output_file_can_be_combined_with_output_format_option[json-\\\"message\\\":\", \"tests/test_self.py::TestRunTC::test_output_file_can_be_combined_with_custom_reporter\", \"tests/test_self.py::TestRunTC::test_output_file_specified_in_rcfile\"]",
  "environment_setup_commit": "49a6206c7756307844c1c32c256afdf9836d7bce",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.907515",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}