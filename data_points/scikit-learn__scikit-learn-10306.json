{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-10306",
  "base_commit": "b90661d6a46aa3619d3eec94d5281f5888add501",
  "patch": "diff --git a/sklearn/cluster/affinity_propagation_.py b/sklearn/cluster/affinity_propagation_.py\n--- a/sklearn/cluster/affinity_propagation_.py\n+++ b/sklearn/cluster/affinity_propagation_.py\n@@ -390,5 +390,5 @@ def predict(self, X):\n         else:\n             warnings.warn(\"This model does not have any cluster centers \"\n                           \"because affinity propagation did not converge. \"\n-                          \"Labeling every sample as '-1'.\")\n+                          \"Labeling every sample as '-1'.\", ConvergenceWarning)\n             return np.array([-1] * X.shape[0])\ndiff --git a/sklearn/cluster/birch.py b/sklearn/cluster/birch.py\n--- a/sklearn/cluster/birch.py\n+++ b/sklearn/cluster/birch.py\n@@ -15,7 +15,7 @@\n from ..utils import check_array\n from ..utils.extmath import row_norms, safe_sparse_dot\n from ..utils.validation import check_is_fitted\n-from ..exceptions import NotFittedError\n+from ..exceptions import NotFittedError, ConvergenceWarning\n from .hierarchical import AgglomerativeClustering\n \n \n@@ -626,7 +626,7 @@ def _global_clustering(self, X=None):\n                 warnings.warn(\n                     \"Number of subclusters found (%d) by Birch is less \"\n                     \"than (%d). Decrease the threshold.\"\n-                    % (len(centroids), self.n_clusters))\n+                    % (len(centroids), self.n_clusters), ConvergenceWarning)\n         else:\n             # The global clustering step that clusters the subclusters of\n             # the leaves. It assumes the centroids of the subclusters as\ndiff --git a/sklearn/cross_decomposition/pls_.py b/sklearn/cross_decomposition/pls_.py\n--- a/sklearn/cross_decomposition/pls_.py\n+++ b/sklearn/cross_decomposition/pls_.py\n@@ -16,6 +16,7 @@\n from ..utils import check_array, check_consistent_length\n from ..utils.extmath import svd_flip\n from ..utils.validation import check_is_fitted, FLOAT_DTYPES\n+from ..exceptions import ConvergenceWarning\n from ..externals import six\n \n __all__ = ['PLSCanonical', 'PLSRegression', 'PLSSVD']\n@@ -74,7 +75,8 @@ def _nipals_twoblocks_inner_loop(X, Y, mode=\"A\", max_iter=500, tol=1e-06,\n         if np.dot(x_weights_diff.T, x_weights_diff) < tol or Y.shape[1] == 1:\n             break\n         if ite == max_iter:\n-            warnings.warn('Maximum number of iterations reached')\n+            warnings.warn('Maximum number of iterations reached',\n+                          ConvergenceWarning)\n             break\n         x_weights_old = x_weights\n         ite += 1\ndiff --git a/sklearn/decomposition/fastica_.py b/sklearn/decomposition/fastica_.py\n--- a/sklearn/decomposition/fastica_.py\n+++ b/sklearn/decomposition/fastica_.py\n@@ -15,6 +15,7 @@\n from scipy import linalg\n \n from ..base import BaseEstimator, TransformerMixin\n+from ..exceptions import ConvergenceWarning\n from ..externals import six\n from ..externals.six import moves\n from ..externals.six import string_types\n@@ -116,7 +117,8 @@ def _ica_par(X, tol, g, fun_args, max_iter, w_init):\n             break\n     else:\n         warnings.warn('FastICA did not converge. Consider increasing '\n-                      'tolerance or the maximum number of iterations.')\n+                      'tolerance or the maximum number of iterations.',\n+                      ConvergenceWarning)\n \n     return W, ii + 1\n \ndiff --git a/sklearn/gaussian_process/gpc.py b/sklearn/gaussian_process/gpc.py\n--- a/sklearn/gaussian_process/gpc.py\n+++ b/sklearn/gaussian_process/gpc.py\n@@ -19,6 +19,7 @@\n from sklearn.utils import check_random_state\n from sklearn.preprocessing import LabelEncoder\n from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n+from sklearn.exceptions import ConvergenceWarning\n \n \n # Values required for approximating the logistic sigmoid by\n@@ -428,7 +429,8 @@ def _constrained_optimization(self, obj_func, initial_theta, bounds):\n                 fmin_l_bfgs_b(obj_func, initial_theta, bounds=bounds)\n             if convergence_dict[\"warnflag\"] != 0:\n                 warnings.warn(\"fmin_l_bfgs_b terminated abnormally with the \"\n-                              \" state: %s\" % convergence_dict)\n+                              \" state: %s\" % convergence_dict,\n+                              ConvergenceWarning)\n         elif callable(self.optimizer):\n             theta_opt, func_min = \\\n                 self.optimizer(obj_func, initial_theta, bounds=bounds)\ndiff --git a/sklearn/gaussian_process/gpr.py b/sklearn/gaussian_process/gpr.py\n--- a/sklearn/gaussian_process/gpr.py\n+++ b/sklearn/gaussian_process/gpr.py\n@@ -16,6 +16,7 @@\n from sklearn.utils import check_random_state\n from sklearn.utils.validation import check_X_y, check_array\n from sklearn.utils.deprecation import deprecated\n+from sklearn.exceptions import ConvergenceWarning\n \n \n class GaussianProcessRegressor(BaseEstimator, RegressorMixin):\n@@ -461,7 +462,8 @@ def _constrained_optimization(self, obj_func, initial_theta, bounds):\n                 fmin_l_bfgs_b(obj_func, initial_theta, bounds=bounds)\n             if convergence_dict[\"warnflag\"] != 0:\n                 warnings.warn(\"fmin_l_bfgs_b terminated abnormally with the \"\n-                              \" state: %s\" % convergence_dict)\n+                              \" state: %s\" % convergence_dict,\n+                              ConvergenceWarning)\n         elif callable(self.optimizer):\n             theta_opt, func_min = \\\n                 self.optimizer(obj_func, initial_theta, bounds=bounds)\ndiff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py\n--- a/sklearn/linear_model/logistic.py\n+++ b/sklearn/linear_model/logistic.py\n@@ -29,7 +29,7 @@\n from ..utils.fixes import logsumexp\n from ..utils.optimize import newton_cg\n from ..utils.validation import check_X_y\n-from ..exceptions import NotFittedError\n+from ..exceptions import NotFittedError, ConvergenceWarning\n from ..utils.multiclass import check_classification_targets\n from ..externals.joblib import Parallel, delayed\n from ..model_selection import check_cv\n@@ -716,7 +716,7 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,\n                     iprint=(verbose > 0) - 1, pgtol=tol)\n             if info[\"warnflag\"] == 1 and verbose > 0:\n                 warnings.warn(\"lbfgs failed to converge. Increase the number \"\n-                              \"of iterations.\")\n+                              \"of iterations.\", ConvergenceWarning)\n             try:\n                 n_iter_i = info['nit'] - 1\n             except:\ndiff --git a/sklearn/linear_model/ransac.py b/sklearn/linear_model/ransac.py\n--- a/sklearn/linear_model/ransac.py\n+++ b/sklearn/linear_model/ransac.py\n@@ -13,6 +13,7 @@\n from ..utils.validation import check_is_fitted\n from .base import LinearRegression\n from ..utils.validation import has_fit_parameter\n+from ..exceptions import ConvergenceWarning\n \n _EPSILON = np.spacing(1)\n \n@@ -453,7 +454,7 @@ def fit(self, X, y, sample_weight=None):\n                               \" early due to skipping more iterations than\"\n                               \" `max_skips`. See estimator attributes for\"\n                               \" diagnostics (n_skips*).\",\n-                              UserWarning)\n+                              ConvergenceWarning)\n \n         # estimate final model using all inliers\n         base_estimator.fit(X_inlier_best, y_inlier_best)\ndiff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py\n--- a/sklearn/linear_model/ridge.py\n+++ b/sklearn/linear_model/ridge.py\n@@ -31,6 +31,7 @@\n from ..model_selection import GridSearchCV\n from ..externals import six\n from ..metrics.scorer import check_scoring\n+from ..exceptions import ConvergenceWarning\n \n \n def _solve_sparse_cg(X, y, alpha, max_iter=None, tol=1e-3, verbose=0):\n@@ -73,7 +74,7 @@ def _mv(x):\n \n         if max_iter is None and info > 0 and verbose:\n             warnings.warn(\"sparse_cg did not converge after %d iterations.\" %\n-                          info)\n+                          info, ConvergenceWarning)\n \n     return coefs\n \n",
  "test_patch": "diff --git a/sklearn/cluster/tests/test_affinity_propagation.py b/sklearn/cluster/tests/test_affinity_propagation.py\n--- a/sklearn/cluster/tests/test_affinity_propagation.py\n+++ b/sklearn/cluster/tests/test_affinity_propagation.py\n@@ -133,12 +133,14 @@ def test_affinity_propagation_predict_non_convergence():\n     X = np.array([[0, 0], [1, 1], [-2, -2]])\n \n     # Force non-convergence by allowing only a single iteration\n-    af = AffinityPropagation(preference=-10, max_iter=1).fit(X)\n+    af = assert_warns(ConvergenceWarning,\n+                      AffinityPropagation(preference=-10, max_iter=1).fit, X)\n \n     # At prediction time, consider new samples as noise since there are no\n     # clusters\n-    assert_array_equal(np.array([-1, -1, -1]),\n-                       af.predict(np.array([[2, 2], [3, 3], [4, 4]])))\n+    to_predict = np.array([[2, 2], [3, 3], [4, 4]])\n+    y = assert_warns(ConvergenceWarning, af.predict, to_predict)\n+    assert_array_equal(np.array([-1, -1, -1]), y)\n \n \n def test_equal_similarities_and_preferences():\ndiff --git a/sklearn/cluster/tests/test_birch.py b/sklearn/cluster/tests/test_birch.py\n--- a/sklearn/cluster/tests/test_birch.py\n+++ b/sklearn/cluster/tests/test_birch.py\n@@ -9,6 +9,7 @@\n from sklearn.cluster.birch import Birch\n from sklearn.cluster.hierarchical import AgglomerativeClustering\n from sklearn.datasets import make_blobs\n+from sklearn.exceptions import ConvergenceWarning\n from sklearn.linear_model import ElasticNet\n from sklearn.metrics import pairwise_distances_argmin, v_measure_score\n \n@@ -93,7 +94,7 @@ def test_n_clusters():\n \n     # Test that a small number of clusters raises a warning.\n     brc4 = Birch(threshold=10000.)\n-    assert_warns(UserWarning, brc4.fit, X)\n+    assert_warns(ConvergenceWarning, brc4.fit, X)\n \n \n def test_sparse_X():\ndiff --git a/sklearn/cross_decomposition/tests/test_pls.py b/sklearn/cross_decomposition/tests/test_pls.py\n--- a/sklearn/cross_decomposition/tests/test_pls.py\n+++ b/sklearn/cross_decomposition/tests/test_pls.py\n@@ -3,11 +3,12 @@\n \n from sklearn.utils.testing import (assert_equal, assert_array_almost_equal,\n                                    assert_array_equal, assert_true,\n-                                   assert_raise_message)\n+                                   assert_raise_message, assert_warns)\n from sklearn.datasets import load_linnerud\n from sklearn.cross_decomposition import pls_, CCA\n from sklearn.preprocessing import StandardScaler\n from sklearn.utils import check_random_state\n+from sklearn.exceptions import ConvergenceWarning\n \n \n def test_pls():\n@@ -260,6 +261,15 @@ def check_ortho(M, err_msg):\n     check_ortho(pls_ca.y_scores_, \"y scores are not orthogonal\")\n \n \n+def test_convergence_fail():\n+    d = load_linnerud()\n+    X = d.data\n+    Y = d.target\n+    pls_bynipals = pls_.PLSCanonical(n_components=X.shape[1],\n+                                     max_iter=2, tol=1e-10)\n+    assert_warns(ConvergenceWarning, pls_bynipals.fit, X, Y)\n+\n+\n def test_PLSSVD():\n     # Let's check the PLSSVD doesn't return all possible component but just\n     # the specified number\ndiff --git a/sklearn/decomposition/tests/test_fastica.py b/sklearn/decomposition/tests/test_fastica.py\n--- a/sklearn/decomposition/tests/test_fastica.py\n+++ b/sklearn/decomposition/tests/test_fastica.py\n@@ -18,6 +18,7 @@\n from sklearn.decomposition import FastICA, fastica, PCA\n from sklearn.decomposition.fastica_ import _gs_decorrelation\n from sklearn.externals.six import moves\n+from sklearn.exceptions import ConvergenceWarning\n \n \n def center_and_norm(x, axis=-1):\n@@ -141,6 +142,31 @@ def test_fastica_nowhiten():\n     assert_true(hasattr(ica, 'mixing_'))\n \n \n+def test_fastica_convergence_fail():\n+    # Test the FastICA algorithm on very simple data\n+    # (see test_non_square_fastica).\n+    # Ensure a ConvergenceWarning raised if the tolerance is sufficiently low.\n+    rng = np.random.RandomState(0)\n+\n+    n_samples = 1000\n+    # Generate two sources:\n+    t = np.linspace(0, 100, n_samples)\n+    s1 = np.sin(t)\n+    s2 = np.ceil(np.sin(np.pi * t))\n+    s = np.c_[s1, s2].T\n+    center_and_norm(s)\n+    s1, s2 = s\n+\n+    # Mixing matrix\n+    mixing = rng.randn(6, 2)\n+    m = np.dot(mixing, s)\n+\n+    # Do fastICA with tolerance 0. to ensure failing convergence\n+    ica = FastICA(algorithm=\"parallel\", n_components=2, random_state=rng,\n+                  max_iter=2, tol=0.)\n+    assert_warns(ConvergenceWarning, ica.fit, m.T)\n+\n+\n def test_non_square_fastica(add_noise=False):\n     # Test the FastICA algorithm on very simple data.\n     rng = np.random.RandomState(0)\ndiff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py\n--- a/sklearn/linear_model/tests/test_logistic.py\n+++ b/sklearn/linear_model/tests/test_logistic.py\n@@ -312,6 +312,15 @@ def test_consistency_path():\n                                   err_msg=\"with solver = %s\" % solver)\n \n \n+def test_logistic_regression_path_convergence_fail():\n+    rng = np.random.RandomState(0)\n+    X = np.concatenate((rng.randn(100, 2) + [1, 1], rng.randn(100, 2)))\n+    y = [1] * 100 + [-1] * 100\n+    Cs = [1e3]\n+    assert_warns(ConvergenceWarning, logistic_regression_path,\n+                 X, y, Cs=Cs, tol=0., max_iter=1, random_state=0, verbose=1)\n+\n+\n def test_liblinear_dual_random_state():\n     # random_state is relevant for liblinear solver only if dual=True\n     X, y = make_classification(n_samples=20, random_state=0)\ndiff --git a/sklearn/linear_model/tests/test_ransac.py b/sklearn/linear_model/tests/test_ransac.py\n--- a/sklearn/linear_model/tests/test_ransac.py\n+++ b/sklearn/linear_model/tests/test_ransac.py\n@@ -13,6 +13,7 @@\n from sklearn.utils.testing import assert_raises\n from sklearn.linear_model import LinearRegression, RANSACRegressor, Lasso\n from sklearn.linear_model.ransac import _dynamic_max_trials\n+from sklearn.exceptions import ConvergenceWarning\n \n \n # Generate coordinates of line\n@@ -230,7 +231,7 @@ def is_data_valid(X, y):\n                                        max_skips=3,\n                                        max_trials=5)\n \n-    assert_warns(UserWarning, ransac_estimator.fit, X, y)\n+    assert_warns(ConvergenceWarning, ransac_estimator.fit, X, y)\n     assert_equal(ransac_estimator.n_skips_no_inliers_, 0)\n     assert_equal(ransac_estimator.n_skips_invalid_data_, 4)\n     assert_equal(ransac_estimator.n_skips_invalid_model_, 0)\ndiff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py\n--- a/sklearn/linear_model/tests/test_ridge.py\n+++ b/sklearn/linear_model/tests/test_ridge.py\n@@ -14,6 +14,8 @@\n from sklearn.utils.testing import ignore_warnings\n from sklearn.utils.testing import assert_warns\n \n+from sklearn.exceptions import ConvergenceWarning\n+\n from sklearn import datasets\n from sklearn.metrics import mean_squared_error\n from sklearn.metrics import make_scorer\n@@ -137,6 +139,16 @@ def test_ridge_regression_sample_weights():\n                 assert_array_almost_equal(coefs, coefs2)\n \n \n+def test_ridge_regression_convergence_fail():\n+    rng = np.random.RandomState(0)\n+    y = rng.randn(5)\n+    X = rng.randn(5, 10)\n+\n+    assert_warns(ConvergenceWarning, ridge_regression,\n+                 X, y, alpha=1.0, solver=\"sparse_cg\",\n+                 tol=0., max_iter=None, verbose=1)\n+\n+\n def test_ridge_sample_weights():\n     # TODO: loop over sparse data as well\n \n",
  "problem_statement": "Some UserWarnings should be ConvergenceWarnings\nSome warnings raised during testing show that we do not use `ConvergenceWarning` when it is appropriate in some cases. For example (from [here](https://github.com/scikit-learn/scikit-learn/issues/10158#issuecomment-345453334)):\r\n\r\n```python\r\n/home/lesteve/dev/alt-scikit-learn/sklearn/decomposition/fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\r\n/home/lesteve/dev/alt-scikit-learn/sklearn/cluster/birch.py:629: UserWarning: Number of subclusters found (2) by Birch is less than (3). Decrease the threshold.\r\n```\r\n\r\nThese should be changed, at least. For bonus points, the contributor could look for other warning messages that mention \"converge\".\n",
  "hints_text": "Could I give this a go?\n@patrick1011 please go ahead!",
  "created_at": "2017-12-13T15:10:48Z",
  "version": "0.20",
  "FAIL_TO_PASS": "[\"sklearn/cluster/tests/test_affinity_propagation.py::test_affinity_propagation_predict_non_convergence\", \"sklearn/cluster/tests/test_birch.py::test_n_clusters\", \"sklearn/cross_decomposition/tests/test_pls.py::test_convergence_fail\", \"sklearn/decomposition/tests/test_fastica.py::test_fastica_convergence_fail\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_path_convergence_fail\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_warn_exceed_max_skips\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_convergence_fail\"]",
  "PASS_TO_PASS": "[\"sklearn/cluster/tests/test_affinity_propagation.py::test_affinity_propagation\", \"sklearn/cluster/tests/test_affinity_propagation.py::test_affinity_propagation_predict\", \"sklearn/cluster/tests/test_affinity_propagation.py::test_affinity_propagation_predict_error\", \"sklearn/cluster/tests/test_affinity_propagation.py::test_affinity_propagation_fit_non_convergence\", \"sklearn/cluster/tests/test_affinity_propagation.py::test_affinity_propagation_equal_mutual_similarities\", \"sklearn/cluster/tests/test_affinity_propagation.py::test_equal_similarities_and_preferences\", \"sklearn/cluster/tests/test_birch.py::test_n_samples_leaves_roots\", \"sklearn/cluster/tests/test_birch.py::test_partial_fit\", \"sklearn/cluster/tests/test_birch.py::test_birch_predict\", \"sklearn/cluster/tests/test_birch.py::test_sparse_X\", \"sklearn/cluster/tests/test_birch.py::test_branching_factor\", \"sklearn/cluster/tests/test_birch.py::test_threshold\", \"sklearn/cross_decomposition/tests/test_pls.py::test_pls\", \"sklearn/cross_decomposition/tests/test_pls.py::test_PLSSVD\", \"sklearn/cross_decomposition/tests/test_pls.py::test_univariate_pls_regression\", \"sklearn/cross_decomposition/tests/test_pls.py::test_predict_transform_copy\", \"sklearn/cross_decomposition/tests/test_pls.py::test_pls_errors\", \"sklearn/cross_decomposition/tests/test_pls.py::test_pls_scaling\", \"sklearn/decomposition/tests/test_fastica.py::test_gs\", \"sklearn/decomposition/tests/test_fastica.py::test_fastica_simple\", \"sklearn/decomposition/tests/test_fastica.py::test_fastica_nowhiten\", \"sklearn/decomposition/tests/test_fastica.py::test_non_square_fastica\", \"sklearn/decomposition/tests/test_fastica.py::test_fit_transform\", \"sklearn/decomposition/tests/test_fastica.py::test_inverse_transform\", \"sklearn/linear_model/tests/test_logistic.py::test_predict_2_classes\", \"sklearn/linear_model/tests/test_logistic.py::test_error\", \"sklearn/linear_model/tests/test_logistic.py::test_lr_liblinear_warning\", \"sklearn/linear_model/tests/test_logistic.py::test_predict_3_classes\", \"sklearn/linear_model/tests/test_logistic.py::test_predict_iris\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_validation\", \"sklearn/linear_model/tests/test_logistic.py::test_check_solver_option\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_binary\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_binary_probabilities\", \"sklearn/linear_model/tests/test_logistic.py::test_sparsify\", \"sklearn/linear_model/tests/test_logistic.py::test_inconsistent_input\", \"sklearn/linear_model/tests/test_logistic.py::test_write_parameters\", \"sklearn/linear_model/tests/test_logistic.py::test_nan\", \"sklearn/linear_model/tests/test_logistic.py::test_consistency_path\", \"sklearn/linear_model/tests/test_logistic.py::test_liblinear_dual_random_state\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_loss_and_grad\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_grad_hess\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_logistic_regression_string_inputs\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv_sparse\", \"sklearn/linear_model/tests/test_logistic.py::test_intercept_logistic_helper\", \"sklearn/linear_model/tests/test_logistic.py::test_ovr_multinomial_iris\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_solvers\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_solvers_multiclass\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regressioncv_class_weights\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_sample_weights\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_class_weights\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_convergence_warnings\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multinomial\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_grad_hess\", \"sklearn/linear_model/tests/test_logistic.py::test_liblinear_decision_function_zero\", \"sklearn/linear_model/tests/test_logistic.py::test_liblinear_logregcv_sparse\", \"sklearn/linear_model/tests/test_logistic.py::test_saga_sparse\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_intercept_scaling\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_intercept_scaling_zero\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_l1\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_l1_sparse_data\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_cv_penalty\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_predict_proba_multinomial\", \"sklearn/linear_model/tests/test_logistic.py::test_n_iter\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start\", \"sklearn/linear_model/tests/test_logistic.py::test_saga_vs_liblinear\", \"sklearn/linear_model/tests/test_logistic.py::test_dtype_match\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_inliers_outliers\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_is_data_valid\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_is_model_valid\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_max_trials\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_stop_n_inliers\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_stop_score\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_score\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_predict\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_resid_thresh_no_inliers\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_no_valid_data\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_no_valid_model\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_exceed_max_skips\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_sparse_coo\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_sparse_csr\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_sparse_csc\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_none_estimator\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_min_n_samples\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_multi_dimensional_targets\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_residual_metric\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_residual_loss\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_default_residual_threshold\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_dynamic_max_trials\", \"sklearn/linear_model/tests/test_ransac.py::test_ransac_fit_sample_weight\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge\", \"sklearn/linear_model/tests/test_ridge.py::test_primal_dual_relationship\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_singular\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_sample_weights\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_sample_weights\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_shapes\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_intercept\", \"sklearn/linear_model/tests/test_ridge.py::test_toy_ridge_object\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_vs_lstsq\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_individual_penalties\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_cv_sparse_svd\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_sparse_svd\", \"sklearn/linear_model/tests/test_ridge.py::test_class_weights\", \"sklearn/linear_model/tests/test_ridge.py::test_class_weight_vs_sample_weight\", \"sklearn/linear_model/tests/test_ridge.py::test_class_weights_cv\", \"sklearn/linear_model/tests/test_ridge.py::test_ridgecv_store_cv_values\", \"sklearn/linear_model/tests/test_ridge.py::test_ridgecv_sample_weight\", \"sklearn/linear_model/tests/test_ridge.py::test_raises_value_error_if_sample_weights_greater_than_1d\", \"sklearn/linear_model/tests/test_ridge.py::test_sparse_design_with_sample_weights\", \"sklearn/linear_model/tests/test_ridge.py::test_raises_value_error_if_solver_not_supported\", \"sklearn/linear_model/tests/test_ridge.py::test_sparse_cg_max_iter\", \"sklearn/linear_model/tests/test_ridge.py::test_n_iter\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_fit_intercept_sparse\", \"sklearn/linear_model/tests/test_ridge.py::test_errors_and_values_helper\", \"sklearn/linear_model/tests/test_ridge.py::test_errors_and_values_svd_helper\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_classifier_no_support_multilabel\", \"sklearn/linear_model/tests/test_ridge.py::test_dtype_match\", \"sklearn/linear_model/tests/test_ridge.py::test_dtype_match_cholesky\"]",
  "environment_setup_commit": "55bf5d93e5674f13a1134d93a11fd0cd11aabcd1",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.949259",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}