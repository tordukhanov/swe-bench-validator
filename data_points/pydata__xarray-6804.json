{
  "repo": "pydata/xarray",
  "instance_id": "pydata__xarray-6804",
  "base_commit": "f045401ca79ecd1b80a0da67f44404c4e208fe31",
  "patch": "diff --git a/xarray/core/duck_array_ops.py b/xarray/core/duck_array_ops.py\n--- a/xarray/core/duck_array_ops.py\n+++ b/xarray/core/duck_array_ops.py\n@@ -329,7 +329,11 @@ def f(values, axis=None, skipna=None, **kwargs):\n             if name in [\"sum\", \"prod\"]:\n                 kwargs.pop(\"min_count\", None)\n \n-            func = getattr(np, name)\n+            if hasattr(values, \"__array_namespace__\"):\n+                xp = values.__array_namespace__()\n+                func = getattr(xp, name)\n+            else:\n+                func = getattr(np, name)\n \n         try:\n             with warnings.catch_warnings():\ndiff --git a/xarray/core/indexing.py b/xarray/core/indexing.py\n--- a/xarray/core/indexing.py\n+++ b/xarray/core/indexing.py\n@@ -679,6 +679,8 @@ def as_indexable(array):\n         return DaskIndexingAdapter(array)\n     if hasattr(array, \"__array_function__\"):\n         return NdArrayLikeIndexingAdapter(array)\n+    if hasattr(array, \"__array_namespace__\"):\n+        return ArrayApiIndexingAdapter(array)\n \n     raise TypeError(f\"Invalid array type: {type(array)}\")\n \n@@ -1288,6 +1290,49 @@ def __init__(self, array):\n         self.array = array\n \n \n+class ArrayApiIndexingAdapter(ExplicitlyIndexedNDArrayMixin):\n+    \"\"\"Wrap an array API array to use explicit indexing.\"\"\"\n+\n+    __slots__ = (\"array\",)\n+\n+    def __init__(self, array):\n+        if not hasattr(array, \"__array_namespace__\"):\n+            raise TypeError(\n+                \"ArrayApiIndexingAdapter must wrap an object that \"\n+                \"implements the __array_namespace__ protocol\"\n+            )\n+        self.array = array\n+\n+    def __getitem__(self, key):\n+        if isinstance(key, BasicIndexer):\n+            return self.array[key.tuple]\n+        elif isinstance(key, OuterIndexer):\n+            # manual orthogonal indexing (implemented like DaskIndexingAdapter)\n+            key = key.tuple\n+            value = self.array\n+            for axis, subkey in reversed(list(enumerate(key))):\n+                value = value[(slice(None),) * axis + (subkey, Ellipsis)]\n+            return value\n+        else:\n+            if isinstance(key, VectorizedIndexer):\n+                raise TypeError(\"Vectorized indexing is not supported\")\n+            else:\n+                raise TypeError(f\"Unrecognized indexer: {key}\")\n+\n+    def __setitem__(self, key, value):\n+        if isinstance(key, (BasicIndexer, OuterIndexer)):\n+            self.array[key.tuple] = value\n+        else:\n+            if isinstance(key, VectorizedIndexer):\n+                raise TypeError(\"Vectorized indexing is not supported\")\n+            else:\n+                raise TypeError(f\"Unrecognized indexer: {key}\")\n+\n+    def transpose(self, order):\n+        xp = self.array.__array_namespace__()\n+        return xp.permute_dims(self.array, order)\n+\n+\n class DaskIndexingAdapter(ExplicitlyIndexedNDArrayMixin):\n     \"\"\"Wrap a dask array to support explicit indexing.\"\"\"\n \ndiff --git a/xarray/core/utils.py b/xarray/core/utils.py\n--- a/xarray/core/utils.py\n+++ b/xarray/core/utils.py\n@@ -263,8 +263,10 @@ def is_duck_array(value: Any) -> bool:\n         hasattr(value, \"ndim\")\n         and hasattr(value, \"shape\")\n         and hasattr(value, \"dtype\")\n-        and hasattr(value, \"__array_function__\")\n-        and hasattr(value, \"__array_ufunc__\")\n+        and (\n+            (hasattr(value, \"__array_function__\") and hasattr(value, \"__array_ufunc__\"))\n+            or hasattr(value, \"__array_namespace__\")\n+        )\n     )\n \n \n@@ -298,6 +300,7 @@ def _is_scalar(value, include_0d):\n         or not (\n             isinstance(value, (Iterable,) + NON_NUMPY_SUPPORTED_ARRAY_TYPES)\n             or hasattr(value, \"__array_function__\")\n+            or hasattr(value, \"__array_namespace__\")\n         )\n     )\n \ndiff --git a/xarray/core/variable.py b/xarray/core/variable.py\n--- a/xarray/core/variable.py\n+++ b/xarray/core/variable.py\n@@ -237,7 +237,9 @@ def as_compatible_data(data, fastpath=False):\n         else:\n             data = np.asarray(data)\n \n-    if not isinstance(data, np.ndarray) and hasattr(data, \"__array_function__\"):\n+    if not isinstance(data, np.ndarray) and (\n+        hasattr(data, \"__array_function__\") or hasattr(data, \"__array_namespace__\")\n+    ):\n         return data\n \n     # validate whether the data is valid data types.\n",
  "test_patch": "diff --git a/xarray/tests/test_array_api.py b/xarray/tests/test_array_api.py\nnew file mode 100644\n--- /dev/null\n+++ b/xarray/tests/test_array_api.py\n@@ -0,0 +1,51 @@\n+from typing import Tuple\n+\n+import pytest\n+\n+import xarray as xr\n+from xarray.testing import assert_equal\n+\n+np = pytest.importorskip(\"numpy\", minversion=\"1.22\")\n+\n+import numpy.array_api as xp  # isort:skip\n+from numpy.array_api._array_object import Array  # isort:skip\n+\n+\n+@pytest.fixture\n+def arrays() -> Tuple[xr.DataArray, xr.DataArray]:\n+    np_arr = xr.DataArray(np.ones((2, 3)), dims=(\"x\", \"y\"), coords={\"x\": [10, 20]})\n+    xp_arr = xr.DataArray(xp.ones((2, 3)), dims=(\"x\", \"y\"), coords={\"x\": [10, 20]})\n+    assert isinstance(xp_arr.data, Array)\n+    return np_arr, xp_arr\n+\n+\n+def test_arithmetic(arrays) -> None:\n+    np_arr, xp_arr = arrays\n+    expected = np_arr + 7\n+    actual = xp_arr + 7\n+    assert isinstance(actual.data, Array)\n+    assert_equal(actual, expected)\n+\n+\n+def test_aggregation(arrays) -> None:\n+    np_arr, xp_arr = arrays\n+    expected = np_arr.sum(skipna=False)\n+    actual = xp_arr.sum(skipna=False)\n+    assert isinstance(actual.data, Array)\n+    assert_equal(actual, expected)\n+\n+\n+def test_indexing(arrays) -> None:\n+    np_arr, xp_arr = arrays\n+    expected = np_arr[:, 0]\n+    actual = xp_arr[:, 0]\n+    assert isinstance(actual.data, Array)\n+    assert_equal(actual, expected)\n+\n+\n+def test_reorganizing_operation(arrays) -> None:\n+    np_arr, xp_arr = arrays\n+    expected = np_arr.transpose()\n+    actual = xp_arr.transpose()\n+    assert isinstance(actual.data, Array)\n+    assert_equal(actual, expected)\n",
  "problem_statement": "Use pytorch as backend for xarrays\nI would be interested in using pytorch as a backend for xarrays - because:\r\na)  pytorch is very similar to numpy - so the conceptual overhead is small\r\nb) [most helpful] enable having a GPU as the underlying hardware for compute - which would provide non-trivial speed up\r\nc) it would allow seamless integration with deep-learning algorithms and techniques\r\n\r\nAny thoughts on what the interest for such a feature might be ? I would be open to implementing parts of it - so any suggestions on where I could start ?\r\n\r\nThanks\r\n\r\n\n",
  "hints_text": "If pytorch implements overrides of NumPy's API via the [`__array_function__` protocol](https://www.numpy.org/neps/nep-0018-array-function-protocol.html), then this could work with minimal effort. We are already using this to support [sparse arrays](https://sparse.pydata.org/en/latest/) (this isn't an official release yet, but functionality is working in the development version).\r\n\r\nI think there has been some discussion about this, but I don't know the current status (CC @rgommers). The biggest challenge for pytorch would be defining the translation layer that implements NumPy's API.\r\n\r\nPersonally, I think the most viable way to achieve seamless integration with deep learning libraries would be to support integration with [JAX](https://github.com/google/jax), which already implements NumPy's API almost exactly. I have an [experimental pull request](https://github.com/google/jax/pull/611) adding `__array_function__` to JAX, but it still needs a bit of work to finish it up, e.g., we probably want to hide this behind a flag at first.\n> I think there has been some discussion about this, but I don't know the current status (CC @rgommers). \r\n\r\nThe PyTorch team is definitely receptive to the idea of adding `__array_function__` and `__array_ufunc__`, as well as expanding the API for better NumPy compatibility.\r\n\r\nAlso, they want a `Tensor.__torch_function__` styled after `__array_function__` so they can make their own API overridable.\r\n\r\nThe tracking issue for all of this is https://github.com/pytorch/pytorch/issues/22402\r\n\r\n> The biggest challenge for pytorch would be defining the translation layer that implements NumPy's API.\r\n\r\nAgreed. No one is working on `__array_function__` at the moment. Implementing it has some backwards compat concerns as well, because people may be relying on `np.somefunc(some_torch_tensor)` to be coerced to `ndarray`. It's not a small project, but implementing a prototype with a few function in the `torch` namespace that are not exactly matching the NumPy API would be a useful way to start pushing this forward.\n> Personally, I think the most viable way to achieve seamless integration with deep learning libraries would be to support integration with JAX, which already implements NumPy's API almost exactly.\r\n\r\nLess familiar with that, but pytorch does have experimental XLA support, so that's a start. \n> Implementing it has some backwards compat concerns as well, because people may be relying on `np.somefunc(some_torch_tensor)` to be coerced to `ndarray`.\r\n\r\nYes, this is a concern for JAX as well. This is a definite downside of reusing NumPy's existing namespace.\r\n\r\nIt turns out even xarray was relying on this behavior with dask in at least one edge case: https://github.com/pydata/xarray/issues/3215\n> This is a definite downside of reusing NumPy's existing namespace.\r\n\r\nWe didn't discuss an alternative very explicitly I think, but at least we'll have wide adoption fast. Hopefully the pain is limited ....\nI haven't used JAX - but was just browsing through its documentation and it looks super cool. Any ideas on how it compares with Pytorch in terms of:\r\n\r\na) Cxecution speed, esp. on GPU\r\nb) Memory management on GPUs. Pytorch has the 'Dataloader/Dataset' paradigm which uses background multithreading to shuttle batches of data back and forth - along with a lot of tips and tricks on efficient memory usage.\r\nc) support for deep-learning optimization algorithms ?\r\n\r\n\r\n\nWithin a `jit` compiled function, JAX's execution speed should be quite competitive on GPUs. It uses the XLA compiler, which was recently enabled by default in TensorFlow.\r\n\r\nFor data loading and deep learning algorithms, take a look at the examples in the `notebooks` directory in the JAX repo. The APIs for deep learning in JAX are still undergoing rapid development, so APIs are not quite as stable/usable as pytorch or keras yet, but they are quite capable. See `jax.experimental.stax` and [`tensor2tensor.trax`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/trax) for examples.\nWhile it is pretty straightforward to implement a lot of standard xarray operations with a pytorch / Jax backend (since they just fallback on native functions) - it will be interesting to think about how to implement rolling operations / expanding / exponential window in a way that is both efficient and maintains differentiability.\r\n\r\nExpanding and exponential window operations would be easy to do leveraging RNN semantics - but doing rolling using convolutions is going to be very inefficient.\r\n\r\nDo you have any thoughts on this? \r\n\nI have not thought too much about these yet. But I agree that they will\nprobably require backend specific logic to do efficiently.\n\nOn Fri, Aug 23, 2019 at 12:13 PM firdaus janoos <notifications@github.com>\nwrote:\n\n> While it is pretty straightforward to implement a lot of standard xarray\n> operations with a pytorch / Jax backend (since they just fallback on native\n> functions) - it will be interesting to think about how to implement rolling\n> operations / expanding / exponential window in a way that is both efficient\n> and maintains differentiability.\n>\n> Expanding and exponential window operations would be easy to do leveraging\n> RNN semantics - but doing rolling using convolutions is going to be very\n> inefficient.\n>\n> Do you have any thoughts on this?\n>\n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pydata/xarray/issues/3232?email_source=notifications&email_token=AAJJFVWRVLTFNT3DYOZIJB3QGASFBA5CNFSM4ING6FH2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5A6IWY#issuecomment-524411995>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAJJFVQ7JBUNO3CAIFGVJ63QGASFBANCNFSM4ING6FHQ>\n> .\n>\n\nThis might be a good time to revive this thread and see if there is wider interest (and bandwidth) in having xarray use CuPy (https://cupy.chainer.org/ ) as a backend (along with numpy). It appears to be a plug-and-play replacement for numpy - so it might not have all the issues that were brought up regarding pytorch/jax ?\r\n\r\nAny thoughts ?\r\ncc @mrocklin \nJust chiming in quickly. I think there's definitely interest in doing this through NEP-18.\r\n\r\nIt looks like CUDA has implemented `__array_function__` (https://docs-cupy.chainer.org/en/stable/reference/interoperability.html) so many things may \"just work\". There was some work earlier on plugging in `pydata/sparse`, and there is some ongoing work to plug in `pint`. With both these efforts, a lot of xarray's code should be \"backend-agnostic\" but its not perfect. \r\n\r\nHave you tried creating `DataArrays` with `cupy` arrays yet? I would just try things and see what works vs what doesn't.\r\n\r\nPractically, our approach so far has been to add a number of xfailed tests (`test_sparse.py` and `test_units.py`) and slowly start fixing them. So that's one way to proceed if you're up for it.\n@jacobtomlinson gave CuPy a go a few months back. I seem to remember that he ran into a few problems but it would be good to get those documented here. \nYeah Jacob and I played with this a few months back. There were some issues, but my recollection is pretty hazy. If someone gives this another try, it would be interesting to hear how things go.\nIf you have any pointers on how to go about this - I can give it a try.\n\n>\n>\n\nWell here's [a blogpost on using Dask + CuPy]( https://blog.dask.org/2019/03/18/dask-nep18 ). Maybe start there and build up to using Xarray.\n> @jacobtomlinson gave CuPy a go a few months back. I seem to remember that he ran into a few problems but it would be good to get those documented here.\r\n\r\n\r\nI've been test driving xarray objects backed by CuPy arrays, and one issue I keep running into is that operations (such as plotting) that expect numpy arrays fail due to xarray's implicit converstion to Numpy arrays via `np.asarray()`. CuPy decided not to allow implicit conversion to NumPy arrays (see https://github.com/cupy/cupy/pull/3421). \r\n\r\nI am wondering whether there is a plan for dealing with this issue?\r\n\r\n\r\n\r\nHere's a small, reproducible example:\r\n\r\n```python\r\n\r\n[23]: ds.tmin.data.device\r\n      <CUDA Device 0>\r\n[24]: ds.isel(time=0, lev=0).tmin.plot() # Fails\r\n```\r\n\r\n\r\n\r\n<details>\r\n<summary>Traceback</summary>\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-21-69a72de2b9fd> in <module>\r\n----> 1 ds.isel(time=0, lev=0).tmin.plot()\r\n\r\n/glade/work/abanihi/softwares/miniconda3/envs/rapids/lib/python3.7/site-packages/xarray/plot/plot.py in __call__(self, **kwargs)\r\n    444 \r\n    445     def __call__(self, **kwargs):\r\n--> 446         return plot(self._da, **kwargs)\r\n    447 \r\n    448     @functools.wraps(hist)\r\n\r\n/glade/work/abanihi/softwares/miniconda3/envs/rapids/lib/python3.7/site-packages/xarray/plot/plot.py in plot(darray, row, col, col_wrap, ax, hue, rtol, subplot_kws, **kwargs)\r\n    198     kwargs[\"ax\"] = ax\r\n    199 \r\n--> 200     return plotfunc(darray, **kwargs)\r\n    201 \r\n    202 \r\n\r\n/glade/work/abanihi/softwares/miniconda3/envs/rapids/lib/python3.7/site-packages/xarray/plot/plot.py in newplotfunc(darray, x, y, figsize, size, aspect, ax, row, col, col_wrap, xincrease, yincrease, add_colorbar, add_labels, vmin, vmax, cmap, center, robust, extend, levels, infer_intervals, colors, subplot_kws, cbar_ax, cbar_kwargs, xscale, yscale, xticks, yticks, xlim, ylim, norm, **kwargs)\r\n    684 \r\n    685         # Pass the data as a masked ndarray too\r\n--> 686         zval = darray.to_masked_array(copy=False)\r\n    687 \r\n    688         # Replace pd.Intervals if contained in xval or yval.\r\n\r\n/glade/work/abanihi/softwares/miniconda3/envs/rapids/lib/python3.7/site-packages/xarray/core/dataarray.py in to_masked_array(self, copy)\r\n   2325             Masked where invalid values (nan or inf) occur.\r\n   2326         \"\"\"\r\n-> 2327         values = self.values  # only compute lazy arrays once\r\n   2328         isnull = pd.isnull(values)\r\n   2329         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)\r\n\r\n/glade/work/abanihi/softwares/miniconda3/envs/rapids/lib/python3.7/site-packages/xarray/core/dataarray.py in values(self)\r\n    556     def values(self) -> np.ndarray:\r\n    557         \"\"\"The array's data as a numpy.ndarray\"\"\"\r\n--> 558         return self.variable.values\r\n    559 \r\n    560     @values.setter\r\n\r\n/glade/work/abanihi/softwares/miniconda3/envs/rapids/lib/python3.7/site-packages/xarray/core/variable.py in values(self)\r\n    444     def values(self):\r\n    445         \"\"\"The variable's data as a numpy.ndarray\"\"\"\r\n--> 446         return _as_array_or_item(self._data)\r\n    447 \r\n    448     @values.setter\r\n\r\n/glade/work/abanihi/softwares/miniconda3/envs/rapids/lib/python3.7/site-packages/xarray/core/variable.py in _as_array_or_item(data)\r\n    247     TODO: remove this (replace with np.asarray) once these issues are fixed\r\n    248     \"\"\"\r\n--> 249     data = np.asarray(data)\r\n    250     if data.ndim == 0:\r\n    251         if data.dtype.kind == \"M\":\r\n\r\n/glade/work/abanihi/softwares/miniconda3/envs/rapids/lib/python3.7/site-packages/numpy/core/_asarray.py in asarray(a, dtype, order)\r\n     83 \r\n     84     \"\"\"\r\n---> 85     return array(a, dtype, copy=False, order=order)\r\n     86 \r\n     87 \r\n\r\nValueError: object __array__ method not producing an array\r\n```\r\n</details>\n@andersy005 I'm about to start working actively on `cupy` support in xarray. Would be great to get some of your input.\r\n\r\nCupy requests that instead of calling `__array__` you instead call their `.get` method for explicit conversion to numpy. So we need to add a little compatibility code for this.\n> @andersy005 I'm about to start working actively on `cupy` support in xarray. Would be great to get some of your input.\r\n> \r\n> Cupy requests that instead of calling `__array__` you instead call their `.get` method for explicit conversion to numpy. So we need to add a little compatibility code for this.\r\n\r\nDo you have a sense of the overhead / effort of making  jax vs cupy as the gpu backend for xarrays ? One advantage of jax would be built in auto-diff functionality that would enable xarray to be plugged directly into deep learning pipelines. Downside is that it is not as numpy compatible as cupy. How much of a non-starter would this be ?\n@fjanoos I'm afraid I don't. In [RAPIDS](https://rapids.ai/) we support cupy as our GPU array implementation. So this request has come from the desire to make xarray compatible with the RAPIDS suite of tools.\r\n\r\nWe commonly see folks using cupy to switch straight over to a tool like pytorch using DLPack. https://docs-cupy.chainer.org/en/stable/reference/interoperability.html#dlpack\r\n\r\nBut I don't really see #4212 as an effort to make cupy the GPU backend for xarray. I see it as adding support for another backend to xarray. The more the merrier!\nI'd like to cast my vote in favor of getting this functionality in. It would be nice to autodiff through xarray operations.\r\n\r\nFrom reading this and related threads, I'm trying to determine a gameplan to make this happen. I'm not familiar with xarray code, so any guidance would be much appreciated. This is what I'm thinking :\r\n\r\n1) Create a custom subclass of PyTorch's Tensors which meets the [duck array](http://xarray.pydata.org/en/latest/internals.html) required methods and attributes. Since this isn't officially supported, looks like I could run into issues getting this subclass to persist through tensor operations.\r\n2) Implement the [\\_\\_array_function\\_\\_ protocol](https://blog.christianperone.com/2019/07/numpy-dispatcher-when-numpy-becomes-a-protocol-for-an-ecosystem/) for PyTorch similar to how is demo-ed [here](https://blog.christianperone.com/2019/07/numpy-dispatcher-when-numpy-becomes-a-protocol-for-an-ecosystem/).\r\n3) Pass this custom class into data array constructors and hope the `.grad` attribute works.\r\n\r\nMy first attempts at this haven't been successful. Whatever custom class I make and past to the `DataArray` constructor gets converted to something xarray can handle with this line :\r\n\r\nhttps://github.com/pydata/xarray/blob/bc35548d96caaec225be9a26afbbaa94069c9494/xarray/core/dataarray.py#L408\r\n\r\nAny suggestions would be appreciated. I'm hoping to figure out the shortest path to a working prototype.\n> No one is working on __array_function__ at the moment. Implementing it has some backwards compat concerns as well, because people may be relying on np.somefunc(some_torch_tensor) to be coerced to ndarray. It's not a small project, but implementing a prototype with a few function in the torch namespace that are not exactly matching the NumPy API would be a useful way to start pushing this forward.\r\n\r\n@rgommers Do you expect this solution to work with a PyTorch Tensor custom subclass? Or is monkey patching necessary?\n> Create a custom subclass of PyTorch's Tensors which meets the [duck array](http://xarray.pydata.org/en/latest/internals.html) required methods and attributes. Since this isn't officially supported, looks like I could run into issues getting this subclass to persist through tensor operations.\r\n\r\nIf you use PyTorch 1.7.1 or later, then Tensor subclasses are much better preserved through pytorch functions and operations like slicing. So a custom subclass, adding the attributes and methods Xarray requires for a duck array should be feasible.\r\n\r\n> `data = as_compatible_data(data)`\r\n\r\nLooks like you need to patch that internally just a bit, probably adding pytorch to `NON_NUMPY_SUPPORTED_ARRAY_TYPES`.\r\n\r\n\r\nNote that I do not expect anymore that we'll be adding `__array_function__` to `torch.Tensor`, and certainly not any time soon. My current expectation is that the \"get the correct namespace from an array/tensor object directly\" from https://numpy.org/neps/nep-0037-array-module.html#how-to-use-get-array-module and https://data-apis.github.io/array-api/latest/ will turn out to be a much better design long-term.\r\n\r\n\nNote that your the main work in adding `__array_function__` is not the dispatch mechanism, but mapping to 100% compatible APIs. That job should have gotten a lot easier now compared to 9 months ago. PyTorch now has a completely matching `fft` module, and a ~70% complete `linalg` module in master. And functions in the main namespace have gained dtype keywords, integer-to-float promotion, and other NumPy compat changes. So it should be feasible to write your custom subclass.\n@Duane321 \r\nWhile it would be fantastic to have gpu-enabled auto-diff-able xarrays / DataArrays, an interesting development worth looking into are the named tensor in https://pytorch.org/docs/stable/named_tensor.html. This appears to be an attempt to bridge the gap from the that they are making pytorch tensors increasingly dataarray like. I would not be surprised if within the next few iterations they add indexes to the tensors closing the gap even further.\n> While it would be fantastic to have gpu-enabled auto-diff-able xarrays / DataArrays, an interesting development worth looking into are the named tensor in https://pytorch.org/docs/stable/named_tensor.html. This appears to be an attempt to bridge the gap from the that they are making pytorch tensors increasingly dataarray like. I would not be surprised if within the next few iterations they add indexes to the tensors closing the gap even further.\r\n\r\nI really hope so. I explored named_tensors at first, but the lack an index for each dimension was a non-starter. So, I'll keep an eye out.\n> Note that your the main work in adding __array_function__ is not the dispatch mechanism, but mapping to 100% compatible APIs. That job should have gotten a lot easier now compared to 9 months ago. PyTorch now has a completely matching fft module, and a ~70% complete linalg module in master. And functions in the main namespace have gained dtype keywords, integer-to-float promotion, and other NumPy compat changes. So it should be feasible to write your custom subclass.\r\n\r\nGlad to hear there's progress I can lean on. I'll come back with a minimum version that does the API matching for maybe 1-2 methods, just to get feedback on theoverall structure. If it works, I can brute through a lot of the rest 🤞 \r\n\r\n> Looks like you need to patch that internally just a bit, probably adding pytorch to NON_NUMPY_SUPPORTED_ARRAY_TYPES.\r\n\r\nThank you, I hesitate to change xarray code but not anymore.\r\n\r\n> Note that I do not expect anymore that we'll be adding __array_function__ to torch.Tensor, and certainly not any time soon. My current expectation is that the \"get the correct namespace from an array/tensor object directly\" from https://numpy.org/neps/nep-0037-array-module.html#how-to-use-get-array-module and https://data-apis.github.io/array-api/latest/ will turn out to be a much better design long-term.\r\n\r\nDoes this mean I shouldn't fill out `__array_function__` in my subclass? Or is this just a forward looking expectation?\r\n\n> Looks like you need to patch that internally just a bit, probably adding pytorch to NON_NUMPY_SUPPORTED_ARRAY_TYPES.\r\n\r\ndefining `__array_function__` (and the other properties listed in the [docs](https://xarray.pydata.org/en/latest/internals.html)) should be enough: https://github.com/pydata/xarray/blob/a0c71c1508f34345ad7eef244cdbbe224e031c1b/xarray/core/variable.py#L232-L235\r\n\n> Does this mean I shouldn't fill out `__array_function__` in my subclass? Or is this just a forward looking expectation?\r\n\r\nNo, adding it should be perfectly fine. The dispatch mechanism itself isn't going anywhere, it's part of numpy and it works. Whether or not `torch.Tensor` itself has an `__array_function__` method isn't too relevant for your subclass.\nI've made some mild progress, but it raises a few questions. I've defined this simple Tensor subclass which meets the duck array criteria:\r\n\r\n```\r\nclass XArrayTensor(torch.Tensor):\r\n    def __new__(cls, data=None, requires_grad=False):\r\n        if data is None:\r\n            data = torch.Tensor()\r\n        return torch.Tensor._make_subclass(cls, data, requires_grad)\r\n\r\n    def __init__(self, data=None, dims: Tuple[str] = None):\r\n        self.dims = dims\r\n\r\n    def __array_function__(self, func, types, args, kwargs):\r\n        if func not in IMPLEMENTED_FUNCTIONS or not (not all(issubclass(t, torch.Tensor) for t in types)):\r\n            return NotImplemented\r\n        return IMPLEMENTED_FUNCTIONS[func](*args, **kwargs)\r\n\r\n    def __array_ufunc__(self, func, types, args, kwargs):\r\n        if func not in IMPLEMENTED_FUNCTIONS or not (not all(issubclass(t, torch.Tensor) for t in types)):\r\n            return NotImplementedError\r\n        return IMPLEMENTED_FUNCTIONS[func](*args, **kwargs)\r\n```\r\n\r\nwhere `IMPLEMENTED_FUNCTIONS` holds a mapping from numpy functions to API compatible tensor operators (similar in style to [this](https://blog.christianperone.com/2019/07/numpy-dispatcher-when-numpy-becomes-a-protocol-for-an-ecosystem/))\r\n\r\nI added a `torch_array_type` to `pycompat.py`, which allows DataArray's `.data` attribute to persist as an `XArrayTensor`:\r\n\r\n```\r\nxr_tsr = XArrayTensor(torch.rand(3, 2))\r\n\r\ndata_array = xr.DataArray(\r\n    xr_tsr,\r\n    coords=dict(a=[\"a1\", \"a2\", \"a3\"], b=[\"b1\", \"b1\"]),\r\n    dims=[\"a\", \"b\"],\r\n    name=\"dummy\",\r\n    attrs={\"grad\": xr_tsr.grad},\r\n)\r\nprint(type(data_array.data)) --> yields 'xarray_tensor.XArrayTensor'\r\n```\r\n\r\nThe issue I'm running into is when I run an operation like `np.mean(data_array).` The operation gets dispatched to functions within `duck_array_ops.py`, which are the things I'd like to override. \r\n\r\nAlso, I'd like to confirm something. If the API matching were complete, would the following be possible?\r\n\r\n```\r\nsome_sum = data_array.sum()\r\nsome_sum.backward()\r\ndata_array.grad --> provides the gradient\r\n```\r\n\r\nI'm starting to suspect not because that would involve data_array being _both_ `DataArray` and a `Torch.Tensor` object. It seems what I'm in fact enabling is that `DataArray.data` is a `Torch.Tensor`.\r\n\r\n \n> I'm starting to suspect not because that would involve data_array being _both_ `DataArray` and a `Torch.Tensor` object. It seems what I'm in fact enabling is that `DataArray.data` is a `Torch.Tensor`.\r\n\r\n`some_sum` is still a `DataArray`, which doesn't have a `backward` method. You could use\r\n```\r\ndata_array = xr.DataArray(\r\n    xr_tsr,\r\n    coords=dict(a=[\"a1\", \"a2\", \"a3\"], b=[\"b1\", \"b1\"]),\r\n    dims=[\"a\", \"b\"],\r\n    name=\"dummy\",\r\n    attrs={\"grad\": xr_tsr.grad, \"backward\": xr_tsr.backward},\r\n)\r\n```\r\nand your example should work (I assume you meant `.grad` not `.grid`).\n> I added a `torch_array_type` to `pycompat.py`\r\n\r\n`torch.Tensor` defines `values`, so the issue is this: https://github.com/pydata/xarray/blob/8cc34cb412ba89ebca12fc84f76a9e452628f1bc/xarray/core/variable.py#L221\r\n@shoyer, any ideas?\r\n\r\nFor now, I guess we can remove it using `__getattribute__`. With that you will have to cast the data first if you want to access `torch.Tensor.values`:\r\n```python\r\ntorch.Tensor(tensor).values()\r\n```\r\n\r\nNot sure if that's the best way, but that would look like this:\r\n\r\n<details><summary><tt>pytorch</tt> wrapper class</summary>\r\n\r\n```python\r\nIn [13]: import numpy as np\r\n    ...: import torch\r\n    ...: from typing import Tuple\r\n    ...: import xarray as xr\r\n    ...: import functools\r\n    ...: \r\n    ...: def wrap_torch(f):\r\n    ...:     @functools.wraps(f)\r\n    ...:     def wrapper(*args, **kwargs):\r\n    ...:         # TODO: use a dict comprehension if there are functions that rely on the order of the parameters\r\n    ...:         if \"axis\" in kwargs:\r\n    ...:             kwargs[\"dim\"] = kwargs.pop(\"axis\")  # torch calls that parameter 'dim' instead of 'axis'\r\n    ...: \r\n    ...:         return f(*args, **kwargs)\r\n    ...: \r\n    ...:     return wrapper\r\n    ...: \r\n    ...: class DTypeWrapper:\r\n    ...:     def __init__(self, dtype):\r\n    ...:         self.dtype = dtype\r\n    ...:         if dtype.is_complex:\r\n    ...:             self.kind = \"c\"\r\n    ...:         elif dtype.is_floating_point:\r\n    ...:             self.kind = \"f\"\r\n    ...:         else:\r\n    ...:             # I don't know pytorch at all, so falling back to \"i\" might not be the best choice\r\n    ...:             self.kind = \"i\"\r\n    ...: \r\n    ...:     def __getattr__(self, name):\r\n    ...:         return getattr(self.dtype, name)\r\n    ...: \r\n    ...:     def __repr__(self):\r\n    ...:         return repr(self.dtype)\r\n    ...: \r\n    ...: IMPLEMENTED_FUNCTIONS = {\r\n    ...:     np.mean: wrap_torch(torch.mean),\r\n    ...:     np.nanmean: wrap_torch(torch.mean),  # not sure if pytorch has a separate nanmean function\r\n    ...: }\r\n    ...: \r\n    ...: class XArrayTensor(torch.Tensor):\r\n    ...:     def __new__(cls, data=None, requires_grad=False):\r\n    ...:         if data is None:\r\n    ...:             data = torch.Tensor()\r\n    ...:         return torch.Tensor._make_subclass(cls, data, requires_grad)\r\n    ...: \r\n    ...:     def __init__(self, data=None, dims: Tuple[str] = None):\r\n    ...:         self.dims = dims\r\n    ...: \r\n    ...:     def __array_function__(self, func, types, args, kwargs):\r\n    ...:         if func not in IMPLEMENTED_FUNCTIONS or any(not issubclass(t, torch.Tensor) for t in types):\r\n    ...:             return NotImplemented\r\n    ...:         return IMPLEMENTED_FUNCTIONS[func](*args, **kwargs)\r\n    ...: \r\n    ...:     def __array_ufunc__(self, func, types, args, kwargs):\r\n    ...:         if func not in IMPLEMENTED_FUNCTIONS or any(not issubclass(t, torch.Tensor) for t in types):\r\n    ...:             return NotImplementedError\r\n    ...:         return IMPLEMENTED_FUNCTIONS[func](*args, **kwargs)\r\n    ...: \r\n    ...:     def __getattribute__(self, name):\r\n    ...:         if name == \"values\":\r\n    ...:             raise AttributeError(\r\n    ...:                 \"'values' has been removed for compatibility with xarray.\"\r\n    ...:                 \" To access it, use `torch.Tensor(tensor).values()`.\"\r\n    ...:             )\r\n    ...:         return object.__getattribute__(self, name)\r\n    ...: \r\n    ...:     @property\r\n    ...:     def shape(self):\r\n    ...:         return tuple(super().shape)\r\n    ...: \r\n    ...:     @property\r\n    ...:     def dtype(self):\r\n    ...:         return DTypeWrapper(super().dtype)\r\n    ...: \r\n    ...: tensor = XArrayTensor(torch.rand(3, 2))\r\n    ...: display(tensor)\r\n    ...: display(tensor.shape)\r\n    ...: display(tensor.dtype)\r\n    ...: display(tensor.ndim)\r\n    ...: \r\n    ...: da = xr.DataArray(tensor, coords={\"a\": [\"a1\", \"a2\", \"a3\"], \"b\": [\"b1\", \"b2\"]}, dims=[\"a\", \"b\"])\r\n    ...: display(da)\r\n    ...: display(da.data)\r\n    ...: display(da.mean(dim=\"a\"))\r\n```\r\n\r\n</details>\r\n\r\nwith that, I can execute `mean` and get back a `torch.Tensor` wrapped by a `DataArray` without modifying the `xarray` code. For a list of features where duck arrays are not supported, yet, see [Working with numpy-like arrays](https://xarray.pydata.org/en/stable/duckarrays.html) (that list should be pretty complete, but if you think there's something missing please open a new issue).\r\n\r\nFor `np.mean(da)`: be aware that `DataArray` does not define `__array_function__`, yet (see #3917), and that with it you have to fall back to `np.mean(da, axis=0)` instead of `da.mean(dim=\"a\")`.\r\n\r\n> If the API matching were complete, would the following be possible?\r\n\r\nno, it won't be because this is fragile: any new method of `DataArray` could shadow the methods of the wrapped object. Also, without tight integration `xarray` does not know what to do with the result, so you would always get the underlying data instead of a new `DataArray`.\r\n\r\nInstead, we recommend extension packages ([extending xarray](https://xarray.pydata.org/en/stable/internals.html#extending-xarray)), so with a hypothetical `xarray-pytorch` library you would write `some_sum.torch.backward()` instead of `some_sum.backward()`. That is a bit more work, but it also gives you a lot more control. For an example, see [pint-xarray](https://github.com/xarray-contrib/pint-xarray).\nThank you very much @keewis - your code did what I was trying to do. big help!\r\n\r\nOne thing I noticed with the [missing features](https://xarray.pydata.org/en/stable/duckarrays.html) is the following : \r\n\r\n![image](https://user-images.githubusercontent.com/19956442/106342256-1771ac80-6255-11eb-8b25-4f61dbb43132.png)\r\n\r\nThis seems like a bit of a problem. Index-based selection is a primary reason to use xarray's. If that changes `.data` to a numpy array, then autodiff-ing through selection seems not possible. Is there another approach I'm not seeing?\nI can't reproduce that:\r\n```python\r\nIn [4]: da.loc[\"a1\"]\r\nOut[4]: \r\n<xarray.DataArray (b: 2)>\r\ntensor([0.4793, 0.7493], dtype=torch.float32)\r\nCoordinates:\r\n    a        <U2 'a1'\r\n  * b        (b) <U2 'b1' 'b2'\r\n```\r\nwith\r\n```\r\nnumpy: 1.19.5\r\nxarray: 0.16.2\r\npytorch: 1.7.1.post2\r\npandas: 1.2.1\r\n```\r\nmaybe this is a environment issue?\r\n\r\nEdit: the missing feature list includes `loc` (and `sel`) because it is currently not possible to have a duck array in a dimension coordinate, so this:\r\n```python\r\nxr.DataArray(\r\n    [0, 1, 2],\r\n    coords={\"x\": XArrayTensor(torch.Tensor([10, 12, 14]))},\r\n    dims=\"x\",\r\n).loc[{\"x\": XArrayTensor(torch.Tensor([10, 14]))}]\r\n```\r\ndoes not work, but\r\n```python\r\nxr.DataArray(\r\n    XArrayTensor(torch.Tensor([0, 1, 2])),\r\n    coords={\"x\": [10, 12, 14]},\r\n    dims=\"x\",\r\n).loc[{\"x\": [10, 14]}]\r\n```\r\nshould work just fine.\nThank again @keewis , that was indeed the case. It was due to my older PyTorch version (1.6.0)\n@Duane321: with `xarray>=0.17.0` you should be able to remove the `__getattributes__` trick.\n@Duane321 or @keewis do you have the full code example for making this work? I'm a novice on numpy ufuncs and am trying to use get gradients while keeping my xarray coords.\nI don't, unfortunately (there's the partial example in https://github.com/pydata/xarray/issues/3232#issuecomment-769789746, though).\r\n\r\nThis is nothing usable right now, but the `pytorch` maintainers are currently looking into providing support for `__array_namespace__` (NEP47). Once there has been sufficient progress in both [`numpy`](https://github.com/numpy/numpy/pull/18585) and [`pytorch`](https://github.com/pytorch/pytorch/issues/58743) we don't have to change much in xarray (i.e. allowing `__array_namespace__` instead of `__array_ufunc__` / `_array_function__` for duck arrays) to make this work without any wrapper code.\r\n\r\nYou (or anyone interested) might still want to maintain a \"pytorch-xarray\" convenience library to allow something like `arr.torch.grad(dim=\"x\")`.\nThanks for the prompt response. Would love to contribute but I have to climb the learning curve first.\nchanging the `xarray` internals is not too much work: we need to get `xarray.core.utils.is_duck_array` to return true if the object has either `__array_namespace__` or `__array_ufunc__` and `__array_function__` (or all three) defined, and we'd need a short test demonstrating that objects that implement only `__array_namespace__` survive unchanged when wrapped by a `xarray` object (i.e. something like `isinstance(xr.DataArray(pytorch_object).mean().data, pytorch.Tensor)`).\r\n\r\nWe might still be a bit too early with this, though: the PR which adds `__array_namespace__` to `numpy` has not been merged into `numpy:main` yet.\n@keewis @shoyer now that numpy is merged in https://github.com/numpy/numpy/pull/18585 `__array_namespace__` support and pytorch is in the process of add `__array_namespace__` support https://github.com/pytorch/pytorch/issues/58743 is it worth exploring adding support through the `__array_namespace__` API?",
  "created_at": "2022-07-18T10:04:02Z",
  "version": "2022.06",
  "FAIL_TO_PASS": "[\"xarray/tests/test_array_api.py::test_arithmetic\", \"xarray/tests/test_array_api.py::test_aggregation\", \"xarray/tests/test_array_api.py::test_indexing\", \"xarray/tests/test_array_api.py::test_reorganizing_operation\"]",
  "PASS_TO_PASS": "[]",
  "environment_setup_commit": "50ea159bfd0872635ebf4281e741f3c87f0bef6b",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.895954",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}