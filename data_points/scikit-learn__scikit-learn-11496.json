{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-11496",
  "base_commit": "cb0140017740d985960911c4f34820beea915846",
  "patch": "diff --git a/sklearn/impute.py b/sklearn/impute.py\n--- a/sklearn/impute.py\n+++ b/sklearn/impute.py\n@@ -133,7 +133,6 @@ class SimpleImputer(BaseEstimator, TransformerMixin):\n         a new copy will always be made, even if `copy=False`:\n \n         - If X is not an array of floating values;\n-        - If X is sparse and `missing_values=0`;\n         - If X is encoded as a CSR matrix.\n \n     Attributes\n@@ -227,10 +226,17 @@ def fit(self, X, y=None):\n                              \"data\".format(fill_value))\n \n         if sparse.issparse(X):\n-            self.statistics_ = self._sparse_fit(X,\n-                                                self.strategy,\n-                                                self.missing_values,\n-                                                fill_value)\n+            # missing_values = 0 not allowed with sparse data as it would\n+            # force densification\n+            if self.missing_values == 0:\n+                raise ValueError(\"Imputation not possible when missing_values \"\n+                                 \"== 0 and input is sparse. Provide a dense \"\n+                                 \"array instead.\")\n+            else:\n+                self.statistics_ = self._sparse_fit(X,\n+                                                    self.strategy,\n+                                                    self.missing_values,\n+                                                    fill_value)\n         else:\n             self.statistics_ = self._dense_fit(X,\n                                                self.strategy,\n@@ -241,80 +247,41 @@ def fit(self, X, y=None):\n \n     def _sparse_fit(self, X, strategy, missing_values, fill_value):\n         \"\"\"Fit the transformer on sparse data.\"\"\"\n-        # Count the zeros\n-        if missing_values == 0:\n-            n_zeros_axis = np.zeros(X.shape[1], dtype=int)\n-        else:\n-            n_zeros_axis = X.shape[0] - np.diff(X.indptr)\n-\n-        # Mean\n-        if strategy == \"mean\":\n-            if missing_values != 0:\n-                n_non_missing = n_zeros_axis\n-\n-                # Mask the missing elements\n-                mask_missing_values = _get_mask(X.data, missing_values)\n-                mask_valids = np.logical_not(mask_missing_values)\n-\n-                # Sum only the valid elements\n-                new_data = X.data.copy()\n-                new_data[mask_missing_values] = 0\n-                X = sparse.csc_matrix((new_data, X.indices, X.indptr),\n-                                      copy=False)\n-                sums = X.sum(axis=0)\n-\n-                # Count the elements != 0\n-                mask_non_zeros = sparse.csc_matrix(\n-                    (mask_valids.astype(np.float64),\n-                     X.indices,\n-                     X.indptr), copy=False)\n-                s = mask_non_zeros.sum(axis=0)\n-                n_non_missing = np.add(n_non_missing, s)\n+        mask_data = _get_mask(X.data, missing_values)\n+        n_implicit_zeros = X.shape[0] - np.diff(X.indptr)\n \n-            else:\n-                sums = X.sum(axis=0)\n-                n_non_missing = np.diff(X.indptr)\n+        statistics = np.empty(X.shape[1])\n \n-            # Ignore the error, columns with a np.nan statistics_\n-            # are not an error at this point. These columns will\n-            # be removed in transform\n-            with np.errstate(all=\"ignore\"):\n-                return np.ravel(sums) / np.ravel(n_non_missing)\n+        if strategy == \"constant\":\n+            # for constant strategy, self.statistcs_ is used to store\n+            # fill_value in each column\n+            statistics.fill(fill_value)\n \n-        # Median + Most frequent + Constant\n         else:\n-            # Remove the missing values, for each column\n-            columns_all = np.hsplit(X.data, X.indptr[1:-1])\n-            mask_missing_values = _get_mask(X.data, missing_values)\n-            mask_valids = np.hsplit(np.logical_not(mask_missing_values),\n-                                    X.indptr[1:-1])\n-\n-            # astype necessary for bug in numpy.hsplit before v1.9\n-            columns = [col[mask.astype(bool, copy=False)]\n-                       for col, mask in zip(columns_all, mask_valids)]\n-\n-            # Median\n-            if strategy == \"median\":\n-                median = np.empty(len(columns))\n-                for i, column in enumerate(columns):\n-                    median[i] = _get_median(column, n_zeros_axis[i])\n-\n-                return median\n-\n-            # Most frequent\n-            elif strategy == \"most_frequent\":\n-                most_frequent = np.empty(len(columns))\n-\n-                for i, column in enumerate(columns):\n-                    most_frequent[i] = _most_frequent(column,\n-                                                      0,\n-                                                      n_zeros_axis[i])\n-\n-                return most_frequent\n-\n-            # Constant\n-            elif strategy == \"constant\":\n-                return np.full(X.shape[1], fill_value)\n+            for i in range(X.shape[1]):\n+                column = X.data[X.indptr[i]:X.indptr[i+1]]\n+                mask_column = mask_data[X.indptr[i]:X.indptr[i+1]]\n+                column = column[~mask_column]\n+\n+                # combine explicit and implicit zeros\n+                mask_zeros = _get_mask(column, 0)\n+                column = column[~mask_zeros]\n+                n_explicit_zeros = mask_zeros.sum()\n+                n_zeros = n_implicit_zeros[i] + n_explicit_zeros\n+\n+                if strategy == \"mean\":\n+                    s = column.size + n_zeros\n+                    statistics[i] = np.nan if s == 0 else column.sum() / s\n+\n+                elif strategy == \"median\":\n+                    statistics[i] = _get_median(column,\n+                                                n_zeros)\n+\n+                elif strategy == \"most_frequent\":\n+                    statistics[i] = _most_frequent(column,\n+                                                   0,\n+                                                   n_zeros)\n+        return statistics\n \n     def _dense_fit(self, X, strategy, missing_values, fill_value):\n         \"\"\"Fit the transformer on dense data.\"\"\"\n@@ -364,6 +331,8 @@ def _dense_fit(self, X, strategy, missing_values, fill_value):\n \n         # Constant\n         elif strategy == \"constant\":\n+            # for constant strategy, self.statistcs_ is used to store\n+            # fill_value in each column\n             return np.full(X.shape[1], fill_value, dtype=X.dtype)\n \n     def transform(self, X):\n@@ -402,17 +371,19 @@ def transform(self, X):\n                 X = X[:, valid_statistics_indexes]\n \n         # Do actual imputation\n-        if sparse.issparse(X) and self.missing_values != 0:\n-            mask = _get_mask(X.data, self.missing_values)\n-            indexes = np.repeat(np.arange(len(X.indptr) - 1, dtype=np.int),\n-                                np.diff(X.indptr))[mask]\n+        if sparse.issparse(X):\n+            if self.missing_values == 0:\n+                raise ValueError(\"Imputation not possible when missing_values \"\n+                                 \"== 0 and input is sparse. Provide a dense \"\n+                                 \"array instead.\")\n+            else:\n+                mask = _get_mask(X.data, self.missing_values)\n+                indexes = np.repeat(np.arange(len(X.indptr) - 1, dtype=np.int),\n+                                    np.diff(X.indptr))[mask]\n \n-            X.data[mask] = valid_statistics[indexes].astype(X.dtype,\n-                                                            copy=False)\n+                X.data[mask] = valid_statistics[indexes].astype(X.dtype,\n+                                                                copy=False)\n         else:\n-            if sparse.issparse(X):\n-                X = X.toarray()\n-\n             mask = _get_mask(X, self.missing_values)\n             n_missing = np.sum(mask, axis=0)\n             values = np.repeat(valid_statistics, n_missing)\n",
  "test_patch": "diff --git a/sklearn/tests/test_impute.py b/sklearn/tests/test_impute.py\n--- a/sklearn/tests/test_impute.py\n+++ b/sklearn/tests/test_impute.py\n@@ -97,6 +97,23 @@ def test_imputation_deletion_warning(strategy):\n         imputer.fit_transform(X)\n \n \n+@pytest.mark.parametrize(\"strategy\", [\"mean\", \"median\",\n+                                      \"most_frequent\", \"constant\"])\n+def test_imputation_error_sparse_0(strategy):\n+    # check that error are raised when missing_values = 0 and input is sparse\n+    X = np.ones((3, 5))\n+    X[0] = 0\n+    X = sparse.csc_matrix(X)\n+\n+    imputer = SimpleImputer(strategy=strategy, missing_values=0)\n+    with pytest.raises(ValueError, match=\"Provide a dense array\"):\n+        imputer.fit(X)\n+\n+    imputer.fit(X.toarray())\n+    with pytest.raises(ValueError, match=\"Provide a dense array\"):\n+        imputer.transform(X)\n+\n+\n def safe_median(arr, *args, **kwargs):\n     # np.median([]) raises a TypeError for numpy >= 1.10.1\n     length = arr.size if hasattr(arr, 'size') else len(arr)\n@@ -123,10 +140,8 @@ def test_imputation_mean_median():\n     values[4::2] = - values[4::2]\n \n     tests = [(\"mean\", np.nan, lambda z, v, p: safe_mean(np.hstack((z, v)))),\n-             (\"mean\", 0, lambda z, v, p: np.mean(v)),\n              (\"median\", np.nan,\n-              lambda z, v, p: safe_median(np.hstack((z, v)))),\n-             (\"median\", 0, lambda z, v, p: np.median(v))]\n+              lambda z, v, p: safe_median(np.hstack((z, v))))]\n \n     for strategy, test_missing_values, true_value_fun in tests:\n         X = np.empty(shape)\n@@ -427,14 +442,18 @@ def test_imputation_constant_pandas(dtype):\n \n def test_imputation_pipeline_grid_search():\n     # Test imputation within a pipeline + gridsearch.\n-    pipeline = Pipeline([('imputer', SimpleImputer(missing_values=0)),\n-                         ('tree', tree.DecisionTreeRegressor(random_state=0))])\n+    X = sparse_random_matrix(100, 100, density=0.10)\n+    missing_values = X.data[0]\n+\n+    pipeline = Pipeline([('imputer',\n+                          SimpleImputer(missing_values=missing_values)),\n+                         ('tree',\n+                          tree.DecisionTreeRegressor(random_state=0))])\n \n     parameters = {\n         'imputer__strategy': [\"mean\", \"median\", \"most_frequent\"]\n     }\n \n-    X = sparse_random_matrix(100, 100, density=0.10)\n     Y = sparse_random_matrix(100, 1, density=0.10).toarray()\n     gs = GridSearchCV(pipeline, parameters)\n     gs.fit(X, Y)\n",
  "problem_statement": "BUG: SimpleImputer gives wrong result on sparse matrix with explicit zeros\nThe current implementation of the `SimpleImputer` can't deal with zeros stored explicitly in sparse matrix.\r\nEven when stored explicitly, we'd expect that all zeros are treating equally, right ?\r\nSee for example the code below:\r\n```python\r\nimport numpy as np\r\nfrom scipy import sparse\r\nfrom sklearn.impute import SimpleImputer\r\n\r\nX = np.array([[0,0,0],[0,0,0],[1,1,1]])\r\nX = sparse.csc_matrix(X)\r\nX[0] = 0    # explicit zeros in first row\r\n\r\nimp = SimpleImputer(missing_values=0, strategy='mean')\r\nimp.fit_transform(X)\r\n\r\n>>> array([[0.5, 0.5, 0.5],\r\n           [0.5, 0.5, 0.5],\r\n           [1. , 1. , 1. ]])\r\n```\r\nWhereas the expected result would be\r\n```python\r\n>>> array([[1. , 1. , 1. ],\r\n           [1. , 1. , 1. ],\r\n           [1. , 1. , 1. ]])\r\n```\n",
  "hints_text": "",
  "created_at": "2018-07-12T17:05:58Z",
  "version": "0.20",
  "FAIL_TO_PASS": "[\"sklearn/tests/test_impute.py::test_imputation_error_sparse_0[mean]\", \"sklearn/tests/test_impute.py::test_imputation_error_sparse_0[median]\", \"sklearn/tests/test_impute.py::test_imputation_error_sparse_0[most_frequent]\", \"sklearn/tests/test_impute.py::test_imputation_error_sparse_0[constant]\"]",
  "PASS_TO_PASS": "[\"sklearn/tests/test_impute.py::test_imputation_shape\", \"sklearn/tests/test_impute.py::test_imputation_error_invalid_strategy[const]\", \"sklearn/tests/test_impute.py::test_imputation_error_invalid_strategy[101]\", \"sklearn/tests/test_impute.py::test_imputation_error_invalid_strategy[None]\", \"sklearn/tests/test_impute.py::test_imputation_deletion_warning[mean]\", \"sklearn/tests/test_impute.py::test_imputation_deletion_warning[median]\", \"sklearn/tests/test_impute.py::test_imputation_deletion_warning[most_frequent]\", \"sklearn/tests/test_impute.py::test_imputation_mean_median\", \"sklearn/tests/test_impute.py::test_imputation_median_special_cases\", \"sklearn/tests/test_impute.py::test_imputation_mean_median_error_invalid_type[None-mean]\", \"sklearn/tests/test_impute.py::test_imputation_mean_median_error_invalid_type[None-median]\", \"sklearn/tests/test_impute.py::test_imputation_mean_median_error_invalid_type[object-mean]\", \"sklearn/tests/test_impute.py::test_imputation_mean_median_error_invalid_type[object-median]\", \"sklearn/tests/test_impute.py::test_imputation_mean_median_error_invalid_type[str-mean]\", \"sklearn/tests/test_impute.py::test_imputation_mean_median_error_invalid_type[str-median]\", \"sklearn/tests/test_impute.py::test_imputation_const_mostf_error_invalid_types[str-constant]\", \"sklearn/tests/test_impute.py::test_imputation_const_mostf_error_invalid_types[str-most_frequent]\", \"sklearn/tests/test_impute.py::test_imputation_const_mostf_error_invalid_types[dtype1-constant]\", \"sklearn/tests/test_impute.py::test_imputation_const_mostf_error_invalid_types[dtype1-most_frequent]\", \"sklearn/tests/test_impute.py::test_imputation_const_mostf_error_invalid_types[dtype2-constant]\", \"sklearn/tests/test_impute.py::test_imputation_const_mostf_error_invalid_types[dtype2-most_frequent]\", \"sklearn/tests/test_impute.py::test_imputation_most_frequent\", \"sklearn/tests/test_impute.py::test_imputation_most_frequent_objects[None]\", \"sklearn/tests/test_impute.py::test_imputation_most_frequent_objects[nan]\", \"sklearn/tests/test_impute.py::test_imputation_most_frequent_objects[NAN]\", \"sklearn/tests/test_impute.py::test_imputation_most_frequent_objects[]\", \"sklearn/tests/test_impute.py::test_imputation_most_frequent_objects[0]\", \"sklearn/tests/test_impute.py::test_imputation_most_frequent_pandas[object]\", \"sklearn/tests/test_impute.py::test_imputation_most_frequent_pandas[category]\", \"sklearn/tests/test_impute.py::test_imputation_constant_error_invalid_type[1-0]\", \"sklearn/tests/test_impute.py::test_imputation_constant_error_invalid_type[1.0-nan]\", \"sklearn/tests/test_impute.py::test_imputation_constant_integer\", \"sklearn/tests/test_impute.py::test_imputation_constant_float[csr_matrix]\", \"sklearn/tests/test_impute.py::test_imputation_constant_float[asarray]\", \"sklearn/tests/test_impute.py::test_imputation_constant_object[None]\", \"sklearn/tests/test_impute.py::test_imputation_constant_object[nan]\", \"sklearn/tests/test_impute.py::test_imputation_constant_object[NAN]\", \"sklearn/tests/test_impute.py::test_imputation_constant_object[]\", \"sklearn/tests/test_impute.py::test_imputation_constant_object[0]\", \"sklearn/tests/test_impute.py::test_imputation_constant_pandas[object]\", \"sklearn/tests/test_impute.py::test_imputation_constant_pandas[category]\", \"sklearn/tests/test_impute.py::test_imputation_pipeline_grid_search\", \"sklearn/tests/test_impute.py::test_imputation_copy\", \"sklearn/tests/test_impute.py::test_chained_imputer_rank_one\", \"sklearn/tests/test_impute.py::test_chained_imputer_imputation_order[random]\", \"sklearn/tests/test_impute.py::test_chained_imputer_imputation_order[roman]\", \"sklearn/tests/test_impute.py::test_chained_imputer_imputation_order[ascending]\", \"sklearn/tests/test_impute.py::test_chained_imputer_imputation_order[descending]\", \"sklearn/tests/test_impute.py::test_chained_imputer_imputation_order[arabic]\", \"sklearn/tests/test_impute.py::test_chained_imputer_predictors[predictor0]\", \"sklearn/tests/test_impute.py::test_chained_imputer_predictors[predictor1]\", \"sklearn/tests/test_impute.py::test_chained_imputer_predictors[predictor2]\", \"sklearn/tests/test_impute.py::test_chained_imputer_clip\", \"sklearn/tests/test_impute.py::test_chained_imputer_missing_at_transform[mean]\", \"sklearn/tests/test_impute.py::test_chained_imputer_missing_at_transform[median]\", \"sklearn/tests/test_impute.py::test_chained_imputer_missing_at_transform[most_frequent]\", \"sklearn/tests/test_impute.py::test_chained_imputer_transform_stochasticity\", \"sklearn/tests/test_impute.py::test_chained_imputer_no_missing\", \"sklearn/tests/test_impute.py::test_chained_imputer_transform_recovery[3]\", \"sklearn/tests/test_impute.py::test_chained_imputer_transform_recovery[5]\", \"sklearn/tests/test_impute.py::test_chained_imputer_additive_matrix\"]",
  "environment_setup_commit": "55bf5d93e5674f13a1134d93a11fd0cd11aabcd1",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.962405",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}