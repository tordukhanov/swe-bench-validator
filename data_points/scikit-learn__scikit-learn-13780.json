{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-13780",
  "base_commit": "8d3b4ff3eec890396a3d7a806bbe944f55a89cb4",
  "patch": "diff --git a/sklearn/ensemble/voting.py b/sklearn/ensemble/voting.py\n--- a/sklearn/ensemble/voting.py\n+++ b/sklearn/ensemble/voting.py\n@@ -30,7 +30,15 @@\n def _parallel_fit_estimator(estimator, X, y, sample_weight=None):\n     \"\"\"Private function used to fit an estimator within a job.\"\"\"\n     if sample_weight is not None:\n-        estimator.fit(X, y, sample_weight=sample_weight)\n+        try:\n+            estimator.fit(X, y, sample_weight=sample_weight)\n+        except TypeError as exc:\n+            if \"unexpected keyword argument 'sample_weight'\" in str(exc):\n+                raise ValueError(\n+                    \"Underlying estimator {} does not support sample weights.\"\n+                    .format(estimator.__class__.__name__)\n+                ) from exc\n+            raise\n     else:\n         estimator.fit(X, y)\n     return estimator\n@@ -53,8 +61,8 @@ def _weights_not_none(self):\n         \"\"\"Get the weights of not `None` estimators\"\"\"\n         if self.weights is None:\n             return None\n-        return [w for est, w in zip(self.estimators,\n-                                    self.weights) if est[1] is not None]\n+        return [w for est, w in zip(self.estimators, self.weights)\n+                if est[1] not in (None, 'drop')]\n \n     def _predict(self, X):\n         \"\"\"Collect results from clf.predict calls. \"\"\"\n@@ -76,26 +84,22 @@ def fit(self, X, y, sample_weight=None):\n                              '; got %d weights, %d estimators'\n                              % (len(self.weights), len(self.estimators)))\n \n-        if sample_weight is not None:\n-            for name, step in self.estimators:\n-                if step is None:\n-                    continue\n-                if not has_fit_parameter(step, 'sample_weight'):\n-                    raise ValueError('Underlying estimator \\'%s\\' does not'\n-                                     ' support sample weights.' % name)\n-\n         names, clfs = zip(*self.estimators)\n         self._validate_names(names)\n \n-        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n+        n_isnone = np.sum(\n+            [clf in (None, 'drop') for _, clf in self.estimators]\n+        )\n         if n_isnone == len(self.estimators):\n-            raise ValueError('All estimators are None. At least one is '\n-                             'required!')\n+            raise ValueError(\n+                'All estimators are None or \"drop\". At least one is required!'\n+            )\n \n         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n                 delayed(_parallel_fit_estimator)(clone(clf), X, y,\n                                                  sample_weight=sample_weight)\n-                for clf in clfs if clf is not None)\n+                for clf in clfs if clf not in (None, 'drop')\n+            )\n \n         self.named_estimators_ = Bunch()\n         for k, e in zip(self.estimators, self.estimators_):\n@@ -149,8 +153,8 @@ class VotingClassifier(_BaseVoting, ClassifierMixin):\n     estimators : list of (string, estimator) tuples\n         Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n         of those original estimators that will be stored in the class attribute\n-        ``self.estimators_``. An estimator can be set to `None` using\n-        ``set_params``.\n+        ``self.estimators_``. An estimator can be set to ``None`` or ``'drop'``\n+        using ``set_params``.\n \n     voting : str, {'hard', 'soft'} (default='hard')\n         If 'hard', uses predicted class labels for majority rule voting.\n@@ -381,9 +385,9 @@ class VotingRegressor(_BaseVoting, RegressorMixin):\n     Parameters\n     ----------\n     estimators : list of (string, estimator) tuples\n-        Invoking the ``fit`` method on the ``VotingRegressor`` will fit\n-        clones of those original estimators that will be stored in the class\n-        attribute ``self.estimators_``. An estimator can be set to `None`\n+        Invoking the ``fit`` method on the ``VotingRegressor`` will fit clones\n+        of those original estimators that will be stored in the class attribute\n+        ``self.estimators_``. An estimator can be set to ``None`` or ``'drop'``\n         using ``set_params``.\n \n     weights : array-like, shape (n_regressors,), optional (default=`None`)\n",
  "test_patch": "diff --git a/sklearn/ensemble/tests/test_voting.py b/sklearn/ensemble/tests/test_voting.py\n--- a/sklearn/ensemble/tests/test_voting.py\n+++ b/sklearn/ensemble/tests/test_voting.py\n@@ -342,12 +342,25 @@ def test_sample_weight():\n     assert_array_equal(eclf3.predict(X), clf1.predict(X))\n     assert_array_almost_equal(eclf3.predict_proba(X), clf1.predict_proba(X))\n \n+    # check that an error is raised and indicative if sample_weight is not\n+    # supported.\n     clf4 = KNeighborsClassifier()\n     eclf3 = VotingClassifier(estimators=[\n         ('lr', clf1), ('svc', clf3), ('knn', clf4)],\n         voting='soft')\n-    msg = ('Underlying estimator \\'knn\\' does not support sample weights.')\n-    assert_raise_message(ValueError, msg, eclf3.fit, X, y, sample_weight)\n+    msg = ('Underlying estimator KNeighborsClassifier does not support '\n+           'sample weights.')\n+    with pytest.raises(ValueError, match=msg):\n+        eclf3.fit(X, y, sample_weight)\n+\n+    # check that _parallel_fit_estimator will raise the right error\n+    # it should raise the original error if this is not linked to sample_weight\n+    class ClassifierErrorFit(BaseEstimator, ClassifierMixin):\n+        def fit(self, X, y, sample_weight):\n+            raise TypeError('Error unrelated to sample_weight.')\n+    clf = ClassifierErrorFit()\n+    with pytest.raises(TypeError, match='Error unrelated to sample_weight'):\n+        clf.fit(X, y, sample_weight=sample_weight)\n \n \n def test_sample_weight_kwargs():\n@@ -404,8 +417,10 @@ def test_set_params():\n @pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22\n @pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22\n @pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n-def test_set_estimator_none():\n-    \"\"\"VotingClassifier set_params should be able to set estimators as None\"\"\"\n+@pytest.mark.parametrize(\"drop\", [None, 'drop'])\n+def test_set_estimator_none(drop):\n+    \"\"\"VotingClassifier set_params should be able to set estimators as None or\n+    drop\"\"\"\n     # Test predict\n     clf1 = LogisticRegression(random_state=123)\n     clf2 = RandomForestClassifier(random_state=123)\n@@ -417,22 +432,22 @@ def test_set_estimator_none():\n     eclf2 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2),\n                                          ('nb', clf3)],\n                              voting='hard', weights=[1, 1, 0.5])\n-    eclf2.set_params(rf=None).fit(X, y)\n+    eclf2.set_params(rf=drop).fit(X, y)\n     assert_array_equal(eclf1.predict(X), eclf2.predict(X))\n \n-    assert dict(eclf2.estimators)[\"rf\"] is None\n+    assert dict(eclf2.estimators)[\"rf\"] is drop\n     assert len(eclf2.estimators_) == 2\n     assert all(isinstance(est, (LogisticRegression, GaussianNB))\n                for est in eclf2.estimators_)\n-    assert eclf2.get_params()[\"rf\"] is None\n+    assert eclf2.get_params()[\"rf\"] is drop\n \n     eclf1.set_params(voting='soft').fit(X, y)\n     eclf2.set_params(voting='soft').fit(X, y)\n     assert_array_equal(eclf1.predict(X), eclf2.predict(X))\n     assert_array_almost_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))\n-    msg = 'All estimators are None. At least one is required!'\n+    msg = 'All estimators are None or \"drop\". At least one is required!'\n     assert_raise_message(\n-        ValueError, msg, eclf2.set_params(lr=None, rf=None, nb=None).fit, X, y)\n+        ValueError, msg, eclf2.set_params(lr=drop, rf=drop, nb=drop).fit, X, y)\n \n     # Test soft voting transform\n     X1 = np.array([[1], [2]])\n@@ -444,7 +459,7 @@ def test_set_estimator_none():\n     eclf2 = VotingClassifier(estimators=[('rf', clf2), ('nb', clf3)],\n                              voting='soft', weights=[1, 0.5],\n                              flatten_transform=False)\n-    eclf2.set_params(rf=None).fit(X1, y1)\n+    eclf2.set_params(rf=drop).fit(X1, y1)\n     assert_array_almost_equal(eclf1.transform(X1),\n                               np.array([[[0.7, 0.3], [0.3, 0.7]],\n                                         [[1., 0.], [0., 1.]]]))\n@@ -522,12 +537,13 @@ def test_transform():\n          [('lr', LinearRegression()),\n           ('rf', RandomForestRegressor(n_estimators=5))]))]\n )\n-def test_none_estimator_with_weights(X, y, voter):\n+@pytest.mark.parametrize(\"drop\", [None, 'drop'])\n+def test_none_estimator_with_weights(X, y, voter, drop):\n     # check that an estimator can be set to None and passing some weight\n     # regression test for\n     # https://github.com/scikit-learn/scikit-learn/issues/13777\n     voter.fit(X, y, sample_weight=np.ones(y.shape))\n-    voter.set_params(lr=None)\n+    voter.set_params(lr=drop)\n     voter.fit(X, y, sample_weight=np.ones(y.shape))\n     y_pred = voter.predict(X)\n     assert y_pred.shape == y.shape\n",
  "problem_statement": "Handle 'drop' together with None to drop estimator in VotingClassifier/VotingRegressor\nAs mentioned in the following https://github.com/scikit-learn/scikit-learn/pull/11047#discussion_r264114338, the `VotingClassifier` and `VotingRegressor` should accept `'drop'` to drop an estimator from the ensemble is the same way that `None` is doing now.\n",
  "hints_text": "",
  "created_at": "2019-05-03T14:25:22Z",
  "version": "0.22",
  "FAIL_TO_PASS": "[\"sklearn/ensemble/tests/test_voting.py::test_sample_weight\", \"sklearn/ensemble/tests/test_voting.py::test_set_estimator_none[None]\", \"sklearn/ensemble/tests/test_voting.py::test_set_estimator_none[drop]\", \"sklearn/ensemble/tests/test_voting.py::test_none_estimator_with_weights[drop-X0-y0-voter0]\", \"sklearn/ensemble/tests/test_voting.py::test_none_estimator_with_weights[drop-X1-y1-voter1]\"]",
  "PASS_TO_PASS": "[\"sklearn/ensemble/tests/test_voting.py::test_estimator_init\", \"sklearn/ensemble/tests/test_voting.py::test_predictproba_hardvoting\", \"sklearn/ensemble/tests/test_voting.py::test_notfitted\", \"sklearn/ensemble/tests/test_voting.py::test_majority_label_iris\", \"sklearn/ensemble/tests/test_voting.py::test_tie_situation\", \"sklearn/ensemble/tests/test_voting.py::test_weights_iris\", \"sklearn/ensemble/tests/test_voting.py::test_weights_regressor\", \"sklearn/ensemble/tests/test_voting.py::test_predict_on_toy_problem\", \"sklearn/ensemble/tests/test_voting.py::test_predict_proba_on_toy_problem\", \"sklearn/ensemble/tests/test_voting.py::test_multilabel\", \"sklearn/ensemble/tests/test_voting.py::test_gridsearch\", \"sklearn/ensemble/tests/test_voting.py::test_parallel_fit\", \"sklearn/ensemble/tests/test_voting.py::test_sample_weight_kwargs\", \"sklearn/ensemble/tests/test_voting.py::test_set_params\", \"sklearn/ensemble/tests/test_voting.py::test_estimator_weights_format\", \"sklearn/ensemble/tests/test_voting.py::test_transform\", \"sklearn/ensemble/tests/test_voting.py::test_none_estimator_with_weights[None-X0-y0-voter0]\", \"sklearn/ensemble/tests/test_voting.py::test_none_estimator_with_weights[None-X1-y1-voter1]\"]",
  "environment_setup_commit": "7e85a6d1f038bbb932b36f18d75df6be937ed00d",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.998730",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}