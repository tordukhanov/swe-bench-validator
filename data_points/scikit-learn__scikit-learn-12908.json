{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-12908",
  "base_commit": "314686a65d543bd3b36d2af4b34ed23711991a57",
  "patch": "diff --git a/sklearn/preprocessing/_encoders.py b/sklearn/preprocessing/_encoders.py\n--- a/sklearn/preprocessing/_encoders.py\n+++ b/sklearn/preprocessing/_encoders.py\n@@ -2,7 +2,6 @@\n #          Joris Van den Bossche <jorisvandenbossche@gmail.com>\n # License: BSD 3 clause\n \n-\n import numbers\n import warnings\n \n@@ -158,6 +157,18 @@ class OneHotEncoder(_BaseEncoder):\n \n         The used categories can be found in the ``categories_`` attribute.\n \n+    drop : 'first' or a list/array of shape (n_features,), default=None.\n+        Specifies a methodology to use to drop one of the categories per\n+        feature. This is useful in situations where perfectly collinear\n+        features cause problems, such as when feeding the resulting data\n+        into a neural network or an unregularized regression.\n+\n+        - None : retain all features (the default).\n+        - 'first' : drop the first category in each feature. If only one\n+          category is present, the feature will be dropped entirely.\n+        - array : ``drop[i]`` is the category in feature ``X[:, i]`` that\n+          should be dropped.\n+\n     sparse : boolean, default=True\n         Will return sparse matrix if set True else will return an array.\n \n@@ -205,7 +216,13 @@ class OneHotEncoder(_BaseEncoder):\n     categories_ : list of arrays\n         The categories of each feature determined during fitting\n         (in order of the features in X and corresponding with the output\n-        of ``transform``).\n+        of ``transform``). This includes the category specified in ``drop``\n+        (if any).\n+\n+    drop_idx_ : array of shape (n_features,)\n+        ``drop_idx_[i]`` isÂ the index in ``categories_[i]`` of the category to\n+        be dropped for each feature. None if all the transformed features will\n+        be retained.\n \n     active_features_ : array\n         Indices for active features, meaning values that actually occur\n@@ -243,9 +260,9 @@ class OneHotEncoder(_BaseEncoder):\n     >>> enc.fit(X)\n     ... # doctest: +ELLIPSIS\n     ... # doctest: +NORMALIZE_WHITESPACE\n-    OneHotEncoder(categorical_features=None, categories=None,\n-           dtype=<... 'numpy.float64'>, handle_unknown='ignore',\n-           n_values=None, sparse=True)\n+    OneHotEncoder(categorical_features=None, categories=None, drop=None,\n+       dtype=<... 'numpy.float64'>, handle_unknown='ignore',\n+       n_values=None, sparse=True)\n \n     >>> enc.categories_\n     [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n@@ -257,6 +274,12 @@ class OneHotEncoder(_BaseEncoder):\n            [None, 2]], dtype=object)\n     >>> enc.get_feature_names()\n     array(['x0_Female', 'x0_Male', 'x1_1', 'x1_2', 'x1_3'], dtype=object)\n+    >>> drop_enc = OneHotEncoder(drop='first').fit(X)\n+    >>> drop_enc.categories_\n+    [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n+    >>> drop_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n+    array([[0., 0., 0.],\n+           [1., 1., 0.]])\n \n     See also\n     --------\n@@ -274,7 +297,7 @@ class OneHotEncoder(_BaseEncoder):\n     \"\"\"\n \n     def __init__(self, n_values=None, categorical_features=None,\n-                 categories=None, sparse=True, dtype=np.float64,\n+                 categories=None, drop=None, sparse=True, dtype=np.float64,\n                  handle_unknown='error'):\n         self.categories = categories\n         self.sparse = sparse\n@@ -282,6 +305,7 @@ def __init__(self, n_values=None, categorical_features=None,\n         self.handle_unknown = handle_unknown\n         self.n_values = n_values\n         self.categorical_features = categorical_features\n+        self.drop = drop\n \n     # Deprecated attributes\n \n@@ -346,7 +370,6 @@ def _handle_deprecations(self, X):\n                     )\n                     warnings.warn(msg, DeprecationWarning)\n             else:\n-\n                 # check if we have integer or categorical input\n                 try:\n                     check_array(X, dtype=np.int)\n@@ -354,20 +377,38 @@ def _handle_deprecations(self, X):\n                     self._legacy_mode = False\n                     self._categories = 'auto'\n                 else:\n-                    msg = (\n-                        \"The handling of integer data will change in version \"\n-                        \"0.22. Currently, the categories are determined \"\n-                        \"based on the range [0, max(values)], while in the \"\n-                        \"future they will be determined based on the unique \"\n-                        \"values.\\nIf you want the future behaviour and \"\n-                        \"silence this warning, you can specify \"\n-                        \"\\\"categories='auto'\\\".\\n\"\n-                        \"In case you used a LabelEncoder before this \"\n-                        \"OneHotEncoder to convert the categories to integers, \"\n-                        \"then you can now use the OneHotEncoder directly.\"\n-                    )\n-                    warnings.warn(msg, FutureWarning)\n-                    self._legacy_mode = True\n+                    if self.drop is None:\n+                        msg = (\n+                            \"The handling of integer data will change in \"\n+                            \"version 0.22. Currently, the categories are \"\n+                            \"determined based on the range \"\n+                            \"[0, max(values)], while in the future they \"\n+                            \"will be determined based on the unique \"\n+                            \"values.\\nIf you want the future behaviour \"\n+                            \"and silence this warning, you can specify \"\n+                            \"\\\"categories='auto'\\\".\\n\"\n+                            \"In case you used a LabelEncoder before this \"\n+                            \"OneHotEncoder to convert the categories to \"\n+                            \"integers, then you can now use the \"\n+                            \"OneHotEncoder directly.\"\n+                        )\n+                        warnings.warn(msg, FutureWarning)\n+                        self._legacy_mode = True\n+                    else:\n+                        msg = (\n+                            \"The handling of integer data will change in \"\n+                            \"version 0.22. Currently, the categories are \"\n+                            \"determined based on the range \"\n+                            \"[0, max(values)], while in the future they \"\n+                            \"will be determined based on the unique \"\n+                            \"values.\\n The old behavior is not compatible \"\n+                            \"with the `drop` parameter. Instead, you \"\n+                            \"must manually specify \\\"categories='auto'\\\" \"\n+                            \"if you wish to use the `drop` parameter on \"\n+                            \"an array of entirely integer data. This will \"\n+                            \"enable the future behavior.\"\n+                        )\n+                        raise ValueError(msg)\n \n         # if user specified categorical_features -> always use legacy mode\n         if self.categorical_features is not None:\n@@ -399,6 +440,13 @@ def _handle_deprecations(self, X):\n         else:\n             self._categorical_features = 'all'\n \n+        # Prevents new drop functionality from being used in legacy mode\n+        if self._legacy_mode and self.drop is not None:\n+            raise ValueError(\n+                \"The `categorical_features` and `n_values` keywords \"\n+                \"are deprecated, and cannot be used together \"\n+                \"with 'drop'.\")\n+\n     def fit(self, X, y=None):\n         \"\"\"Fit OneHotEncoder to X.\n \n@@ -411,10 +459,8 @@ def fit(self, X, y=None):\n         -------\n         self\n         \"\"\"\n-        if self.handle_unknown not in ('error', 'ignore'):\n-            msg = (\"handle_unknown should be either 'error' or 'ignore', \"\n-                   \"got {0}.\".format(self.handle_unknown))\n-            raise ValueError(msg)\n+\n+        self._validate_keywords()\n \n         self._handle_deprecations(X)\n \n@@ -425,8 +471,59 @@ def fit(self, X, y=None):\n             return self\n         else:\n             self._fit(X, handle_unknown=self.handle_unknown)\n+            self.drop_idx_ = self._compute_drop_idx()\n             return self\n \n+    def _compute_drop_idx(self):\n+        if self.drop is None:\n+            return None\n+        elif (isinstance(self.drop, str) and self.drop == 'first'):\n+            return np.zeros(len(self.categories_), dtype=np.int_)\n+        elif not isinstance(self.drop, str):\n+            try:\n+                self.drop = np.asarray(self.drop, dtype=object)\n+                droplen = len(self.drop)\n+            except (ValueError, TypeError):\n+                msg = (\"Wrong input for parameter `drop`. Expected \"\n+                       \"'first', None or array of objects, got {}\")\n+                raise ValueError(msg.format(type(self.drop)))\n+            if droplen != len(self.categories_):\n+                msg = (\"`drop` should have length equal to the number \"\n+                       \"of features ({}), got {}\")\n+                raise ValueError(msg.format(len(self.categories_),\n+                                            len(self.drop)))\n+            missing_drops = [(i, val) for i, val in enumerate(self.drop)\n+                             if val not in self.categories_[i]]\n+            if any(missing_drops):\n+                msg = (\"The following categories were supposed to be \"\n+                       \"dropped, but were not found in the training \"\n+                       \"data.\\n{}\".format(\n+                           \"\\n\".join(\n+                                [\"Category: {}, Feature: {}\".format(c, v)\n+                                    for c, v in missing_drops])))\n+                raise ValueError(msg)\n+            return np.array([np.where(cat_list == val)[0][0]\n+                             for (val, cat_list) in\n+                             zip(self.drop, self.categories_)], dtype=np.int_)\n+        else:\n+            msg = (\"Wrong input for parameter `drop`. Expected \"\n+                   \"'first', None or array of objects, got {}\")\n+            raise ValueError(msg.format(type(self.drop)))\n+\n+    def _validate_keywords(self):\n+        if self.handle_unknown not in ('error', 'ignore'):\n+            msg = (\"handle_unknown should be either 'error' or 'ignore', \"\n+                   \"got {0}.\".format(self.handle_unknown))\n+            raise ValueError(msg)\n+        # If we have both dropped columns and ignored unknown\n+        # values, there will be ambiguous cells. This creates difficulties\n+        # in interpreting the model.\n+        if self.drop is not None and self.handle_unknown != 'error':\n+            raise ValueError(\n+                \"`handle_unknown` must be 'error' when the drop parameter is \"\n+                \"specified, as both would create categories that are all \"\n+                \"zero.\")\n+\n     def _legacy_fit_transform(self, X):\n         \"\"\"Assumes X contains only categorical features.\"\"\"\n         dtype = getattr(X, 'dtype', None)\n@@ -501,10 +598,8 @@ def fit_transform(self, X, y=None):\n         X_out : sparse matrix if sparse=True else a 2-d array\n             Transformed input.\n         \"\"\"\n-        if self.handle_unknown not in ('error', 'ignore'):\n-            msg = (\"handle_unknown should be either 'error' or 'ignore', \"\n-                   \"got {0}.\".format(self.handle_unknown))\n-            raise ValueError(msg)\n+\n+        self._validate_keywords()\n \n         self._handle_deprecations(X)\n \n@@ -571,11 +666,22 @@ def _transform_new(self, X):\n \n         X_int, X_mask = self._transform(X, handle_unknown=self.handle_unknown)\n \n+        if self.drop is not None:\n+            to_drop = self.drop_idx_.reshape(1, -1)\n+\n+            # We remove all the dropped categories from mask, and decrement all\n+            # categories that occur after them to avoid an empty column.\n+\n+            keep_cells = X_int != to_drop\n+            X_mask &= keep_cells\n+            X_int[X_int > to_drop] -= 1\n+            n_values = [len(cats) - 1 for cats in self.categories_]\n+        else:\n+            n_values = [len(cats) for cats in self.categories_]\n+\n         mask = X_mask.ravel()\n-        n_values = [cats.shape[0] for cats in self.categories_]\n         n_values = np.array([0] + n_values)\n         feature_indices = np.cumsum(n_values)\n-\n         indices = (X_int + feature_indices[:-1]).ravel()[mask]\n         indptr = X_mask.sum(axis=1).cumsum()\n         indptr = np.insert(indptr, 0, 0)\n@@ -613,7 +719,7 @@ def transform(self, X):\n     def inverse_transform(self, X):\n         \"\"\"Convert the back data to the original representation.\n \n-        In case unknown categories are encountered (all zero's in the\n+        In case unknown categories are encountered (all zeros in the\n         one-hot encoding), ``None`` is used to represent this category.\n \n         Parameters\n@@ -635,7 +741,12 @@ def inverse_transform(self, X):\n \n         n_samples, _ = X.shape\n         n_features = len(self.categories_)\n-        n_transformed_features = sum([len(cats) for cats in self.categories_])\n+        if self.drop is None:\n+            n_transformed_features = sum(len(cats)\n+                                         for cats in self.categories_)\n+        else:\n+            n_transformed_features = sum(len(cats) - 1\n+                                         for cats in self.categories_)\n \n         # validate shape of passed X\n         msg = (\"Shape of the passed X data is not correct. Expected {0} \"\n@@ -651,18 +762,35 @@ def inverse_transform(self, X):\n         found_unknown = {}\n \n         for i in range(n_features):\n-            n_categories = len(self.categories_[i])\n+            if self.drop is None:\n+                cats = self.categories_[i]\n+            else:\n+                cats = np.delete(self.categories_[i], self.drop_idx_[i])\n+            n_categories = len(cats)\n+\n+            # Only happens if there was a column with a unique\n+            # category. In this case we just fill the column with this\n+            # unique category value.\n+            if n_categories == 0:\n+                X_tr[:, i] = self.categories_[i][self.drop_idx_[i]]\n+                j += n_categories\n+                continue\n             sub = X[:, j:j + n_categories]\n-\n             # for sparse X argmax returns 2D matrix, ensure 1D array\n             labels = np.asarray(_argmax(sub, axis=1)).flatten()\n-            X_tr[:, i] = self.categories_[i][labels]\n-\n+            X_tr[:, i] = cats[labels]\n             if self.handle_unknown == 'ignore':\n-                # ignored unknown categories: we have a row of all zero's\n                 unknown = np.asarray(sub.sum(axis=1) == 0).flatten()\n+                # ignored unknown categories: we have a row of all zero\n                 if unknown.any():\n                     found_unknown[i] = unknown\n+            # drop will either be None or handle_unknown will be error. If\n+            # self.drop is not None, then we can safely assume that all of\n+            # the nulls in each column are the dropped value\n+            elif self.drop is not None:\n+                dropped = np.asarray(sub.sum(axis=1) == 0).flatten()\n+                if dropped.any():\n+                    X_tr[dropped, i] = self.categories_[i][self.drop_idx_[i]]\n \n             j += n_categories\n \n",
  "test_patch": "diff --git a/sklearn/preprocessing/tests/test_encoders.py b/sklearn/preprocessing/tests/test_encoders.py\n--- a/sklearn/preprocessing/tests/test_encoders.py\n+++ b/sklearn/preprocessing/tests/test_encoders.py\n@@ -96,6 +96,20 @@ def test_one_hot_encoder_sparse():\n         enc.fit([[0], [1]])\n     assert_raises(ValueError, enc.transform, [[0], [-1]])\n \n+    with ignore_warnings(category=(DeprecationWarning, FutureWarning)):\n+        enc = OneHotEncoder(drop='first', n_values=1)\n+        for method in (enc.fit, enc.fit_transform):\n+            assert_raises_regex(\n+                ValueError,\n+                'The `categorical_features` and `n_values` keywords ',\n+                method, [[0], [-1]])\n+\n+            enc = OneHotEncoder(drop='first', categorical_features='all')\n+            assert_raises_regex(\n+                ValueError,\n+                'The `categorical_features` and `n_values` keywords ',\n+                method, [[0], [-1]])\n+\n \n def test_one_hot_encoder_dense():\n     # check for sparse=False\n@@ -278,7 +292,7 @@ def test_one_hot_encoder_no_categorical_features():\n     enc = OneHotEncoder(categorical_features=cat)\n     with ignore_warnings(category=(DeprecationWarning, FutureWarning)):\n         X_tr = enc.fit_transform(X)\n-    expected_features = np.array(list(), dtype='object')\n+    expected_features = np.array([], dtype='object')\n     assert_array_equal(X, X_tr)\n     assert_array_equal(enc.get_feature_names(), expected_features)\n     assert enc.categories_ == []\n@@ -373,21 +387,25 @@ def test_one_hot_encoder(X):\n     assert_allclose(Xtr.toarray(), [[0, 1, 1, 0,  1], [1, 0, 0, 1, 1]])\n \n \n-def test_one_hot_encoder_inverse():\n-    for sparse_ in [True, False]:\n-        X = [['abc', 2, 55], ['def', 1, 55], ['abc', 3, 55]]\n-        enc = OneHotEncoder(sparse=sparse_)\n-        X_tr = enc.fit_transform(X)\n-        exp = np.array(X, dtype=object)\n-        assert_array_equal(enc.inverse_transform(X_tr), exp)\n+@pytest.mark.parametrize('sparse_', [False, True])\n+@pytest.mark.parametrize('drop', [None, 'first'])\n+def test_one_hot_encoder_inverse(sparse_, drop):\n+    X = [['abc', 2, 55], ['def', 1, 55], ['abc', 3, 55]]\n+    enc = OneHotEncoder(sparse=sparse_, drop=drop)\n+    X_tr = enc.fit_transform(X)\n+    exp = np.array(X, dtype=object)\n+    assert_array_equal(enc.inverse_transform(X_tr), exp)\n \n-        X = [[2, 55], [1, 55], [3, 55]]\n-        enc = OneHotEncoder(sparse=sparse_, categories='auto')\n-        X_tr = enc.fit_transform(X)\n-        exp = np.array(X)\n-        assert_array_equal(enc.inverse_transform(X_tr), exp)\n+    X = [[2, 55], [1, 55], [3, 55]]\n+    enc = OneHotEncoder(sparse=sparse_, categories='auto',\n+                        drop=drop)\n+    X_tr = enc.fit_transform(X)\n+    exp = np.array(X)\n+    assert_array_equal(enc.inverse_transform(X_tr), exp)\n \n+    if drop is None:\n         # with unknown categories\n+        # drop is incompatible with handle_unknown=ignore\n         X = [['abc', 2, 55], ['def', 1, 55], ['abc', 3, 55]]\n         enc = OneHotEncoder(sparse=sparse_, handle_unknown='ignore',\n                             categories=[['abc', 'def'], [1, 2],\n@@ -407,10 +425,10 @@ def test_one_hot_encoder_inverse():\n         exp[:, 1] = None\n         assert_array_equal(enc.inverse_transform(X_tr), exp)\n \n-        # incorrect shape raises\n-        X_tr = np.array([[0, 1, 1], [1, 0, 1]])\n-        msg = re.escape('Shape of the passed X data is not correct')\n-        assert_raises_regex(ValueError, msg, enc.inverse_transform, X_tr)\n+    # incorrect shape raises\n+    X_tr = np.array([[0, 1, 1], [1, 0, 1]])\n+    msg = re.escape('Shape of the passed X data is not correct')\n+    assert_raises_regex(ValueError, msg, enc.inverse_transform, X_tr)\n \n \n @pytest.mark.parametrize(\"X, cat_exp, cat_dtype\", [\n@@ -687,3 +705,90 @@ def test_one_hot_encoder_warning():\n     enc = OneHotEncoder()\n     X = [['Male', 1], ['Female', 3]]\n     np.testing.assert_no_warnings(enc.fit_transform, X)\n+\n+\n+def test_one_hot_encoder_drop_manual():\n+    cats_to_drop = ['def', 12, 3, 56]\n+    enc = OneHotEncoder(drop=cats_to_drop)\n+    X = [['abc', 12, 2, 55],\n+         ['def', 12, 1, 55],\n+         ['def', 12, 3, 56]]\n+    trans = enc.fit_transform(X).toarray()\n+    exp = [[1, 0, 1, 1],\n+           [0, 1, 0, 1],\n+           [0, 0, 0, 0]]\n+    assert_array_equal(trans, exp)\n+    dropped_cats = [cat[feature]\n+                    for cat, feature in zip(enc.categories_,\n+                                            enc.drop_idx_)]\n+    assert_array_equal(dropped_cats, cats_to_drop)\n+    assert_array_equal(np.array(X, dtype=object),\n+                       enc.inverse_transform(trans))\n+\n+\n+def test_one_hot_encoder_invalid_params():\n+    enc = OneHotEncoder(drop='second')\n+    assert_raises_regex(\n+        ValueError,\n+        \"Wrong input for parameter `drop`.\",\n+        enc.fit, [[\"Male\"], [\"Female\"]])\n+\n+    enc = OneHotEncoder(handle_unknown='ignore', drop='first')\n+    assert_raises_regex(\n+        ValueError,\n+        \"`handle_unknown` must be 'error'\",\n+        enc.fit, [[\"Male\"], [\"Female\"]])\n+\n+    enc = OneHotEncoder(drop='first')\n+    assert_raises_regex(\n+        ValueError,\n+        \"The handling of integer data will change in version\",\n+        enc.fit, [[1], [2]])\n+\n+    enc = OneHotEncoder(drop='first', categories='auto')\n+    assert_no_warnings(enc.fit_transform, [[1], [2]])\n+\n+    enc = OneHotEncoder(drop=np.asarray('b', dtype=object))\n+    assert_raises_regex(\n+        ValueError,\n+        \"Wrong input for parameter `drop`.\",\n+        enc.fit, [['abc', 2, 55], ['def', 1, 55], ['def', 3, 59]])\n+\n+    enc = OneHotEncoder(drop=['ghi', 3, 59])\n+    assert_raises_regex(\n+        ValueError,\n+        \"The following categories were supposed\",\n+        enc.fit, [['abc', 2, 55], ['def', 1, 55], ['def', 3, 59]])\n+\n+\n+@pytest.mark.parametrize('drop', [['abc', 3], ['abc', 3, 41, 'a']])\n+def test_invalid_drop_length(drop):\n+    enc = OneHotEncoder(drop=drop)\n+    assert_raises_regex(\n+        ValueError,\n+        \"`drop` should have length equal to the number\",\n+        enc.fit, [['abc', 2, 55], ['def', 1, 55], ['def', 3, 59]])\n+\n+\n+@pytest.mark.parametrize(\"density\", [True, False],\n+                         ids=['sparse', 'dense'])\n+@pytest.mark.parametrize(\"drop\", ['first',\n+                                  ['a', 2, 'b']],\n+                         ids=['first', 'manual'])\n+def test_categories(density, drop):\n+    ohe_base = OneHotEncoder(sparse=density)\n+    ohe_test = OneHotEncoder(sparse=density, drop=drop)\n+    X = [['c', 1, 'a'],\n+         ['a', 2, 'b']]\n+    ohe_base.fit(X)\n+    ohe_test.fit(X)\n+    assert_array_equal(ohe_base.categories_, ohe_test.categories_)\n+    if drop == 'first':\n+        assert_array_equal(ohe_test.drop_idx_, 0)\n+    else:\n+        for drop_cat, drop_idx, cat_list in zip(drop,\n+                                                ohe_test.drop_idx_,\n+                                                ohe_test.categories_):\n+            assert cat_list[drop_idx] == drop_cat\n+    assert isinstance(ohe_test.drop_idx_, np.ndarray)\n+    assert ohe_test.drop_idx_.dtype == np.int_\n",
  "problem_statement": "OneHotEncoder - add option for 1 of k-1 encoding\nLike the title says. Would it be possible to add an option, say \"independent = True\" to OneHotEncoder that would return a 1 of k-1 encoding instead of a 1 of k encoding. This would be very useful to me when I am encoding categorical variables since the 1 of k encoding adds an extra (non-independent) degree of freedom to the model. It would also be nice if I could specify which category to keep as the baseline.\n\nSomething like:\n\n```\nX = np.array([12,24,36]).reshape(-1,1)  \nOneHotEncoder(sparse=False, independent=True, baseline=24).fit_transform(X)  \nOutput: array([[ 1., 0.],\n       [ 0., 0.],\n       [ 0., 1.]])\n```\n\nOneHotEncoding - Defining a reference category \nIn order to avoid multicollinearity in modelling, the number of dummy-coded variables needed should be one less than the number of categories. Therefore, it would be very code if OneHotEncoding could accept a reference category as an input variable. \n\n[MRG] ENH: add support for dropping first level of categorical feature\n#### Reference Issues\r\n\r\nFixes #6053\r\nFixes #9073\r\n\r\n#### What does this implement/fix? Explain your changes.\r\nThis Pull Request adds an extra argument to `DictVectorizer` that, if set to `True`, drops the first level of each categorical variable. This is extremely useful in a regression model that to not use regularisation, as it avoids multicollinearity.\r\n\r\n#### Any other comments\r\nEven though multicollinearity doesn't affect the predictions, it hugely affects the regression coefficients, which makes troublesome both model inspection and further usage of such coefficients.\n[MRG] add drop_first option to OneHotEncoder\n<!--\r\nThanks for contributing a pull request! Please ensure you have taken a look at\r\nthe contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist\r\n-->\r\n\r\n#### Reference Issues/PRs\r\n<!--\r\nExample: Fixes #1234. See also #3456.\r\nPlease use keywords (e.g., Fixes) to create link to the issues or pull requests\r\nyou resolved, so that they will automatically be closed when your pull request\r\nis merged. See https://github.com/blog/1506-closing-issues-via-pull-requests\r\n-->\r\n\r\nCloses #6488\r\n\r\n#### What does this implement/fix? Explain your changes.\r\n\r\nThis PR adds a `drop_first` option to `OneHotEncoder`.\r\nEach feature is encoded into `n_unique_values - 1` columns instead of `n_unique_values` columns. The first one is dropped, resulting in all of the others being zero.\r\n\r\n#### Any other comments?\r\n\r\nThis is incompatible with `handle_missing='ignore'` because the ignored missing categories result in all of the one-hot columns being zeros, which is also how the first category is treated when `drop_first=True`. So by allowing both, there would be no way to distinguish between a missing category and the first one.\r\n\r\n<!--\r\nPlease be aware that we are a loose team of volunteers so patience is\r\nnecessary; assistance handling other issues is very welcome. We value\r\nall user contributions, no matter how minor they are. If we are slow to\r\nreview, either the pull request needs some benchmarking, tinkering,\r\nconvincing, etc. or more likely the reviewers are simply busy. In either\r\ncase, we ask for your understanding during the review process.\r\nFor more information, see our FAQ on this topic:\r\nhttp://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.\r\n\r\nThanks for contributing!\r\n-->\r\n\n",
  "hints_text": "I guess we could do that as many people ask about it. I don't think there is that much of a point in doing that. Nearly all the models in scikit-learn are regularized, so this doesn't matter afaik.\nI guess you are using a linear model?\n\n@vighneshbirodkar is working on the `OneHotEncoder`, it might be worth waiting till that is finished.\n\nYup. I'm using a linear regression on categorical variables. So actually no regularization. \n\nThe regression works fine so there must be a fix for collinearity (non invertibility) in `scipy.linalg.lstsq` but I find this way of building a model a bit confusing. The solution to the least squared problem with collinearity is under determined - there is a family of solutions. And so to solve it there is some behind the scenes choice being made as to _the_ solution which is hidden from the user. Basically I'd rather not introduce collinearity into a model that will then have to deal with that collinearity. \n\nThis is all obviously just my under informed opinion :) \n\nWhy are you not using regularization? I think the main reason people don't use regularization is that they want simple statistics on the coefficients. But scikit-learn doesn't provide any statistics of the coefficients. Maybe statsmodels would be a better fit for your application.\n\nIf you are interested in predictive performance, just replace `LinearRegression` with `RidgeCV` and your predictions will improve.\n\nOk. I guess regularization is the way to go in scikit-learn. I do still disagree in that I think dependence shouldn't be introduced into the model by way of preprocessors (or at least there should be an option to turn this off). But maybe this is getting at the difference between machine learning and statistical modelling. Or maybe who cares about independence if we have regularization.\n\nSorry for the slow reply. I'm ok with adding an option to turn this off. But as I said, OneHotEncoder is still being refactored.\n\nI've elsewhere discussed similar for `LabelBinarizer` in the multiclass case, proposing the parameter name `drop_first` to ignore the encoding of the smallest value.\nnot sure if that was raise somewhere already. multicollinearity is really not a problem in any model in scikit-learn. But feel free to create a pull-request. The OneHotEncoder is being restructured quite heavily right now, though.\n\nI'm interested in working on this feature! I ran into some problems using a OneHotEncoder in a pipeline that used a Keras Neural Network as the classifier. I was attempting to transform a few columns of categorical features into a dummy variable representation and feed the resulting columns (plus some numerical variables that were passed through) into the NN for classification. However, the one hot encoding played poorly with the collinear columns, and my model performed poorly out of sample. I was eventually able to design a workaround, but it seems to me that it would be valuable to have a tool in scikit-learn that could do this simply. \r\nI see the above pull request, which began to implement this in the DictVectorizer class, but it looks like this was never implemented (probably due to some unresolved fixes that were suggested). Is there anything stopping this from being implemented in the OneHotEncoder case instead?\nI think we'd accept a PR. I'm a bit surprised there's none yet. We also changed the OneHotEncoder quite a bit recently. You probably don't want to modify the \"legacy\" mode. A question is whether/how we allow users to specify which category to drop. In regularized models this actually makes a difference IIRC.\r\nWe could have a parameter ``drop`` that's ``'none'`` by default, and could be ``'first'`` or a datastructure with the values to drop. could be a list/numpy array of length n_features (all input features are categorical in the new OneHotEncoder).\nReading through the comments on the old PR, I was thinking that those\noptions seem to be the natural choice. I'm in the midst of graduate school\napplications right now so my time is somewhat limited, but this seems to be\nsomething that is going to keep appearing in my work, so I'm going to have\nto address this (or keep using workarounds) at some point.\n\nOn Wed, Nov 28, 2018 at 3:49 PM Andreas Mueller <notifications@github.com>\nwrote:\n\n> I think we'd accept a PR. I'm a bit surprised there's none yet. We also\n> changed the OneHotEncoder quite a bit recently. You probably don't want to\n> modify the \"legacy\" mode. A question is whether/how we allow users to\n> specify which category to drop. In regularized models this actually makes a\n> difference IIRC.\n> We could have a parameter drop that's 'none' by default, and could be\n> 'first' or a datastructure with the values to drop. could be a list/numpy\n> array of length n_features (all input features are categorical in the new\n> OneHotEncoder).\n>\n> â\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/6053#issuecomment-442599181>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/Am4ucQaSxhMTVGCeWx4cy-xx5Xl3EHmOks5uzvbugaJpZM4G2bMF>\n> .\n>\n\n@jnothman are you happy with the changes I made? Feel free to leave additional comments if you find something that can be improved. I'm hoping to start working on some new bug fix as soon as this weekend.\nI think you'd best adopt something like my approach. Imagine someone\nanalysing the most stable important features under control validation. If\nthe feature dropped differs for each cv split, the results are\nuninterpretable\n\nOn 21 Jul 2017 6:43 am, \"Gianluca Rossi\" <notifications@github.com> wrote:\n\n*@IamGianluca* commented on this pull request.\n------------------------------\n\nIn sklearn/feature_extraction/dict_vectorizer.py\n<https://github.com/scikit-learn/scikit-learn/pull/9361#discussion_r128625938>\n:\n\n>          for x in X:\n             for f, v in six.iteritems(x):\n                 if isinstance(v, six.string_types):\n+                    if self.drop_first_category and f not in to_drop:\n\nHi Joel,\n\nI like your solution! I've intentionally avoided splitting the string using\na separator to overcome issues of ambiguity â I hope people don't ever use\nthe = character in columns names, but you never know :-) Let me know if you\nwant me to implement your suggestion, and I'll update my PR.\n\nI also fear this is too sensitive to the ordering of the data for the user\nto find it explicable.\n\nThat's a valid point. In my own project to overcome such problem, I've\nstored the dictionaries that I want to pass to DictVectorizer inside a\n\"master\" dictionary. This master dictionary has keys that can be sorted in\na way that guarantees the first category is deterministic.\n\nx = vectorizer.fit_transform([v for k, v in sorted(master.items())])\n\nIn this example a key in master could be something like the following tuple:\n\n(582498109, 'Desktop')\n\n... where Desktop is the level I want to drop, and each id (the first\nelement in the tuple) is associated with multiple devices, such as Tablet,\nMobile, etc. I appreciate this is specific to my use case and not always\ntrue.\n\nTo be entirely fair, as a Data Scientist, 99% of the times you don't really\ncare about which category is being dropped since that is simply your\nbaseline. I guess in those situations when you need a specific category to\nbe dropped, you can always build your own sorting function to pass to the\nkey argument in sorted.\n\nWhat do you think?\n\nâ\nYou are receiving this because you were mentioned.\n\nReply to this email directly, view it on GitHub\n<https://github.com/scikit-learn/scikit-learn/pull/9361#discussion_r128625938>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/AAEz6_FXTJlr9yn4TatzsLy6RkFc1lgfks5sP7vogaJpZM4OYxec>\n.\n\n",
  "created_at": "2019-01-03T04:21:02Z",
  "version": "0.21",
  "FAIL_TO_PASS": "[\"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_sparse\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_inverse[None-False]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_inverse[None-True]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_inverse[first-False]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_inverse[first-True]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_drop_manual\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_invalid_params\", \"sklearn/preprocessing/tests/test_encoders.py::test_invalid_drop_length[drop0]\", \"sklearn/preprocessing/tests/test_encoders.py::test_invalid_drop_length[drop1]\", \"sklearn/preprocessing/tests/test_encoders.py::test_categories[first-sparse]\", \"sklearn/preprocessing/tests/test_encoders.py::test_categories[first-dense]\", \"sklearn/preprocessing/tests/test_encoders.py::test_categories[manual-sparse]\", \"sklearn/preprocessing/tests/test_encoders.py::test_categories[manual-dense]\"]",
  "PASS_TO_PASS": "[\"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_dense\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_deprecationwarnings\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_force_new_behaviour\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_categorical_features\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_categorical_features_ignore_unknown\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_handle_unknown\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_not_fitted\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_no_categorical_features\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_handle_unknown_strings\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_dtype[int32-int32]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_dtype[int32-float32]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_dtype[int32-float64]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_dtype[float32-int32]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_dtype[float32-float32]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_dtype[float32-float64]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_dtype[float64-int32]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_dtype[float64-float32]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_dtype[float64-float64]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_dtype_pandas[int32]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_dtype_pandas[float32]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_dtype_pandas[float64]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_set_params\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder[mixed]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder[numeric]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder[object]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_categories[mixed]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_categories[numeric]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_categories[object]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_categories[string]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_specified_categories[object]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_specified_categories[numeric]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_specified_categories[object-string-cat]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_unsorted_categories\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_specified_categories_mixed_columns\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_pandas\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_feature_names\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_feature_names_unicode\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_raise_missing[error-numeric]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_raise_missing[error-object]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_raise_missing[ignore-numeric]\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_raise_missing[ignore-object]\", \"sklearn/preprocessing/tests/test_encoders.py::test_ordinal_encoder[mixed]\", \"sklearn/preprocessing/tests/test_encoders.py::test_ordinal_encoder[numeric]\", \"sklearn/preprocessing/tests/test_encoders.py::test_ordinal_encoder[object]\", \"sklearn/preprocessing/tests/test_encoders.py::test_ordinal_encoder_specified_categories[object]\", \"sklearn/preprocessing/tests/test_encoders.py::test_ordinal_encoder_specified_categories[numeric]\", \"sklearn/preprocessing/tests/test_encoders.py::test_ordinal_encoder_specified_categories[object-string-cat]\", \"sklearn/preprocessing/tests/test_encoders.py::test_ordinal_encoder_inverse\", \"sklearn/preprocessing/tests/test_encoders.py::test_ordinal_encoder_raise_missing[numeric]\", \"sklearn/preprocessing/tests/test_encoders.py::test_ordinal_encoder_raise_missing[object]\", \"sklearn/preprocessing/tests/test_encoders.py::test_encoder_dtypes\", \"sklearn/preprocessing/tests/test_encoders.py::test_encoder_dtypes_pandas\", \"sklearn/preprocessing/tests/test_encoders.py::test_one_hot_encoder_warning\"]",
  "environment_setup_commit": "7813f7efb5b2012412888b69e73d76f2df2b50b6",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.969197",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}