{
  "repo": "pytest-dev/pytest",
  "instance_id": "pytest-dev__pytest-7151",
  "base_commit": "2b51ed46d54be58da6bbcd28f68149b3fc2cd104",
  "patch": "diff --git a/src/_pytest/debugging.py b/src/_pytest/debugging.py\n--- a/src/_pytest/debugging.py\n+++ b/src/_pytest/debugging.py\n@@ -272,11 +272,15 @@ def pytest_internalerror(self, excrepr, excinfo):\n class PdbTrace:\n     @hookimpl(hookwrapper=True)\n     def pytest_pyfunc_call(self, pyfuncitem):\n-        _test_pytest_function(pyfuncitem)\n+        wrap_pytest_function_for_tracing(pyfuncitem)\n         yield\n \n \n-def _test_pytest_function(pyfuncitem):\n+def wrap_pytest_function_for_tracing(pyfuncitem):\n+    \"\"\"Changes the python function object of the given Function item by a wrapper which actually\n+    enters pdb before calling the python function itself, effectively leaving the user\n+    in the pdb prompt in the first statement of the function.\n+    \"\"\"\n     _pdb = pytestPDB._init_pdb(\"runcall\")\n     testfunction = pyfuncitem.obj\n \n@@ -291,6 +295,13 @@ def wrapper(*args, **kwargs):\n     pyfuncitem.obj = wrapper\n \n \n+def maybe_wrap_pytest_function_for_tracing(pyfuncitem):\n+    \"\"\"Wrap the given pytestfunct item for tracing support if --trace was given in\n+    the command line\"\"\"\n+    if pyfuncitem.config.getvalue(\"trace\"):\n+        wrap_pytest_function_for_tracing(pyfuncitem)\n+\n+\n def _enter_pdb(node, excinfo, rep):\n     # XXX we re-use the TerminalReporter's terminalwriter\n     # because this seems to avoid some encoding related troubles\ndiff --git a/src/_pytest/unittest.py b/src/_pytest/unittest.py\n--- a/src/_pytest/unittest.py\n+++ b/src/_pytest/unittest.py\n@@ -1,5 +1,4 @@\n \"\"\" discovery and running of std-library \"unittest\" style tests. \"\"\"\n-import functools\n import sys\n import traceback\n \n@@ -114,15 +113,17 @@ class TestCaseFunction(Function):\n     _testcase = None\n \n     def setup(self):\n-        self._needs_explicit_tearDown = False\n+        # a bound method to be called during teardown() if set (see 'runtest()')\n+        self._explicit_tearDown = None\n         self._testcase = self.parent.obj(self.name)\n         self._obj = getattr(self._testcase, self.name)\n         if hasattr(self, \"_request\"):\n             self._request._fillfixtures()\n \n     def teardown(self):\n-        if self._needs_explicit_tearDown:\n-            self._testcase.tearDown()\n+        if self._explicit_tearDown is not None:\n+            self._explicit_tearDown()\n+            self._explicit_tearDown = None\n         self._testcase = None\n         self._obj = None\n \n@@ -205,40 +206,31 @@ def _expecting_failure(self, test_method) -> bool:\n         return bool(expecting_failure_class or expecting_failure_method)\n \n     def runtest(self):\n-        # TODO: move testcase reporter into separate class, this shouldnt be on item\n-        import unittest\n+        from _pytest.debugging import maybe_wrap_pytest_function_for_tracing\n \n-        testMethod = getattr(self._testcase, self._testcase._testMethodName)\n-\n-        class _GetOutOf_testPartExecutor(KeyboardInterrupt):\n-            \"\"\"Helper exception to get out of unittests's testPartExecutor (see TestCase.run).\"\"\"\n-\n-        @functools.wraps(testMethod)\n-        def wrapped_testMethod(*args, **kwargs):\n-            \"\"\"Wrap the original method to call into pytest's machinery, so other pytest\n-            features can have a chance to kick in (notably --pdb)\"\"\"\n-            try:\n-                self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n-            except unittest.SkipTest:\n-                raise\n-            except Exception as exc:\n-                expecting_failure = self._expecting_failure(testMethod)\n-                if expecting_failure:\n-                    raise\n-                self._needs_explicit_tearDown = True\n-                raise _GetOutOf_testPartExecutor(exc)\n+        maybe_wrap_pytest_function_for_tracing(self)\n \n         # let the unittest framework handle async functions\n         if is_async_function(self.obj):\n             self._testcase(self)\n         else:\n-            setattr(self._testcase, self._testcase._testMethodName, wrapped_testMethod)\n+            # when --pdb is given, we want to postpone calling tearDown() otherwise\n+            # when entering the pdb prompt, tearDown() would have probably cleaned up\n+            # instance variables, which makes it difficult to debug\n+            # arguably we could always postpone tearDown(), but this changes the moment where the\n+            # TestCase instance interacts with the results object, so better to only do it\n+            # when absolutely needed\n+            if self.config.getoption(\"usepdb\"):\n+                self._explicit_tearDown = self._testcase.tearDown\n+                setattr(self._testcase, \"tearDown\", lambda *args: None)\n+\n+            # we need to update the actual bound method with self.obj, because\n+            # wrap_pytest_function_for_tracing replaces self.obj by a wrapper\n+            setattr(self._testcase, self.name, self.obj)\n             try:\n                 self._testcase(result=self)\n-            except _GetOutOf_testPartExecutor as exc:\n-                raise exc.args[0] from exc.args[0]\n             finally:\n-                delattr(self._testcase, self._testcase._testMethodName)\n+                delattr(self._testcase, self.name)\n \n     def _prunetraceback(self, excinfo):\n         Function._prunetraceback(self, excinfo)\n",
  "test_patch": "diff --git a/testing/test_unittest.py b/testing/test_unittest.py\n--- a/testing/test_unittest.py\n+++ b/testing/test_unittest.py\n@@ -537,28 +537,24 @@ def f(_):\n         )\n         result.stdout.fnmatch_lines(\n             [\n-                \"test_trial_error.py::TC::test_four SKIPPED\",\n+                \"test_trial_error.py::TC::test_four FAILED\",\n                 \"test_trial_error.py::TC::test_four ERROR\",\n                 \"test_trial_error.py::TC::test_one FAILED\",\n                 \"test_trial_error.py::TC::test_three FAILED\",\n-                \"test_trial_error.py::TC::test_two SKIPPED\",\n-                \"test_trial_error.py::TC::test_two ERROR\",\n+                \"test_trial_error.py::TC::test_two FAILED\",\n                 \"*ERRORS*\",\n                 \"*_ ERROR at teardown of TC.test_four _*\",\n-                \"NOTE: Incompatible Exception Representation, displaying natively:\",\n-                \"*DelayedCalls*\",\n-                \"*_ ERROR at teardown of TC.test_two _*\",\n-                \"NOTE: Incompatible Exception Representation, displaying natively:\",\n                 \"*DelayedCalls*\",\n                 \"*= FAILURES =*\",\n-                # \"*_ TC.test_four _*\",\n-                # \"*NameError*crash*\",\n+                \"*_ TC.test_four _*\",\n+                \"*NameError*crash*\",\n                 \"*_ TC.test_one _*\",\n                 \"*NameError*crash*\",\n                 \"*_ TC.test_three _*\",\n-                \"NOTE: Incompatible Exception Representation, displaying natively:\",\n                 \"*DelayedCalls*\",\n-                \"*= 2 failed, 2 skipped, 2 errors in *\",\n+                \"*_ TC.test_two _*\",\n+                \"*NameError*crash*\",\n+                \"*= 4 failed, 1 error in *\",\n             ]\n         )\n \n@@ -876,6 +872,37 @@ def test_notTornDown():\n     reprec.assertoutcome(passed=1, failed=1)\n \n \n+def test_cleanup_functions(testdir):\n+    \"\"\"Ensure functions added with addCleanup are always called after each test ends (#6947)\"\"\"\n+    testdir.makepyfile(\n+        \"\"\"\n+        import unittest\n+\n+        cleanups = []\n+\n+        class Test(unittest.TestCase):\n+\n+            def test_func_1(self):\n+                self.addCleanup(cleanups.append, \"test_func_1\")\n+\n+            def test_func_2(self):\n+                self.addCleanup(cleanups.append, \"test_func_2\")\n+                assert 0\n+\n+            def test_func_3_check_cleanups(self):\n+                assert cleanups == [\"test_func_1\", \"test_func_2\"]\n+    \"\"\"\n+    )\n+    result = testdir.runpytest(\"-v\")\n+    result.stdout.fnmatch_lines(\n+        [\n+            \"*::test_func_1 PASSED *\",\n+            \"*::test_func_2 FAILED *\",\n+            \"*::test_func_3_check_cleanups PASSED *\",\n+        ]\n+    )\n+\n+\n def test_issue333_result_clearing(testdir):\n     testdir.makeconftest(\n         \"\"\"\n@@ -1131,6 +1158,41 @@ def test(self):\n     assert result.ret == 0\n \n \n+def test_pdb_teardown_called(testdir, monkeypatch):\n+    \"\"\"Ensure tearDown() is always called when --pdb is given in the command-line.\n+\n+    We delay the normal tearDown() calls when --pdb is given, so this ensures we are calling\n+    tearDown() eventually to avoid memory leaks when using --pdb.\n+    \"\"\"\n+    teardowns = []\n+    monkeypatch.setattr(\n+        pytest, \"test_pdb_teardown_called_teardowns\", teardowns, raising=False\n+    )\n+\n+    testdir.makepyfile(\n+        \"\"\"\n+        import unittest\n+        import pytest\n+\n+        class MyTestCase(unittest.TestCase):\n+\n+            def tearDown(self):\n+                pytest.test_pdb_teardown_called_teardowns.append(self.id())\n+\n+            def test_1(self):\n+                pass\n+            def test_2(self):\n+                pass\n+    \"\"\"\n+    )\n+    result = testdir.runpytest_inprocess(\"--pdb\")\n+    result.stdout.fnmatch_lines(\"* 2 passed in *\")\n+    assert teardowns == [\n+        \"test_pdb_teardown_called.MyTestCase.test_1\",\n+        \"test_pdb_teardown_called.MyTestCase.test_2\",\n+    ]\n+\n+\n def test_async_support(testdir):\n     pytest.importorskip(\"unittest.async_case\")\n \n",
  "problem_statement": "unittest.TestCase cleanup functions not invoked on test failure\nstdlib unittest style cleanup functions registered with `unittest.TestCase.addCleanup` are not invoked when a test fails.  It appears this issue was introduced in pytest version 5.4.0, examples below for version 5.4.1.\r\n\r\n### System Info\r\n\r\n- Ubuntu 18.04.3 LTS\r\n- Python 3.6.8\r\n- pytest 5.4.1\r\n\r\n\r\n### Example test and pytest output\r\n\r\n```python\r\nimport unittest\r\n\r\ndef cleanup():\r\n    raise Exception('cleanup')\r\n\r\nclass Test(unittest.TestCase):\r\n    def setUp(self):\r\n        print('setup')\r\n        self.addCleanup(cleanup)\r\n\r\n    def tearDown(self):\r\n        print('teardown')\r\n\r\n    def test_no_cleanup(self):\r\n        assert False\r\n\r\n    def test_cleanup(self):\r\n        assert True\r\n\r\n```\r\n\r\n```\r\n(venv-3.6.8) cecil@python36-vm:~$ pytest ceciltest.py\r\n========================================================================================= test session starts ==========================================================================================\r\nplatform linux -- Python 3.6.8, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\r\nrootdir: /home/cecil\r\nplugins: cov-2.8.1\r\ncollected 2 items\r\n\r\nceciltest.py FF                                                                                                                                                                                  [100%]\r\n\r\n=============================================================================================== FAILURES ===============================================================================================\r\n__________________________________________________________________________________________ Test.test_cleanup ___________________________________________________________________________________________\r\n\r\n    def cleanup():\r\n>       raise Exception('cleanup!')\r\nE       Exception: cleanup!\r\n\r\nceciltest.py:4: Exception\r\n----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------\r\nsetup\r\nteardown\r\n_________________________________________________________________________________________ Test.test_no_cleanup _________________________________________________________________________________________\r\n\r\nself = <ceciltest.Test testMethod=test_no_cleanup>\r\n\r\n    def test_no_cleanup(self):\r\n>       assert False\r\nE       assert False\r\n\r\nceciltest.py:16: AssertionError\r\n----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------\r\nsetup\r\n--------------------------------------------------------------------------------------- Captured stdout teardown ---------------------------------------------------------------------------------------\r\nteardown\r\n======================================================================================= short test summary info ========================================================================================\r\nFAILED ceciltest.py::Test::test_cleanup - Exception: cleanup!\r\nFAILED ceciltest.py::Test::test_no_cleanup - assert False\r\n========================================================================================== 2 failed in 0.12s ===========================================================================================\r\n\r\n```\r\n\r\n### Trying pytest 5.3.5 (works as expected)\r\n\r\n```\r\n(venv-3.6.8) cecil@python36-vm:~$ pytest ceciltest.py\r\n========================================================================================= test session starts ==========================================================================================\r\nplatform linux -- Python 3.6.8, pytest-5.3.5, py-1.8.1, pluggy-0.13.1\r\nrootdir: /home/cecil\r\nplugins: cov-2.8.1\r\ncollected 2 items\r\n\r\nceciltest.py FFE                                                                                                                                                                                 [100%]\r\n\r\n================================================================================================ ERRORS ================================================================================================\r\n______________________________________________________________________________ ERROR at teardown of Test.test_no_cleanup _______________________________________________________________________________\r\n\r\n    def cleanup():\r\n>       raise Exception('cleanup!')\r\nE       Exception: cleanup!\r\n\r\nceciltest.py:4: Exception\r\n----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------\r\nsetup\r\nteardown\r\n=============================================================================================== FAILURES ===============================================================================================\r\n__________________________________________________________________________________________ Test.test_cleanup ___________________________________________________________________________________________\r\n\r\n    def cleanup():\r\n>       raise Exception('cleanup!')\r\nE       Exception: cleanup!\r\n\r\nceciltest.py:4: Exception\r\n----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------\r\nsetup\r\nteardown\r\n_________________________________________________________________________________________ Test.test_no_cleanup _________________________________________________________________________________________\r\n\r\nself = <ceciltest.Test testMethod=test_no_cleanup>\r\n\r\n    def test_no_cleanup(self):\r\n>       assert False\r\nE       AssertionError: assert False\r\n\r\nceciltest.py:16: AssertionError\r\n----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------\r\nsetup\r\nteardown\r\n====================================================================================== 2 failed, 1 error in 0.12s ======================================================================================\r\n```\r\n\r\n### pip list\r\n```\r\n(venv-3.6.8) cecil@python36-vm:~$ pip list\r\nPackage            Version\r\n------------------ ----------\r\nastroid            2.3.3\r\nattrs              19.3.0\r\nboto               2.49.0\r\ncertifi            2019.11.28\r\nchardet            3.0.4\r\ncoverage           5.0.4\r\nhttpretty          0.8.10\r\nidna               2.9\r\nimportlib-metadata 1.5.0\r\nisort              4.3.21\r\nJinja2             2.11.1\r\nlazy-object-proxy  1.4.3\r\nMarkupSafe         1.1.1\r\nmccabe             0.6.1\r\nmore-itertools     8.2.0\r\nmoto               0.4.31\r\npackaging          20.3\r\nparameterized      0.7.0\r\npep8               1.6.2\r\npip                18.1\r\npluggy             0.13.1\r\npy                 1.8.1\r\npylint             2.4.4\r\npyparsing          2.4.6\r\npytest             5.4.1\r\npytest-cov         2.8.1\r\npython-dateutil    2.8.1\r\npytz               2019.3\r\nrequests           2.23.0\r\nsetuptools         40.6.2\r\nsix                1.14.0\r\ntyped-ast          1.4.1\r\nurllib3            1.25.8\r\nwcwidth            0.1.8\r\nWerkzeug           1.0.0\r\nwrapt              1.11.2\r\nxmltodict          0.12.0\r\nzipp               3.1.0\r\n\r\n```\n",
  "hints_text": "Here's a unit test to add to https://github.com/pytest-dev/pytest/blob/master/testing/test_unittest.py when the fix is ready. (Passes on ``5.3.5``, Fails on ``5.4.0``)\r\n```python\r\ndef test_outcome_errors(testdir):\r\n    testpath = testdir.makepyfile(\r\n        \"\"\"\r\n        import unittest\r\n        class MyTestCase(unittest.TestCase):\r\n            def test_fail(self):\r\n                raise Exception(\"FAIL!\")\r\n            def tearDown(self):\r\n                print(self._outcome.errors)\r\n    \"\"\"\r\n    )\r\n    reprec = testdir.inline_run(testpath)\r\n    passed, skipped, failed = reprec.countoutcomes()\r\n    assert failed == 1, failed\r\n```\r\nIf pytest does TDD, I can just create a pull-request for this test right now if you want, and it'll be failing until the issue is fixed.\n@mdmintz are you sure your test is related to this issue? See https://github.com/pytest-dev/pytest/pull/7049#issuecomment-611399172.\r\n\r\nIf not, could you please open a separate issue? Thanks!\n@nicoddemus I created #7000 for it, but @blueyed closed that as a duplicate, and it might not be a duplicate, but it is related to unittest tearDown, as the outcome._errors value is missing there. Also see @blueyed's comment here: https://github.com/seleniumbase/SeleniumBase/issues/534#issuecomment-607570211 \r\nWhen #7000 is fixed, that test will pass.\n@mdmintz \r\nYes, it is the same root cause, just another symptom (https://github.com/pytest-dev/pytest/issues/7000#issuecomment-607749633).\nCan I work on this?",
  "created_at": "2020-05-01T20:35:40Z",
  "version": "5.4",
  "FAIL_TO_PASS": "[\"testing/test_unittest.py::test_cleanup_functions\"]",
  "PASS_TO_PASS": "[\"testing/test_unittest.py::test_simple_unittest\", \"testing/test_unittest.py::test_runTest_method\", \"testing/test_unittest.py::test_isclasscheck_issue53\", \"testing/test_unittest.py::test_setup\", \"testing/test_unittest.py::test_setUpModule\", \"testing/test_unittest.py::test_setUpModule_failing_no_teardown\", \"testing/test_unittest.py::test_new_instances\", \"testing/test_unittest.py::test_function_item_obj_is_instance\", \"testing/test_unittest.py::test_teardown\", \"testing/test_unittest.py::test_teardown_issue1649\", \"testing/test_unittest.py::test_unittest_skip_issue148\", \"testing/test_unittest.py::test_method_and_teardown_failing_reporting\", \"testing/test_unittest.py::test_setup_failure_is_shown\", \"testing/test_unittest.py::test_setup_setUpClass\", \"testing/test_unittest.py::test_setup_class\", \"testing/test_unittest.py::test_testcase_adderrorandfailure_defers[Error]\", \"testing/test_unittest.py::test_testcase_adderrorandfailure_defers[Failure]\", \"testing/test_unittest.py::test_testcase_custom_exception_info[Error]\", \"testing/test_unittest.py::test_testcase_custom_exception_info[Failure]\", \"testing/test_unittest.py::test_testcase_totally_incompatible_exception_info\", \"testing/test_unittest.py::test_module_level_pytestmark\", \"testing/test_unittest.py::test_djangolike_testcase\", \"testing/test_unittest.py::test_unittest_not_shown_in_traceback\", \"testing/test_unittest.py::test_unorderable_types\", \"testing/test_unittest.py::test_unittest_typerror_traceback\", \"testing/test_unittest.py::test_unittest_expected_failure_for_failing_test_is_xfail[pytest]\", \"testing/test_unittest.py::test_unittest_expected_failure_for_failing_test_is_xfail[unittest]\", \"testing/test_unittest.py::test_unittest_expected_failure_for_passing_test_is_fail[pytest]\", \"testing/test_unittest.py::test_unittest_expected_failure_for_passing_test_is_fail[unittest]\", \"testing/test_unittest.py::test_unittest_setup_interaction[fixture-return]\", \"testing/test_unittest.py::test_unittest_setup_interaction[yield_fixture-yield]\", \"testing/test_unittest.py::test_non_unittest_no_setupclass_support\", \"testing/test_unittest.py::test_no_teardown_if_setupclass_failed\", \"testing/test_unittest.py::test_issue333_result_clearing\", \"testing/test_unittest.py::test_unittest_raise_skip_issue748\", \"testing/test_unittest.py::test_unittest_skip_issue1169\", \"testing/test_unittest.py::test_class_method_containing_test_issue1558\", \"testing/test_unittest.py::test_usefixtures_marker_on_unittest[builtins.object]\", \"testing/test_unittest.py::test_usefixtures_marker_on_unittest[unittest.TestCase]\", \"testing/test_unittest.py::test_testcase_handles_init_exceptions\", \"testing/test_unittest.py::test_error_message_with_parametrized_fixtures\", \"testing/test_unittest.py::test_setup_inheritance_skipping[test_setup_skip.py-1\", \"testing/test_unittest.py::test_setup_inheritance_skipping[test_setup_skip_class.py-1\", \"testing/test_unittest.py::test_setup_inheritance_skipping[test_setup_skip_module.py-1\", \"testing/test_unittest.py::test_BdbQuit\", \"testing/test_unittest.py::test_exit_outcome\", \"testing/test_unittest.py::test_trace\", \"testing/test_unittest.py::test_pdb_teardown_called\", \"testing/test_unittest.py::test_async_support\"]",
  "environment_setup_commit": "678c1a0745f1cf175c442c719906a1f13e496910",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.933665",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}