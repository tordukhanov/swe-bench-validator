{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-14999",
  "base_commit": "d2476fb679f05e80c56e8b151ff0f6d7a470e4ae",
  "patch": "diff --git a/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py b/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\n--- a/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\n+++ b/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\n@@ -104,12 +104,14 @@ def fit(self, X, y):\n         X, y = check_X_y(X, y, dtype=[X_DTYPE], force_all_finite=False)\n         y = self._encode_y(y)\n \n-        # The rng state must be preserved if warm_start is True\n-        if (self.warm_start and hasattr(self, '_rng')):\n-            rng = self._rng\n-        else:\n-            rng = check_random_state(self.random_state)\n-            self._rng = rng\n+        rng = check_random_state(self.random_state)\n+\n+        # When warm starting, we want to re-use the same seed that was used\n+        # the first time fit was called (e.g. for subsampling or for the\n+        # train/val split).\n+        if not (self.warm_start and self._is_fitted()):\n+            self._random_seed = rng.randint(np.iinfo(np.uint32).max,\n+                                            dtype='u8')\n \n         self._validate_parameters()\n         self.n_features_ = X.shape[1]  # used for validation in predict()\n@@ -138,12 +140,10 @@ def fit(self, X, y):\n             # Save the state of the RNG for the training and validation split.\n             # This is needed in order to have the same split when using\n             # warm starting.\n-            if not (self._is_fitted() and self.warm_start):\n-                self._train_val_split_seed = rng.randint(1024)\n \n             X_train, X_val, y_train, y_val = train_test_split(\n                 X, y, test_size=self.validation_fraction, stratify=stratify,\n-                random_state=self._train_val_split_seed)\n+                random_state=self._random_seed)\n         else:\n             X_train, y_train = X, y\n             X_val, y_val = None, None\n@@ -159,10 +159,11 @@ def fit(self, X, y):\n         # actual total number of bins. Everywhere in the code, the\n         # convention is that n_bins == max_bins + 1\n         n_bins = self.max_bins + 1  # + 1 for missing values\n-        self.bin_mapper_ = _BinMapper(n_bins=n_bins, random_state=rng)\n-        X_binned_train = self._bin_data(X_train, rng, is_training_data=True)\n+        self.bin_mapper_ = _BinMapper(n_bins=n_bins,\n+                                      random_state=self._random_seed)\n+        X_binned_train = self._bin_data(X_train, is_training_data=True)\n         if X_val is not None:\n-            X_binned_val = self._bin_data(X_val, rng, is_training_data=False)\n+            X_binned_val = self._bin_data(X_val, is_training_data=False)\n         else:\n             X_binned_val = None\n \n@@ -241,13 +242,10 @@ def fit(self, X, y):\n                     # the predictions of all the trees. So we use a subset of\n                     # the training set to compute train scores.\n \n-                    # Save the seed for the small trainset generator\n-                    self._small_trainset_seed = rng.randint(1024)\n-\n                     # Compute the subsample set\n                     (X_binned_small_train,\n                      y_small_train) = self._get_small_trainset(\n-                        X_binned_train, y_train, self._small_trainset_seed)\n+                        X_binned_train, y_train, self._random_seed)\n \n                     self._check_early_stopping_scorer(\n                         X_binned_small_train, y_small_train,\n@@ -276,7 +274,7 @@ def fit(self, X, y):\n             if self.do_early_stopping_ and self.scoring != 'loss':\n                 # Compute the subsample set\n                 X_binned_small_train, y_small_train = self._get_small_trainset(\n-                    X_binned_train, y_train, self._small_trainset_seed)\n+                    X_binned_train, y_train, self._random_seed)\n \n             # Initialize the gradients and hessians\n             gradients, hessians = self.loss_.init_gradients_and_hessians(\n@@ -400,7 +398,7 @@ def _is_fitted(self):\n \n     def _clear_state(self):\n         \"\"\"Clear the state of the gradient boosting model.\"\"\"\n-        for var in ('train_score_', 'validation_score_', '_rng'):\n+        for var in ('train_score_', 'validation_score_'):\n             if hasattr(self, var):\n                 delattr(self, var)\n \n@@ -488,7 +486,7 @@ def _should_stop(self, scores):\n                                for score in recent_scores]\n         return not any(recent_improvements)\n \n-    def _bin_data(self, X, rng, is_training_data):\n+    def _bin_data(self, X, is_training_data):\n         \"\"\"Bin data X.\n \n         If is_training_data, then set the bin_mapper_ attribute.\n",
  "test_patch": "diff --git a/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py b/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py\n--- a/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py\n+++ b/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py\n@@ -154,13 +154,15 @@ def test_warm_start_clear(GradientBoosting, X, y):\n     (HistGradientBoostingClassifier, X_classification, y_classification),\n     (HistGradientBoostingRegressor, X_regression, y_regression)\n ])\n-@pytest.mark.parametrize('rng_type', ('int', 'instance'))\n+@pytest.mark.parametrize('rng_type', ('none', 'int', 'instance'))\n def test_random_seeds_warm_start(GradientBoosting, X, y, rng_type):\n     # Make sure the seeds for train/val split and small trainset subsampling\n     # are correctly set in a warm start context.\n     def _get_rng(rng_type):\n         # Helper to avoid consuming rngs\n-        if rng_type == 'int':\n+        if rng_type == 'none':\n+            return None\n+        elif rng_type == 'int':\n             return 42\n         else:\n             return np.random.RandomState(0)\n@@ -169,22 +171,30 @@ def _get_rng(rng_type):\n     gb_1 = GradientBoosting(n_iter_no_change=5, max_iter=2,\n                             random_state=random_state)\n     gb_1.fit(X, y)\n-    train_val_seed_1 = gb_1._train_val_split_seed\n-    small_trainset_seed_1 = gb_1._small_trainset_seed\n+    random_seed_1_1 = gb_1._random_seed\n+\n+    gb_1.fit(X, y)\n+    random_seed_1_2 = gb_1._random_seed  # clear the old state, different seed\n \n     random_state = _get_rng(rng_type)\n     gb_2 = GradientBoosting(n_iter_no_change=5, max_iter=2,\n                             random_state=random_state, warm_start=True)\n     gb_2.fit(X, y)  # inits state\n-    train_val_seed_2 = gb_2._train_val_split_seed\n-    small_trainset_seed_2 = gb_2._small_trainset_seed\n+    random_seed_2_1 = gb_2._random_seed\n     gb_2.fit(X, y)  # clears old state and equals est\n-    train_val_seed_3 = gb_2._train_val_split_seed\n-    small_trainset_seed_3 = gb_2._small_trainset_seed\n-\n-    # Check that all seeds are equal\n-    assert train_val_seed_1 == train_val_seed_2\n-    assert small_trainset_seed_1 == small_trainset_seed_2\n-\n-    assert train_val_seed_2 == train_val_seed_3\n-    assert small_trainset_seed_2 == small_trainset_seed_3\n+    random_seed_2_2 = gb_2._random_seed\n+\n+    # Without warm starting, the seeds should be\n+    # * all different if random state is None\n+    # * all equal if random state is an integer\n+    # * different when refitting and equal with a new estimator (because\n+    #   the random state is mutated)\n+    if rng_type == 'none':\n+        assert random_seed_1_1 != random_seed_1_2 != random_seed_2_1\n+    elif rng_type == 'int':\n+        assert random_seed_1_1 == random_seed_1_2 == random_seed_2_1\n+    else:\n+        assert random_seed_1_1 == random_seed_2_1 != random_seed_1_2\n+\n+    # With warm starting, the seeds must be equal\n+    assert random_seed_2_1 == random_seed_2_2\n",
  "problem_statement": "data leak in GBDT due to warm start\n(This is about the non-histogram-based version of GBDTs)\r\n\r\nX is split into train and validation data with `train_test_split(random_state=self.random_state)`.\r\n\r\nAs @johannfaouzi noted, in a warm starting context, this will produce a leak if If `self.random_state` is a `RandomState` instance: some samples that were used for training in a previous `fit` might be used for validation now.\r\n\r\n~~I think the right fix would be to raise a `ValueError` if the provided random state isn't a number and early-stopping is activated~~\n",
  "hints_text": "After discussing with @amueller \t, maybe the best option would be to:\r\n\r\n- store a seed attribute e.g. `_train_val_split_seed` that would be generated **once**, the first time `fit` is called\r\n- pass this seed as the `random_state`  parameter to `train_test_split()`.\r\n- add a small test making sure this parameter stays constant between different calls to `fit`\r\n\nThis should only be done when warm_start is true, though. So different calls to ``fit`` without warm start should probably use different splits if random state was None or a random state object.\nRight.\r\n\r\nThe problem is that we allow warm_start to be False the first time we fit, and later set it to true with `set_params` :(\nSorry, we should always store the seed, but only re-use it if ``warm_start=True``.\nWhy do you prefer to store a seed over `get_state()`\nAt the end of the day, something has to be stored in order to generate the same training and validation sets. An integer is smaller than a tuple returned by `get_state()`, but the difference can probably be overlooked.\nIf we store a seed we can just directly pass it to `train_test_split`, avoiding the need to create a new RandomState instance.\r\n\r\nboth are fine with me. \n+1 for storing a seed as fit param and reuse that to seed an rng in fit only when `warm_start=True`.\r\n\r\nAFAIK, `np.random.RandomState` accept `uint32` seed only (between `0` and `2**32 - 1`). So the correct way to get a seed from an existing random state object is:\r\n\r\n```python\r\nself.random_seed_ = check_random_state(self.random_state).randint(np.iinfo(np.uint32).max)\r\n```\nI did something that is not as good for HGBDT:\r\nhttps://github.com/scikit-learn/scikit-learn/blob/06632c0d185128a53c57ccc73b25b6408e90bb89/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py#L142\r\nIt would probably be worth changing this as well.\r\n\r\nI can work on this if there is no one.\nPR welcome @johannfaouzi :)\r\n\r\nAlso instead of having `_small_trainset_seed` and `_train_val_seed` maybe we can just have one single seed. And I just realized that seed should also be passed to the binmapper that will also subsample.",
  "created_at": "2019-09-17T07:06:27Z",
  "version": "0.22",
  "FAIL_TO_PASS": "[\"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_random_seeds_warm_start[none-HistGradientBoostingClassifier-X0-y0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_random_seeds_warm_start[none-HistGradientBoostingRegressor-X1-y1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_random_seeds_warm_start[int-HistGradientBoostingClassifier-X0-y0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_random_seeds_warm_start[int-HistGradientBoostingRegressor-X1-y1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_random_seeds_warm_start[instance-HistGradientBoostingClassifier-X0-y0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_random_seeds_warm_start[instance-HistGradientBoostingRegressor-X1-y1]\"]",
  "PASS_TO_PASS": "[\"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_max_iter_with_warm_start_validation[HistGradientBoostingClassifier-X0-y0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_max_iter_with_warm_start_validation[HistGradientBoostingRegressor-X1-y1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_yields_identical_results[HistGradientBoostingClassifier-X0-y0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_yields_identical_results[HistGradientBoostingRegressor-X1-y1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_max_depth[HistGradientBoostingClassifier-X0-y0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_max_depth[HistGradientBoostingRegressor-X1-y1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_early_stopping[HistGradientBoostingClassifier-X0-y0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_early_stopping[HistGradientBoostingRegressor-X1-y1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_equal_n_estimators[HistGradientBoostingClassifier-X0-y0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_equal_n_estimators[HistGradientBoostingRegressor-X1-y1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_clear[HistGradientBoostingClassifier-X0-y0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_clear[HistGradientBoostingRegressor-X1-y1]\"]",
  "environment_setup_commit": "7e85a6d1f038bbb932b36f18d75df6be937ed00d",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:31.010093",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}