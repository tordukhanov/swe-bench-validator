{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-14012",
  "base_commit": "15b54340ee7dc7cb870a418d1b5f6f553672f5dd",
  "patch": "diff --git a/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py b/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\n--- a/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\n+++ b/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\n@@ -26,8 +26,8 @@ class BaseHistGradientBoosting(BaseEstimator, ABC):\n     @abstractmethod\n     def __init__(self, loss, learning_rate, max_iter, max_leaf_nodes,\n                  max_depth, min_samples_leaf, l2_regularization, max_bins,\n-                 scoring, validation_fraction, n_iter_no_change, tol, verbose,\n-                 random_state):\n+                 warm_start, scoring, validation_fraction, n_iter_no_change,\n+                 tol, verbose, random_state):\n         self.loss = loss\n         self.learning_rate = learning_rate\n         self.max_iter = max_iter\n@@ -36,9 +36,10 @@ def __init__(self, loss, learning_rate, max_iter, max_leaf_nodes,\n         self.min_samples_leaf = min_samples_leaf\n         self.l2_regularization = l2_regularization\n         self.max_bins = max_bins\n-        self.n_iter_no_change = n_iter_no_change\n-        self.validation_fraction = validation_fraction\n+        self.warm_start = warm_start\n         self.scoring = scoring\n+        self.validation_fraction = validation_fraction\n+        self.n_iter_no_change = n_iter_no_change\n         self.tol = tol\n         self.verbose = verbose\n         self.random_state = random_state\n@@ -88,7 +89,6 @@ def fit(self, X, y):\n         -------\n         self : object\n         \"\"\"\n-\n         fit_start_time = time()\n         acc_find_split_time = 0.  # time spent finding the best splits\n         acc_apply_split_time = 0.  # time spent splitting nodes\n@@ -97,7 +97,13 @@ def fit(self, X, y):\n         acc_prediction_time = 0.\n         X, y = check_X_y(X, y, dtype=[X_DTYPE])\n         y = self._encode_y(y)\n-        rng = check_random_state(self.random_state)\n+\n+        # The rng state must be preserved if warm_start is True\n+        if (self.warm_start and hasattr(self, '_rng')):\n+            rng = self._rng\n+        else:\n+            rng = check_random_state(self.random_state)\n+            self._rng = rng\n \n         self._validate_parameters()\n         self.n_features_ = X.shape[1]  # used for validation in predict()\n@@ -112,7 +118,6 @@ def fit(self, X, y):\n         # data.\n         self._in_fit = True\n \n-\n         self.loss_ = self._get_loss()\n \n         self.do_early_stopping_ = (self.n_iter_no_change is not None and\n@@ -124,9 +129,15 @@ def fit(self, X, y):\n             # stratify for classification\n             stratify = y if hasattr(self.loss_, 'predict_proba') else None\n \n+            # Save the state of the RNG for the training and validation split.\n+            # This is needed in order to have the same split when using\n+            # warm starting.\n+            if not (self._is_fitted() and self.warm_start):\n+                self._train_val_split_seed = rng.randint(1024)\n+\n             X_train, X_val, y_train, y_val = train_test_split(\n                 X, y, test_size=self.validation_fraction, stratify=stratify,\n-                random_state=rng)\n+                random_state=self._train_val_split_seed)\n         else:\n             X_train, y_train = X, y\n             X_val, y_val = None, None\n@@ -142,86 +153,127 @@ def fit(self, X, y):\n         if self.verbose:\n             print(\"Fitting gradient boosted rounds:\")\n \n-        # initialize raw_predictions: those are the accumulated values\n-        # predicted by the trees for the training data. raw_predictions has\n-        # shape (n_trees_per_iteration, n_samples) where\n-        # n_trees_per_iterations is n_classes in multiclass classification,\n-        # else 1.\n         n_samples = X_binned_train.shape[0]\n-        self._baseline_prediction = self.loss_.get_baseline_prediction(\n-            y_train, self.n_trees_per_iteration_\n-        )\n-        raw_predictions = np.zeros(\n-            shape=(self.n_trees_per_iteration_, n_samples),\n-            dtype=self._baseline_prediction.dtype\n-        )\n-        raw_predictions += self._baseline_prediction\n \n-        # initialize gradients and hessians (empty arrays).\n-        # shape = (n_trees_per_iteration, n_samples).\n-        gradients, hessians = self.loss_.init_gradients_and_hessians(\n-            n_samples=n_samples,\n-            prediction_dim=self.n_trees_per_iteration_\n-        )\n+        # First time calling fit, or no warm start\n+        if not (self._is_fitted() and self.warm_start):\n+            # Clear random state and score attributes\n+            self._clear_state()\n+\n+            # initialize raw_predictions: those are the accumulated values\n+            # predicted by the trees for the training data. raw_predictions has\n+            # shape (n_trees_per_iteration, n_samples) where\n+            # n_trees_per_iterations is n_classes in multiclass classification,\n+            # else 1.\n+            self._baseline_prediction = self.loss_.get_baseline_prediction(\n+                y_train, self.n_trees_per_iteration_\n+            )\n+            raw_predictions = np.zeros(\n+                shape=(self.n_trees_per_iteration_, n_samples),\n+                dtype=self._baseline_prediction.dtype\n+            )\n+            raw_predictions += self._baseline_prediction\n \n-        # predictors is a matrix (list of lists) of TreePredictor objects\n-        # with shape (n_iter_, n_trees_per_iteration)\n-        self._predictors = predictors = []\n+            # initialize gradients and hessians (empty arrays).\n+            # shape = (n_trees_per_iteration, n_samples).\n+            gradients, hessians = self.loss_.init_gradients_and_hessians(\n+                n_samples=n_samples,\n+                prediction_dim=self.n_trees_per_iteration_\n+            )\n \n-        # Initialize structures and attributes related to early stopping\n-        self.scorer_ = None  # set if scoring != loss\n-        raw_predictions_val = None  # set if scoring == loss and use val\n-        self.train_score_ = []\n-        self.validation_score_ = []\n-        if self.do_early_stopping_:\n-            # populate train_score and validation_score with the predictions\n-            # of the initial model (before the first tree)\n+            # predictors is a matrix (list of lists) of TreePredictor objects\n+            # with shape (n_iter_, n_trees_per_iteration)\n+            self._predictors = predictors = []\n \n-            if self.scoring == 'loss':\n-                # we're going to compute scoring w.r.t the loss. As losses\n-                # take raw predictions as input (unlike the scorers), we can\n-                # optimize a bit and avoid repeating computing the predictions\n-                # of the previous trees. We'll re-use raw_predictions (as it's\n-                # needed for training anyway) for evaluating the training\n-                # loss, and create raw_predictions_val for storing the\n-                # raw predictions of the validation data.\n-\n-                if self._use_validation_data:\n-                    raw_predictions_val = np.zeros(\n-                        shape=(self.n_trees_per_iteration_,\n-                               X_binned_val.shape[0]),\n-                        dtype=self._baseline_prediction.dtype\n-                    )\n+            # Initialize structures and attributes related to early stopping\n+            self.scorer_ = None  # set if scoring != loss\n+            raw_predictions_val = None  # set if scoring == loss and use val\n+            self.train_score_ = []\n+            self.validation_score_ = []\n+\n+            if self.do_early_stopping_:\n+                # populate train_score and validation_score with the\n+                # predictions of the initial model (before the first tree)\n \n-                    raw_predictions_val += self._baseline_prediction\n+                if self.scoring == 'loss':\n+                    # we're going to compute scoring w.r.t the loss. As losses\n+                    # take raw predictions as input (unlike the scorers), we\n+                    # can optimize a bit and avoid repeating computing the\n+                    # predictions of the previous trees. We'll re-use\n+                    # raw_predictions (as it's needed for training anyway) for\n+                    # evaluating the training loss, and create\n+                    # raw_predictions_val for storing the raw predictions of\n+                    # the validation data.\n \n-                self._check_early_stopping_loss(raw_predictions, y_train,\n-                                                raw_predictions_val, y_val)\n-            else:\n-                self.scorer_ = check_scoring(self, self.scoring)\n-                # scorer_ is a callable with signature (est, X, y) and calls\n-                # est.predict() or est.predict_proba() depending on its nature.\n-                # Unfortunately, each call to scorer_() will compute\n-                # the predictions of all the trees. So we use a subset of the\n-                # training set to compute train scores.\n-                subsample_size = 10000  # should we expose this parameter?\n-                indices = np.arange(X_binned_train.shape[0])\n-                if X_binned_train.shape[0] > subsample_size:\n-                    # TODO: not critical but stratify using resample()\n-                    indices = rng.choice(indices, subsample_size,\n-                                         replace=False)\n-                X_binned_small_train = X_binned_train[indices]\n-                y_small_train = y_train[indices]\n-                # Predicting is faster on C-contiguous arrays.\n-                X_binned_small_train = np.ascontiguousarray(\n-                    X_binned_small_train)\n-\n-                self._check_early_stopping_scorer(\n-                    X_binned_small_train, y_small_train,\n-                    X_binned_val, y_val,\n+                    if self._use_validation_data:\n+                        raw_predictions_val = np.zeros(\n+                            shape=(self.n_trees_per_iteration_,\n+                                   X_binned_val.shape[0]),\n+                            dtype=self._baseline_prediction.dtype\n+                        )\n+\n+                        raw_predictions_val += self._baseline_prediction\n+\n+                    self._check_early_stopping_loss(raw_predictions, y_train,\n+                                                    raw_predictions_val, y_val)\n+                else:\n+                    self.scorer_ = check_scoring(self, self.scoring)\n+                    # scorer_ is a callable with signature (est, X, y) and\n+                    # calls est.predict() or est.predict_proba() depending on\n+                    # its nature.\n+                    # Unfortunately, each call to scorer_() will compute\n+                    # the predictions of all the trees. So we use a subset of\n+                    # the training set to compute train scores.\n+\n+                    # Save the seed for the small trainset generator\n+                    self._small_trainset_seed = rng.randint(1024)\n+\n+                    # Compute the subsample set\n+                    (X_binned_small_train,\n+                     y_small_train) = self._get_small_trainset(\n+                        X_binned_train, y_train, self._small_trainset_seed)\n+\n+                    self._check_early_stopping_scorer(\n+                        X_binned_small_train, y_small_train,\n+                        X_binned_val, y_val,\n+                    )\n+            begin_at_stage = 0\n+\n+        # warm start: this is not the first time fit was called\n+        else:\n+            # Check that the maximum number of iterations is not smaller\n+            # than the number of iterations from the previous fit\n+            if self.max_iter < self.n_iter_:\n+                raise ValueError(\n+                    'max_iter=%d must be larger than or equal to '\n+                    'n_iter_=%d when warm_start==True'\n+                    % (self.max_iter, self.n_iter_)\n                 )\n \n-        for iteration in range(self.max_iter):\n+            # Convert array attributes to lists\n+            self.train_score_ = self.train_score_.tolist()\n+            self.validation_score_ = self.validation_score_.tolist()\n+\n+            # Compute raw predictions\n+            raw_predictions = self._raw_predict(X_binned_train)\n+\n+            if self.do_early_stopping_ and self.scoring != 'loss':\n+                # Compute the subsample set\n+                X_binned_small_train, y_small_train = self._get_small_trainset(\n+                    X_binned_train, y_train, self._small_trainset_seed)\n+\n+            # Initialize the gradients and hessians\n+            gradients, hessians = self.loss_.init_gradients_and_hessians(\n+                n_samples=n_samples,\n+                prediction_dim=self.n_trees_per_iteration_\n+            )\n+\n+            # Get the predictors from the previous fit\n+            predictors = self._predictors\n+\n+            begin_at_stage = self.n_iter_\n+\n+        for iteration in range(begin_at_stage, self.max_iter):\n \n             if self.verbose:\n                 iteration_start_time = time()\n@@ -318,13 +370,38 @@ def fit(self, X, y):\n         del self._in_fit  # hard delete so we're sure it can't be used anymore\n         return self\n \n+    def _is_fitted(self):\n+        return len(getattr(self, '_predictors', [])) > 0\n+\n+    def _clear_state(self):\n+        \"\"\"Clear the state of the gradient boosting model.\"\"\"\n+        for var in ('train_score_', 'validation_score_', '_rng'):\n+            if hasattr(self, var):\n+                delattr(self, var)\n+\n+    def _get_small_trainset(self, X_binned_train, y_train, seed):\n+        \"\"\"Compute the indices of the subsample set and return this set.\n+\n+        For efficiency, we need to subsample the training set to compute scores\n+        with scorers.\n+        \"\"\"\n+        subsample_size = 10000\n+        rng = check_random_state(seed)\n+        indices = np.arange(X_binned_train.shape[0])\n+        if X_binned_train.shape[0] > subsample_size:\n+            # TODO: not critical but stratify using resample()\n+            indices = rng.choice(indices, subsample_size, replace=False)\n+        X_binned_small_train = X_binned_train[indices]\n+        y_small_train = y_train[indices]\n+        X_binned_small_train = np.ascontiguousarray(X_binned_small_train)\n+        return X_binned_small_train, y_small_train\n+\n     def _check_early_stopping_scorer(self, X_binned_small_train, y_small_train,\n                                      X_binned_val, y_val):\n         \"\"\"Check if fitting should be early-stopped based on scorer.\n \n         Scores are computed on validation data or on training data.\n         \"\"\"\n-\n         self.train_score_.append(\n             self.scorer_(self, X_binned_small_train, y_small_train)\n         )\n@@ -555,6 +632,11 @@ class HistGradientBoostingRegressor(BaseHistGradientBoosting, RegressorMixin):\n         allows for a much faster training stage. Features with a small\n         number of unique values may use less than ``max_bins`` bins. Must be no\n         larger than 256.\n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble. For results to be valid, the\n+        estimator should be re-trained on the same data only.\n+        See :term:`the Glossary <warm_start>`.\n     scoring : str or callable or None, optional (default=None)\n         Scoring parameter to use for early stopping. It can be a single\n         string (see :ref:`scoring_parameter`) or a callable (see\n@@ -568,7 +650,7 @@ class HistGradientBoostingRegressor(BaseHistGradientBoosting, RegressorMixin):\n     n_iter_no_change : int or None, optional (default=None)\n         Used to determine when to \"early stop\". The fitting process is\n         stopped when none of the last ``n_iter_no_change`` scores are better\n-        than the ``n_iter_no_change - 1``th-to-last one, up to some\n+        than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n         tolerance. If None or 0, no early-stopping is done.\n     tol : float or None, optional (default=1e-7)\n         The absolute tolerance to use when comparing scores during early\n@@ -592,13 +674,13 @@ class HistGradientBoostingRegressor(BaseHistGradientBoosting, RegressorMixin):\n     n_trees_per_iteration_ : int\n         The number of tree that are built at each iteration. For regressors,\n         this is always 1.\n-    train_score_ : ndarray, shape (max_iter + 1,)\n+    train_score_ : ndarray, shape (n_iter_ + 1,)\n         The scores at each iteration on the training data. The first entry\n         is the score of the ensemble before the first iteration. Scores are\n         computed according to the ``scoring`` parameter. If ``scoring`` is\n         not 'loss', scores are computed on a subset of at most 10 000\n         samples. Empty if no early stopping.\n-    validation_score_ : ndarray, shape (max_iter + 1,)\n+    validation_score_ : ndarray, shape (n_iter_ + 1,)\n         The scores at each iteration on the held-out validation data. The\n         first entry is the score of the ensemble before the first iteration.\n         Scores are computed according to the ``scoring`` parameter. Empty if\n@@ -621,14 +703,16 @@ class HistGradientBoostingRegressor(BaseHistGradientBoosting, RegressorMixin):\n     def __init__(self, loss='least_squares', learning_rate=0.1,\n                  max_iter=100, max_leaf_nodes=31, max_depth=None,\n                  min_samples_leaf=20, l2_regularization=0., max_bins=256,\n-                 scoring=None, validation_fraction=0.1, n_iter_no_change=None,\n-                 tol=1e-7, verbose=0, random_state=None):\n+                 warm_start=False, scoring=None, validation_fraction=0.1,\n+                 n_iter_no_change=None, tol=1e-7, verbose=0,\n+                 random_state=None):\n         super(HistGradientBoostingRegressor, self).__init__(\n             loss=loss, learning_rate=learning_rate, max_iter=max_iter,\n             max_leaf_nodes=max_leaf_nodes, max_depth=max_depth,\n             min_samples_leaf=min_samples_leaf,\n             l2_regularization=l2_regularization, max_bins=max_bins,\n-            scoring=scoring, validation_fraction=validation_fraction,\n+            warm_start=warm_start, scoring=scoring,\n+            validation_fraction=validation_fraction,\n             n_iter_no_change=n_iter_no_change, tol=tol, verbose=verbose,\n             random_state=random_state)\n \n@@ -723,6 +807,11 @@ class HistGradientBoostingClassifier(BaseHistGradientBoosting,\n         allows for a much faster training stage. Features with a small\n         number of unique values may use less than ``max_bins`` bins. Must be no\n         larger than 256.\n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble. For results to be valid, the\n+        estimator should be re-trained on the same data only.\n+        See :term:`the Glossary <warm_start>`.\n     scoring : str or callable or None, optional (default=None)\n         Scoring parameter to use for early stopping. It can be a single\n         string (see :ref:`scoring_parameter`) or a callable (see\n@@ -736,7 +825,7 @@ class HistGradientBoostingClassifier(BaseHistGradientBoosting,\n     n_iter_no_change : int or None, optional (default=None)\n         Used to determine when to \"early stop\". The fitting process is\n         stopped when none of the last ``n_iter_no_change`` scores are better\n-        than the ``n_iter_no_change - 1``th-to-last one, up to some\n+        than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n         tolerance. If None or 0, no early-stopping is done.\n     tol : float or None, optional (default=1e-7)\n         The absolute tolerance to use when comparing scores. The higher the\n@@ -761,13 +850,13 @@ class HistGradientBoostingClassifier(BaseHistGradientBoosting,\n         The number of tree that are built at each iteration. This is equal to 1\n         for binary classification, and to ``n_classes`` for multiclass\n         classification.\n-    train_score_ : ndarray, shape (max_iter + 1,)\n+    train_score_ : ndarray, shape (n_iter_ + 1,)\n         The scores at each iteration on the training data. The first entry\n         is the score of the ensemble before the first iteration. Scores are\n         computed according to the ``scoring`` parameter. If ``scoring`` is\n         not 'loss', scores are computed on a subset of at most 10 000\n         samples. Empty if no early stopping.\n-    validation_score_ : ndarray, shape (max_iter + 1,)\n+    validation_score_ : ndarray, shape (n_iter_ + 1,)\n         The scores at each iteration on the held-out validation data. The\n         first entry is the score of the ensemble before the first iteration.\n         Scores are computed according to the ``scoring`` parameter. Empty if\n@@ -790,15 +879,16 @@ class HistGradientBoostingClassifier(BaseHistGradientBoosting,\n \n     def __init__(self, loss='auto', learning_rate=0.1, max_iter=100,\n                  max_leaf_nodes=31, max_depth=None, min_samples_leaf=20,\n-                 l2_regularization=0., max_bins=256, scoring=None,\n-                 validation_fraction=0.1, n_iter_no_change=None, tol=1e-7,\n-                 verbose=0, random_state=None):\n+                 l2_regularization=0., max_bins=256, warm_start=False,\n+                 scoring=None, validation_fraction=0.1, n_iter_no_change=None,\n+                 tol=1e-7, verbose=0, random_state=None):\n         super(HistGradientBoostingClassifier, self).__init__(\n             loss=loss, learning_rate=learning_rate, max_iter=max_iter,\n             max_leaf_nodes=max_leaf_nodes, max_depth=max_depth,\n             min_samples_leaf=min_samples_leaf,\n             l2_regularization=l2_regularization, max_bins=max_bins,\n-            scoring=scoring, validation_fraction=validation_fraction,\n+            warm_start=warm_start, scoring=scoring,\n+            validation_fraction=validation_fraction,\n             n_iter_no_change=n_iter_no_change, tol=tol, verbose=verbose,\n             random_state=random_state)\n \n",
  "test_patch": "diff --git a/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py b/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py\nnew file mode 100644\n--- /dev/null\n+++ b/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py\n@@ -0,0 +1,190 @@\n+import numpy as np\n+from numpy.testing import assert_array_equal\n+from numpy.testing import assert_allclose\n+\n+import pytest\n+\n+from sklearn.base import clone\n+from sklearn.datasets import make_classification, make_regression\n+\n+# To use this experimental feature, we need to explicitly ask for it:\n+from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n+from sklearn.ensemble import HistGradientBoostingRegressor\n+from sklearn.ensemble import HistGradientBoostingClassifier\n+\n+\n+X_classification, y_classification = make_classification(random_state=0)\n+X_regression, y_regression = make_regression(random_state=0)\n+\n+\n+def _assert_predictor_equal(gb_1, gb_2, X):\n+    \"\"\"Assert that two HistGBM instances are identical.\"\"\"\n+    # Check identical nodes for each tree\n+    for (pred_ith_1, pred_ith_2) in zip(gb_1._predictors, gb_2._predictors):\n+        for (predictor_1, predictor_2) in zip(pred_ith_1, pred_ith_2):\n+            assert_array_equal(predictor_1.nodes, predictor_2.nodes)\n+\n+    # Check identical predictions\n+    assert_allclose(gb_1.predict(X), gb_2.predict(X))\n+\n+\n+@pytest.mark.parametrize('GradientBoosting, X, y', [\n+    (HistGradientBoostingClassifier, X_classification, y_classification),\n+    (HistGradientBoostingRegressor, X_regression, y_regression)\n+])\n+def test_max_iter_with_warm_start_validation(GradientBoosting, X, y):\n+    # Check that a ValueError is raised when the maximum number of iterations\n+    # is smaller than the number of iterations from the previous fit when warm\n+    # start is True.\n+\n+    estimator = GradientBoosting(max_iter=50, warm_start=True)\n+    estimator.fit(X, y)\n+    estimator.set_params(max_iter=25)\n+    err_msg = ('max_iter=25 must be larger than or equal to n_iter_=50 '\n+               'when warm_start==True')\n+    with pytest.raises(ValueError, match=err_msg):\n+        estimator.fit(X, y)\n+\n+\n+@pytest.mark.parametrize('GradientBoosting, X, y', [\n+    (HistGradientBoostingClassifier, X_classification, y_classification),\n+    (HistGradientBoostingRegressor, X_regression, y_regression)\n+])\n+def test_warm_start_yields_identical_results(GradientBoosting, X, y):\n+    # Make sure that fitting 50 iterations and then 25 with warm start is\n+    # equivalent to fitting 75 iterations.\n+\n+    rng = 42\n+    gb_warm_start = GradientBoosting(\n+        n_iter_no_change=100, max_iter=50, random_state=rng, warm_start=True\n+    )\n+    gb_warm_start.fit(X, y).set_params(max_iter=75).fit(X, y)\n+\n+    gb_no_warm_start = GradientBoosting(\n+        n_iter_no_change=100, max_iter=75, random_state=rng, warm_start=False\n+    )\n+    gb_no_warm_start.fit(X, y)\n+\n+    # Check that both predictors are equal\n+    _assert_predictor_equal(gb_warm_start, gb_no_warm_start, X)\n+\n+\n+@pytest.mark.parametrize('GradientBoosting, X, y', [\n+    (HistGradientBoostingClassifier, X_classification, y_classification),\n+    (HistGradientBoostingRegressor, X_regression, y_regression)\n+])\n+def test_warm_start_max_depth(GradientBoosting, X, y):\n+    # Test if possible to fit trees of different depth in ensemble.\n+    gb = GradientBoosting(max_iter=100, min_samples_leaf=1,\n+                          warm_start=True, max_depth=2)\n+    gb.fit(X, y)\n+    gb.set_params(max_iter=110, max_depth=3)\n+    gb.fit(X, y)\n+\n+    # First 100 trees have max_depth == 2\n+    for i in range(100):\n+        assert gb._predictors[i][0].get_max_depth() == 2\n+    # Last 10 trees have max_depth == 3\n+    for i in range(1, 11):\n+        assert gb._predictors[-i][0].get_max_depth() == 3\n+\n+\n+@pytest.mark.parametrize('GradientBoosting, X, y', [\n+    (HistGradientBoostingClassifier, X_classification, y_classification),\n+    (HistGradientBoostingRegressor, X_regression, y_regression)\n+])\n+def test_warm_start_early_stopping(GradientBoosting, X, y):\n+    # Make sure that early stopping occurs after a small number of iterations\n+    # when fitting a second time with warm starting.\n+\n+    n_iter_no_change = 5\n+    gb = GradientBoosting(\n+        n_iter_no_change=n_iter_no_change, max_iter=10000,\n+        random_state=42, warm_start=True, tol=1e-3\n+    )\n+    gb.fit(X, y)\n+    n_iter_first_fit = gb.n_iter_\n+    gb.fit(X, y)\n+    n_iter_second_fit = gb.n_iter_\n+    assert n_iter_second_fit - n_iter_first_fit < n_iter_no_change\n+\n+\n+@pytest.mark.parametrize('GradientBoosting, X, y', [\n+    (HistGradientBoostingClassifier, X_classification, y_classification),\n+    (HistGradientBoostingRegressor, X_regression, y_regression)\n+])\n+def test_warm_start_equal_n_estimators(GradientBoosting, X, y):\n+    # Test if warm start with equal n_estimators does nothing\n+    gb_1 = GradientBoosting(max_depth=2)\n+    gb_1.fit(X, y)\n+\n+    gb_2 = clone(gb_1)\n+    gb_2.set_params(max_iter=gb_1.max_iter, warm_start=True)\n+    gb_2.fit(X, y)\n+\n+    # Check that both predictors are equal\n+    _assert_predictor_equal(gb_1, gb_2, X)\n+\n+\n+@pytest.mark.parametrize('GradientBoosting, X, y', [\n+    (HistGradientBoostingClassifier, X_classification, y_classification),\n+    (HistGradientBoostingRegressor, X_regression, y_regression)\n+])\n+def test_warm_start_clear(GradientBoosting, X, y):\n+    # Test if fit clears state.\n+    gb_1 = GradientBoosting(n_iter_no_change=5, random_state=42)\n+    gb_1.fit(X, y)\n+\n+    gb_2 = GradientBoosting(n_iter_no_change=5, random_state=42,\n+                            warm_start=True)\n+    gb_2.fit(X, y)  # inits state\n+    gb_2.set_params(warm_start=False)\n+    gb_2.fit(X, y)  # clears old state and equals est\n+\n+    # Check that both predictors have the same train_score_ and\n+    # validation_score_ attributes\n+    assert_allclose(gb_1.train_score_, gb_2.train_score_)\n+    assert_allclose(gb_1.validation_score_, gb_2.validation_score_)\n+\n+    # Check that both predictors are equal\n+    _assert_predictor_equal(gb_1, gb_2, X)\n+\n+\n+@pytest.mark.parametrize('GradientBoosting, X, y', [\n+    (HistGradientBoostingClassifier, X_classification, y_classification),\n+    (HistGradientBoostingRegressor, X_regression, y_regression)\n+])\n+@pytest.mark.parametrize('rng_type', ('int', 'instance'))\n+def test_random_seeds_warm_start(GradientBoosting, X, y, rng_type):\n+    # Make sure the seeds for train/val split and small trainset subsampling\n+    # are correctly set in a warm start context.\n+    def _get_rng(rng_type):\n+        # Helper to avoid consuming rngs\n+        if rng_type == 'int':\n+            return 42\n+        else:\n+            return np.random.RandomState(0)\n+\n+    random_state = _get_rng(rng_type)\n+    gb_1 = GradientBoosting(n_iter_no_change=5, max_iter=2,\n+                            random_state=random_state)\n+    gb_1.fit(X, y)\n+    train_val_seed_1 = gb_1._train_val_split_seed\n+    small_trainset_seed_1 = gb_1._small_trainset_seed\n+\n+    random_state = _get_rng(rng_type)\n+    gb_2 = GradientBoosting(n_iter_no_change=5, max_iter=2,\n+                            random_state=random_state, warm_start=True)\n+    gb_2.fit(X, y)  # inits state\n+    train_val_seed_2 = gb_2._train_val_split_seed\n+    small_trainset_seed_2 = gb_2._small_trainset_seed\n+    gb_2.fit(X, y)  # clears old state and equals est\n+    train_val_seed_3 = gb_2._train_val_split_seed\n+    small_trainset_seed_3 = gb_2._small_trainset_seed\n+\n+    # Check that all seeds are equal\n+    assert train_val_seed_1 == train_val_seed_2\n+    assert small_trainset_seed_1 == small_trainset_seed_2\n+\n+    assert train_val_seed_2 == train_val_seed_3\n+    assert small_trainset_seed_2 == small_trainset_seed_3\n",
  "problem_statement": "Feature request: warm starting for histogram-based GBM\n#### Description\r\nThis is a feature request to add the warm start parameter, which exists for [gradient boosting](https://scikit-learn.org/dev/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier), to the new [histogram-based gradient boosting](https://scikit-learn.org/dev/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier).\r\n\r\nRationale: We're using gradient boosting in [Auto-sklearn](https://automl.github.io/auto-sklearn/master/), and fit each model until either the time (given by the user) is up, or the number of trees is reached. For sufficiently large datasets, it is possible that certain configurations time out. Therefore, if possible, we train models iteratively, and only add more trees if time allows. Since the new GBM implementation is really great (not only faster, but also better default accuracy for the problems I tried) we would like to use it within Auto-sklearn as a drop-in, ideally together with iterative training.\n",
  "hints_text": "This is on my TODO list, but I don't know yet when I'll start working on this.\r\n\r\nIf anyone wants to give it a try I'll be happy to provide review and/or guidance.\n@mfeurer thanks for the input!\r\n@NicolasHug I think this would be great to prioritize. Shouldn't be too hard, right?\nHonestly I'm happy to work on it but I've been flooding with a lot of PRs recently (still awaiting reviews) and I feel it'd be more helpful to the project if I go back in reviewing mode for a little while.\r\n\r\nIt shouldn't be too hard for someone to pick it up.\r\n\r\nIf I'm not the one writing it, we only need one more reviewer instead of 2.\n@NicolasHug I agree re PRs but I feel this one would be relatively small compared to the rest? I might even be able to review it ;)\r\nAlso: this ties in with successive halving nicely!\nIf not one picks it up within a week I'll get started then :)\nI'll work on this one, though will probably need some guidance along the way @NicolasHug. \nCool. A good starting point is to look at how this is implemented in `ensemble/gradient_boosting.py`, and start translating it to the new implementation.\n@NicolasHug I just realised that this issues is still beyond my current skill level. Looking forward to seeing and learning from the PR though. \nThanks for letting us know, Matts\n\nI can try to work on this if you did not change your mind @NicolasHug .\nCool go ahead!",
  "created_at": "2019-06-03T15:16:58Z",
  "version": "0.22",
  "FAIL_TO_PASS": "[\"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_max_iter_with_warm_start_validation[HistGradientBoostingClassifier-X0-y0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_max_iter_with_warm_start_validation[HistGradientBoostingRegressor-X1-y1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_yields_identical_results[HistGradientBoostingClassifier-X0-y0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_yields_identical_results[HistGradientBoostingRegressor-X1-y1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_max_depth[HistGradientBoostingClassifier-X0-y0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_max_depth[HistGradientBoostingRegressor-X1-y1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_early_stopping[HistGradientBoostingClassifier-X0-y0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_early_stopping[HistGradientBoostingRegressor-X1-y1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_equal_n_estimators[HistGradientBoostingClassifier-X0-y0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_equal_n_estimators[HistGradientBoostingRegressor-X1-y1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_clear[HistGradientBoostingClassifier-X0-y0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_warm_start_clear[HistGradientBoostingRegressor-X1-y1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_random_seeds_warm_start[int-HistGradientBoostingClassifier-X0-y0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_random_seeds_warm_start[int-HistGradientBoostingRegressor-X1-y1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_random_seeds_warm_start[instance-HistGradientBoostingClassifier-X0-y0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py::test_random_seeds_warm_start[instance-HistGradientBoostingRegressor-X1-y1]\"]",
  "PASS_TO_PASS": "[]",
  "environment_setup_commit": "7e85a6d1f038bbb932b36f18d75df6be937ed00d",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:31.001215",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}