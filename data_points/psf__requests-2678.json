{
  "repo": "psf/requests",
  "instance_id": "psf__requests-2678",
  "base_commit": "276202f51ee9967969eafc1880c4785c80d63d3b",
  "patch": "diff --git a/requests/adapters.py b/requests/adapters.py\n--- a/requests/adapters.py\n+++ b/requests/adapters.py\n@@ -19,6 +19,7 @@\n from .utils import (DEFAULT_CA_BUNDLE_PATH, get_encoding_from_headers,\n                     prepend_scheme_if_needed, get_auth_from_url, urldefragauth)\n from .structures import CaseInsensitiveDict\n+from .packages.urllib3.exceptions import ClosedPoolError\n from .packages.urllib3.exceptions import ConnectTimeoutError\n from .packages.urllib3.exceptions import HTTPError as _HTTPError\n from .packages.urllib3.exceptions import MaxRetryError\n@@ -421,6 +422,9 @@ def send(self, request, stream=False, timeout=None, verify=True, cert=None, prox\n \n             raise ConnectionError(e, request=request)\n \n+        except ClosedPoolError as e:\n+            raise ConnectionError(e, request=request)\n+\n         except _ProxyError as e:\n             raise ProxyError(e)\n \ndiff --git a/requests/auth.py b/requests/auth.py\n--- a/requests/auth.py\n+++ b/requests/auth.py\n@@ -11,6 +11,7 @@\n import re\n import time\n import hashlib\n+import threading\n \n from base64 import b64encode\n \n@@ -63,19 +64,26 @@ class HTTPDigestAuth(AuthBase):\n     def __init__(self, username, password):\n         self.username = username\n         self.password = password\n-        self.last_nonce = ''\n-        self.nonce_count = 0\n-        self.chal = {}\n-        self.pos = None\n-        self.num_401_calls = 1\n+        # Keep state in per-thread local storage\n+        self._thread_local = threading.local()\n+\n+    def init_per_thread_state(self):\n+        # Ensure state is initialized just once per-thread\n+        if not hasattr(self._thread_local, 'init'):\n+            self._thread_local.init = True\n+            self._thread_local.last_nonce = ''\n+            self._thread_local.nonce_count = 0\n+            self._thread_local.chal = {}\n+            self._thread_local.pos = None\n+            self._thread_local.num_401_calls = None\n \n     def build_digest_header(self, method, url):\n \n-        realm = self.chal['realm']\n-        nonce = self.chal['nonce']\n-        qop = self.chal.get('qop')\n-        algorithm = self.chal.get('algorithm')\n-        opaque = self.chal.get('opaque')\n+        realm = self._thread_local.chal['realm']\n+        nonce = self._thread_local.chal['nonce']\n+        qop = self._thread_local.chal.get('qop')\n+        algorithm = self._thread_local.chal.get('algorithm')\n+        opaque = self._thread_local.chal.get('opaque')\n \n         if algorithm is None:\n             _algorithm = 'MD5'\n@@ -114,12 +122,12 @@ def sha_utf8(x):\n         HA1 = hash_utf8(A1)\n         HA2 = hash_utf8(A2)\n \n-        if nonce == self.last_nonce:\n-            self.nonce_count += 1\n+        if nonce == self._thread_local.last_nonce:\n+            self._thread_local.nonce_count += 1\n         else:\n-            self.nonce_count = 1\n-        ncvalue = '%08x' % self.nonce_count\n-        s = str(self.nonce_count).encode('utf-8')\n+            self._thread_local.nonce_count = 1\n+        ncvalue = '%08x' % self._thread_local.nonce_count\n+        s = str(self._thread_local.nonce_count).encode('utf-8')\n         s += nonce.encode('utf-8')\n         s += time.ctime().encode('utf-8')\n         s += os.urandom(8)\n@@ -139,7 +147,7 @@ def sha_utf8(x):\n             # XXX handle auth-int.\n             return None\n \n-        self.last_nonce = nonce\n+        self._thread_local.last_nonce = nonce\n \n         # XXX should the partial digests be encoded too?\n         base = 'username=\"%s\", realm=\"%s\", nonce=\"%s\", uri=\"%s\", ' \\\n@@ -158,23 +166,22 @@ def sha_utf8(x):\n     def handle_redirect(self, r, **kwargs):\n         \"\"\"Reset num_401_calls counter on redirects.\"\"\"\n         if r.is_redirect:\n-            self.num_401_calls = 1\n+            self._thread_local.num_401_calls = 1\n \n     def handle_401(self, r, **kwargs):\n         \"\"\"Takes the given response and tries digest-auth, if needed.\"\"\"\n \n-        if self.pos is not None:\n+        if self._thread_local.pos is not None:\n             # Rewind the file position indicator of the body to where\n             # it was to resend the request.\n-            r.request.body.seek(self.pos)\n-        num_401_calls = getattr(self, 'num_401_calls', 1)\n+            r.request.body.seek(self._thread_local.pos)\n         s_auth = r.headers.get('www-authenticate', '')\n \n-        if 'digest' in s_auth.lower() and num_401_calls < 2:\n+        if 'digest' in s_auth.lower() and self._thread_local.num_401_calls < 2:\n \n-            self.num_401_calls += 1\n+            self._thread_local.num_401_calls += 1\n             pat = re.compile(r'digest ', flags=re.IGNORECASE)\n-            self.chal = parse_dict_header(pat.sub('', s_auth, count=1))\n+            self._thread_local.chal = parse_dict_header(pat.sub('', s_auth, count=1))\n \n             # Consume content and release the original connection\n             # to allow our new request to reuse the same one.\n@@ -192,21 +199,25 @@ def handle_401(self, r, **kwargs):\n \n             return _r\n \n-        self.num_401_calls = 1\n+        self._thread_local.num_401_calls = 1\n         return r\n \n     def __call__(self, r):\n+        # Initialize per-thread state, if needed\n+        self.init_per_thread_state()\n         # If we have a saved nonce, skip the 401\n-        if self.last_nonce:\n+        if self._thread_local.last_nonce:\n             r.headers['Authorization'] = self.build_digest_header(r.method, r.url)\n         try:\n-            self.pos = r.body.tell()\n+            self._thread_local.pos = r.body.tell()\n         except AttributeError:\n             # In the case of HTTPDigestAuth being reused and the body of\n             # the previous request was a file-like object, pos has the\n             # file position of the previous body. Ensure it's set to\n             # None.\n-            self.pos = None\n+            self._thread_local.pos = None\n         r.register_hook('response', self.handle_401)\n         r.register_hook('response', self.handle_redirect)\n+        self._thread_local.num_401_calls = 1\n+\n         return r\ndiff --git a/requests/packages/__init__.py b/requests/packages/__init__.py\n--- a/requests/packages/__init__.py\n+++ b/requests/packages/__init__.py\n@@ -1,3 +1,36 @@\n+'''\n+Debian and other distributions \"unbundle\" requests' vendored dependencies, and\n+rewrite all imports to use the global versions of ``urllib3`` and ``chardet``.\n+The problem with this is that not only requests itself imports those\n+dependencies, but third-party code outside of the distros' control too.\n+\n+In reaction to these problems, the distro maintainers replaced\n+``requests.packages`` with a magical \"stub module\" that imports the correct\n+modules. The implementations were varying in quality and all had severe\n+problems. For example, a symlink (or hardlink) that links the correct modules\n+into place introduces problems regarding object identity, since you now have\n+two modules in `sys.modules` with the same API, but different identities::\n+\n+    requests.packages.urllib3 is not urllib3\n+\n+With version ``2.5.2``, requests started to maintain its own stub, so that\n+distro-specific breakage would be reduced to a minimum, even though the whole\n+issue is not requests' fault in the first place. See\n+https://github.com/kennethreitz/requests/pull/2375 for the corresponding pull\n+request.\n+'''\n+\n from __future__ import absolute_import\n+import sys\n+\n+try:\n+    from . import urllib3\n+except ImportError:\n+    import urllib3\n+    sys.modules['%s.urllib3' % __name__] = urllib3\n \n-from . import urllib3\n+try:\n+    from . import chardet\n+except ImportError:\n+    import chardet\n+    sys.modules['%s.chardet' % __name__] = chardet\ndiff --git a/requests/packages/urllib3/__init__.py b/requests/packages/urllib3/__init__.py\n--- a/requests/packages/urllib3/__init__.py\n+++ b/requests/packages/urllib3/__init__.py\n@@ -4,7 +4,7 @@\n \n __author__ = 'Andrey Petrov (andrey.petrov@shazow.net)'\n __license__ = 'MIT'\n-__version__ = '1.10.4'\n+__version__ = 'dev'\n \n \n from .connectionpool import (\n@@ -58,6 +58,8 @@ def add_stderr_logger(level=logging.DEBUG):\n import warnings\n # SecurityWarning's always go off by default.\n warnings.simplefilter('always', exceptions.SecurityWarning, append=True)\n+# SubjectAltNameWarning's should go off once per host\n+warnings.simplefilter('default', exceptions.SubjectAltNameWarning)\n # InsecurePlatformWarning's don't vary between requests, so we keep it default.\n warnings.simplefilter('default', exceptions.InsecurePlatformWarning,\n                       append=True)\ndiff --git a/requests/packages/urllib3/connection.py b/requests/packages/urllib3/connection.py\n--- a/requests/packages/urllib3/connection.py\n+++ b/requests/packages/urllib3/connection.py\n@@ -38,7 +38,7 @@ class ConnectionError(Exception):\n from .exceptions import (\n     ConnectTimeoutError,\n     SystemTimeWarning,\n-    SecurityWarning,\n+    SubjectAltNameWarning,\n )\n from .packages.ssl_match_hostname import match_hostname\n \n@@ -192,6 +192,9 @@ def set_cert(self, key_file=None, cert_file=None,\n                  cert_reqs=None, ca_certs=None,\n                  assert_hostname=None, assert_fingerprint=None):\n \n+        if ca_certs and cert_reqs is None:\n+            cert_reqs = 'CERT_REQUIRED'\n+\n         self.key_file = key_file\n         self.cert_file = cert_file\n         self.cert_reqs = cert_reqs\n@@ -245,10 +248,11 @@ def connect(self):\n             cert = self.sock.getpeercert()\n             if not cert.get('subjectAltName', ()):\n                 warnings.warn((\n-                    'Certificate has no `subjectAltName`, falling back to check for a `commonName` for now. '\n-                    'This feature is being removed by major browsers and deprecated by RFC 2818. '\n-                    '(See https://github.com/shazow/urllib3/issues/497 for details.)'),\n-                    SecurityWarning\n+                    'Certificate for {0} has no `subjectAltName`, falling back to check for a '\n+                    '`commonName` for now. This feature is being removed by major browsers and '\n+                    'deprecated by RFC 2818. (See https://github.com/shazow/urllib3/issues/497 '\n+                    'for details.)'.format(hostname)),\n+                    SubjectAltNameWarning\n                 )\n             match_hostname(cert, self.assert_hostname or hostname)\n \ndiff --git a/requests/packages/urllib3/connectionpool.py b/requests/packages/urllib3/connectionpool.py\n--- a/requests/packages/urllib3/connectionpool.py\n+++ b/requests/packages/urllib3/connectionpool.py\n@@ -17,6 +17,7 @@\n     ClosedPoolError,\n     ProtocolError,\n     EmptyPoolError,\n+    HeaderParsingError,\n     HostChangedError,\n     LocationValueError,\n     MaxRetryError,\n@@ -38,9 +39,10 @@\n from .response import HTTPResponse\n \n from .util.connection import is_connection_dropped\n+from .util.response import assert_header_parsing\n from .util.retry import Retry\n from .util.timeout import Timeout\n-from .util.url import get_host\n+from .util.url import get_host, Url\n \n \n xrange = six.moves.xrange\n@@ -120,7 +122,7 @@ class HTTPConnectionPool(ConnectionPool, RequestMethods):\n \n     :param maxsize:\n         Number of connections to save that can be reused. More than 1 is useful\n-        in multithreaded situations. If ``block`` is set to false, more\n+        in multithreaded situations. If ``block`` is set to False, more\n         connections will be created but they will not be saved once they've\n         been used.\n \n@@ -381,8 +383,19 @@ def _make_request(self, conn, method, url, timeout=_Default,\n         log.debug(\"\\\"%s %s %s\\\" %s %s\" % (method, url, http_version,\n                                           httplib_response.status,\n                                           httplib_response.length))\n+\n+        try:\n+            assert_header_parsing(httplib_response.msg)\n+        except HeaderParsingError as hpe:  # Platform-specific: Python 3\n+            log.warning(\n+                'Failed to parse headers (url=%s): %s',\n+                self._absolute_url(url), hpe, exc_info=True)\n+\n         return httplib_response\n \n+    def _absolute_url(self, path):\n+        return Url(scheme=self.scheme, host=self.host, port=self.port, path=path).url\n+\n     def close(self):\n         \"\"\"\n         Close all pooled connections and disable the pool.\n@@ -409,7 +422,7 @@ def is_same_host(self, url):\n \n         # TODO: Add optional support for socket.gethostbyname checking.\n         scheme, host, port = get_host(url)\n-\n+ \n         # Use explicit default port for comparison when none is given\n         if self.port and not port:\n             port = port_by_scheme.get(scheme)\n@@ -568,25 +581,22 @@ def urlopen(self, method, url, body=None, headers=None, retries=None,\n             # Close the connection. If a connection is reused on which there\n             # was a Certificate error, the next request will certainly raise\n             # another Certificate error.\n-            if conn:\n-                conn.close()\n-                conn = None\n+            conn = conn and conn.close()\n+            release_conn = True\n             raise SSLError(e)\n \n         except SSLError:\n             # Treat SSLError separately from BaseSSLError to preserve\n             # traceback.\n-            if conn:\n-                conn.close()\n-                conn = None\n+            conn = conn and conn.close()\n+            release_conn = True\n             raise\n \n         except (TimeoutError, HTTPException, SocketError, ConnectionError) as e:\n-            if conn:\n-                # Discard the connection for these exceptions. It will be\n-                # be replaced during the next _get_conn() call.\n-                conn.close()\n-                conn = None\n+            # Discard the connection for these exceptions. It will be\n+            # be replaced during the next _get_conn() call.\n+            conn = conn and conn.close()\n+            release_conn = True\n \n             if isinstance(e, SocketError) and self.proxy:\n                 e = ProxyError('Cannot connect to proxy.', e)\n@@ -626,6 +636,9 @@ def urlopen(self, method, url, body=None, headers=None, retries=None,\n                 retries = retries.increment(method, url, response=response, _pool=self)\n             except MaxRetryError:\n                 if retries.raise_on_redirect:\n+                    # Release the connection for this response, since we're not\n+                    # returning it to be released manually.\n+                    response.release_conn()\n                     raise\n                 return response\n \n@@ -683,6 +696,10 @@ def __init__(self, host, port=None,\n         HTTPConnectionPool.__init__(self, host, port, strict, timeout, maxsize,\n                                     block, headers, retries, _proxy, _proxy_headers,\n                                     **conn_kw)\n+\n+        if ca_certs and cert_reqs is None:\n+            cert_reqs = 'CERT_REQUIRED'\n+\n         self.key_file = key_file\n         self.cert_file = cert_file\n         self.cert_reqs = cert_reqs\ndiff --git a/requests/packages/urllib3/contrib/appengine.py b/requests/packages/urllib3/contrib/appengine.py\nnew file mode 100644\n--- /dev/null\n+++ b/requests/packages/urllib3/contrib/appengine.py\n@@ -0,0 +1,222 @@\n+import logging\n+import os\n+import warnings\n+\n+from ..exceptions import (\n+    HTTPError,\n+    HTTPWarning,\n+    MaxRetryError,\n+    ProtocolError,\n+    TimeoutError,\n+    SSLError\n+)\n+\n+from ..packages.six import BytesIO\n+from ..request import RequestMethods\n+from ..response import HTTPResponse\n+from ..util.timeout import Timeout\n+from ..util.retry import Retry\n+\n+try:\n+    from google.appengine.api import urlfetch\n+except ImportError:\n+    urlfetch = None\n+\n+\n+log = logging.getLogger(__name__)\n+\n+\n+class AppEnginePlatformWarning(HTTPWarning):\n+    pass\n+\n+\n+class AppEnginePlatformError(HTTPError):\n+    pass\n+\n+\n+class AppEngineManager(RequestMethods):\n+    \"\"\"\n+    Connection manager for Google App Engine sandbox applications.\n+\n+    This manager uses the URLFetch service directly instead of using the\n+    emulated httplib, and is subject to URLFetch limitations as described in\n+    the App Engine documentation here:\n+\n+        https://cloud.google.com/appengine/docs/python/urlfetch\n+\n+    Notably it will raise an AppEnginePlatformError if:\n+        * URLFetch is not available.\n+        * If you attempt to use this on GAEv2 (Managed VMs), as full socket\n+          support is available.\n+        * If a request size is more than 10 megabytes.\n+        * If a response size is more than 32 megabtyes.\n+        * If you use an unsupported request method such as OPTIONS.\n+\n+    Beyond those cases, it will raise normal urllib3 errors.\n+    \"\"\"\n+\n+    def __init__(self, headers=None, retries=None, validate_certificate=True):\n+        if not urlfetch:\n+            raise AppEnginePlatformError(\n+                \"URLFetch is not available in this environment.\")\n+\n+        if is_prod_appengine_v2():\n+            raise AppEnginePlatformError(\n+                \"Use normal urllib3.PoolManager instead of AppEngineManager\"\n+                \"on Managed VMs, as using URLFetch is not necessary in \"\n+                \"this environment.\")\n+\n+        warnings.warn(\n+            \"urllib3 is using URLFetch on Google App Engine sandbox instead \"\n+            \"of sockets. To use sockets directly instead of URLFetch see \"\n+            \"https://urllib3.readthedocs.org/en/latest/contrib.html.\",\n+            AppEnginePlatformWarning)\n+\n+        RequestMethods.__init__(self, headers)\n+        self.validate_certificate = validate_certificate\n+\n+        self.retries = retries or Retry.DEFAULT\n+\n+    def __enter__(self):\n+        return self\n+\n+    def __exit__(self, exc_type, exc_val, exc_tb):\n+        # Return False to re-raise any potential exceptions\n+        return False\n+\n+    def urlopen(self, method, url, body=None, headers=None,\n+                retries=None, redirect=True, timeout=Timeout.DEFAULT_TIMEOUT,\n+                **response_kw):\n+\n+        retries = self._get_retries(retries, redirect)\n+\n+        try:\n+            response = urlfetch.fetch(\n+                url,\n+                payload=body,\n+                method=method,\n+                headers=headers or {},\n+                allow_truncated=False,\n+                follow_redirects=(\n+                    redirect and\n+                    retries.redirect != 0 and\n+                    retries.total),\n+                deadline=self._get_absolute_timeout(timeout),\n+                validate_certificate=self.validate_certificate,\n+            )\n+        except urlfetch.DeadlineExceededError as e:\n+            raise TimeoutError(self, e)\n+\n+        except urlfetch.InvalidURLError as e:\n+            if 'too large' in e.message:\n+                raise AppEnginePlatformError(\n+                    \"URLFetch request too large, URLFetch only \"\n+                    \"supports requests up to 10mb in size.\", e)\n+            raise ProtocolError(e)\n+\n+        except urlfetch.DownloadError as e:\n+            if 'Too many redirects' in e.message:\n+                raise MaxRetryError(self, url, reason=e)\n+            raise ProtocolError(e)\n+\n+        except urlfetch.ResponseTooLargeError as e:\n+            raise AppEnginePlatformError(\n+                \"URLFetch response too large, URLFetch only supports\"\n+                \"responses up to 32mb in size.\", e)\n+\n+        except urlfetch.SSLCertificateError as e:\n+            raise SSLError(e)\n+\n+        except urlfetch.InvalidMethodError as e:\n+            raise AppEnginePlatformError(\n+                \"URLFetch does not support method: %s\" % method, e)\n+\n+        http_response = self._urlfetch_response_to_http_response(\n+            response, **response_kw)\n+\n+        # Check for redirect response\n+        if (http_response.get_redirect_location() and\n+                retries.raise_on_redirect and redirect):\n+            raise MaxRetryError(self, url, \"too many redirects\")\n+\n+        # Check if we should retry the HTTP response.\n+        if retries.is_forced_retry(method, status_code=http_response.status):\n+            retries = retries.increment(\n+                method, url, response=http_response, _pool=self)\n+            log.info(\"Forced retry: %s\" % url)\n+            retries.sleep()\n+            return self.urlopen(\n+                method, url,\n+                body=body, headers=headers,\n+                retries=retries, redirect=redirect,\n+                timeout=timeout, **response_kw)\n+\n+        return http_response\n+\n+    def _urlfetch_response_to_http_response(self, urlfetch_resp, **response_kw):\n+\n+        if is_prod_appengine_v1():\n+            # Production GAE handles deflate encoding automatically, but does\n+            # not remove the encoding header.\n+            content_encoding = urlfetch_resp.headers.get('content-encoding')\n+\n+            if content_encoding == 'deflate':\n+                del urlfetch_resp.headers['content-encoding']\n+\n+        return HTTPResponse(\n+            # In order for decoding to work, we must present the content as\n+            # a file-like object.\n+            body=BytesIO(urlfetch_resp.content),\n+            headers=urlfetch_resp.headers,\n+            status=urlfetch_resp.status_code,\n+            **response_kw\n+        )\n+\n+    def _get_absolute_timeout(self, timeout):\n+        if timeout is Timeout.DEFAULT_TIMEOUT:\n+            return 5  # 5s is the default timeout for URLFetch.\n+        if isinstance(timeout, Timeout):\n+            if not timeout.read is timeout.connect:\n+                warnings.warn(\n+                    \"URLFetch does not support granular timeout settings, \"\n+                    \"reverting to total timeout.\", AppEnginePlatformWarning)\n+            return timeout.total\n+        return timeout\n+\n+    def _get_retries(self, retries, redirect):\n+        if not isinstance(retries, Retry):\n+            retries = Retry.from_int(\n+                retries, redirect=redirect, default=self.retries)\n+\n+        if retries.connect or retries.read or retries.redirect:\n+            warnings.warn(\n+                \"URLFetch only supports total retries and does not \"\n+                \"recognize connect, read, or redirect retry parameters.\",\n+                AppEnginePlatformWarning)\n+\n+        return retries\n+\n+\n+def is_appengine():\n+    return (is_local_appengine() or\n+            is_prod_appengine_v1() or\n+            is_prod_appengine_v2())\n+\n+\n+def is_appengine_sandbox():\n+    return is_appengine() and not is_prod_appengine_v2()\n+\n+\n+def is_local_appengine():\n+    return ('APPENGINE_RUNTIME' in os.environ and\n+            'Development/' in os.environ['SERVER_SOFTWARE'])\n+\n+\n+def is_prod_appengine_v1():\n+    return ('APPENGINE_RUNTIME' in os.environ and\n+            'Google App Engine/' in os.environ['SERVER_SOFTWARE'] and\n+            not is_prod_appengine_v2())\n+\n+\n+def is_prod_appengine_v2():\n+    return os.environ.get('GAE_VM', False) == 'true'\ndiff --git a/requests/packages/urllib3/contrib/pyopenssl.py b/requests/packages/urllib3/contrib/pyopenssl.py\n--- a/requests/packages/urllib3/contrib/pyopenssl.py\n+++ b/requests/packages/urllib3/contrib/pyopenssl.py\n@@ -85,6 +85,14 @@\n \n DEFAULT_SSL_CIPHER_LIST = util.ssl_.DEFAULT_CIPHERS\n \n+# OpenSSL will only write 16K at a time\n+SSL_WRITE_BLOCKSIZE = 16384\n+\n+try:\n+    _ = memoryview\n+    has_memoryview = True\n+except NameError:\n+    has_memoryview = False\n \n orig_util_HAS_SNI = util.HAS_SNI\n orig_connection_ssl_wrap_socket = connection.ssl_wrap_socket\n@@ -204,13 +212,21 @@ def _send_until_done(self, data):\n                 continue\n \n     def sendall(self, data):\n-        while len(data):\n-            sent = self._send_until_done(data)\n-            data = data[sent:]\n+        if has_memoryview and not isinstance(data, memoryview):\n+            data = memoryview(data)\n+\n+        total_sent = 0\n+        while total_sent < len(data):\n+            sent = self._send_until_done(data[total_sent:total_sent+SSL_WRITE_BLOCKSIZE])\n+            total_sent += sent\n+\n+    def shutdown(self):\n+        # FIXME rethrow compatible exceptions should we ever use this\n+        self.connection.shutdown()\n \n     def close(self):\n         if self._makefile_refs < 1:\n-            return self.connection.shutdown()\n+            return self.connection.close()\n         else:\n             self._makefile_refs -= 1\n \n@@ -287,7 +303,7 @@ def ssl_wrap_socket(sock, keyfile=None, certfile=None, cert_reqs=None,\n                 raise timeout('select timed out')\n             continue\n         except OpenSSL.SSL.Error as e:\n-            raise ssl.SSLError('bad handshake', e)\n+            raise ssl.SSLError('bad handshake: %r' % e)\n         break\n \n     return WrappedSocket(cnx, sock)\ndiff --git a/requests/packages/urllib3/exceptions.py b/requests/packages/urllib3/exceptions.py\n--- a/requests/packages/urllib3/exceptions.py\n+++ b/requests/packages/urllib3/exceptions.py\n@@ -149,6 +149,11 @@ class SecurityWarning(HTTPWarning):\n     pass\n \n \n+class SubjectAltNameWarning(SecurityWarning):\n+    \"Warned when connecting to a host with a certificate missing a SAN.\"\n+    pass\n+\n+\n class InsecureRequestWarning(SecurityWarning):\n     \"Warned when making an unverified HTTPS request.\"\n     pass\n@@ -167,3 +172,19 @@ class InsecurePlatformWarning(SecurityWarning):\n class ResponseNotChunked(ProtocolError, ValueError):\n     \"Response needs to be chunked in order to read it as chunks.\"\n     pass\n+\n+\n+class ProxySchemeUnknown(AssertionError, ValueError):\n+    \"ProxyManager does not support the supplied scheme\"\n+    # TODO(t-8ch): Stop inheriting from AssertionError in v2.0.\n+\n+    def __init__(self, scheme):\n+        message = \"Not supported proxy scheme %s\" % scheme\n+        super(ProxySchemeUnknown, self).__init__(message)\n+\n+\n+class HeaderParsingError(HTTPError):\n+    \"Raised by assert_header_parsing, but we convert it to a log.warning statement.\"\n+    def __init__(self, defects, unparsed_data):\n+        message = '%s, unparsed data: %r' % (defects or 'Unknown', unparsed_data)\n+        super(HeaderParsingError, self).__init__(message)\ndiff --git a/requests/packages/urllib3/poolmanager.py b/requests/packages/urllib3/poolmanager.py\n--- a/requests/packages/urllib3/poolmanager.py\n+++ b/requests/packages/urllib3/poolmanager.py\n@@ -8,7 +8,7 @@\n from ._collections import RecentlyUsedContainer\n from .connectionpool import HTTPConnectionPool, HTTPSConnectionPool\n from .connectionpool import port_by_scheme\n-from .exceptions import LocationValueError, MaxRetryError\n+from .exceptions import LocationValueError, MaxRetryError, ProxySchemeUnknown\n from .request import RequestMethods\n from .util.url import parse_url\n from .util.retry import Retry\n@@ -227,8 +227,8 @@ def __init__(self, proxy_url, num_pools=10, headers=None,\n             port = port_by_scheme.get(proxy.scheme, 80)\n             proxy = proxy._replace(port=port)\n \n-        assert proxy.scheme in (\"http\", \"https\"), \\\n-            'Not supported proxy scheme %s' % proxy.scheme\n+        if proxy.scheme not in (\"http\", \"https\"):\n+            raise ProxySchemeUnknown(proxy.scheme)\n \n         self.proxy = proxy\n         self.proxy_headers = proxy_headers or {}\ndiff --git a/requests/packages/urllib3/request.py b/requests/packages/urllib3/request.py\n--- a/requests/packages/urllib3/request.py\n+++ b/requests/packages/urllib3/request.py\n@@ -71,14 +71,22 @@ def request(self, method, url, fields=None, headers=None, **urlopen_kw):\n                                             headers=headers,\n                                             **urlopen_kw)\n \n-    def request_encode_url(self, method, url, fields=None, **urlopen_kw):\n+    def request_encode_url(self, method, url, fields=None, headers=None,\n+                           **urlopen_kw):\n         \"\"\"\n         Make a request using :meth:`urlopen` with the ``fields`` encoded in\n         the url. This is useful for request methods like GET, HEAD, DELETE, etc.\n         \"\"\"\n+        if headers is None:\n+            headers = self.headers\n+\n+        extra_kw = {'headers': headers}\n+        extra_kw.update(urlopen_kw)\n+\n         if fields:\n             url += '?' + urlencode(fields)\n-        return self.urlopen(method, url, **urlopen_kw)\n+\n+        return self.urlopen(method, url, **extra_kw)\n \n     def request_encode_body(self, method, url, fields=None, headers=None,\n                             encode_multipart=True, multipart_boundary=None,\ndiff --git a/requests/packages/urllib3/response.py b/requests/packages/urllib3/response.py\n--- a/requests/packages/urllib3/response.py\n+++ b/requests/packages/urllib3/response.py\n@@ -2,6 +2,7 @@\n     import http.client as httplib\n except ImportError:\n     import httplib\n+from contextlib import contextmanager\n import zlib\n import io\n from socket import timeout as SocketTimeout\n@@ -12,7 +13,7 @@\n )\n from .packages.six import string_types as basestring, binary_type, PY3\n from .connection import HTTPException, BaseSSLError\n-from .util.response import is_fp_closed\n+from .util.response import is_fp_closed, is_response_to_head\n \n \n class DeflateDecoder(object):\n@@ -202,6 +203,47 @@ def _decode(self, data, decode_content, flush_decoder):\n \n         return data\n \n+    @contextmanager\n+    def _error_catcher(self):\n+        \"\"\"\n+        Catch low-level python exceptions, instead re-raising urllib3\n+        variants, so that low-level exceptions are not leaked in the\n+        high-level api.\n+\n+        On exit, release the connection back to the pool.\n+        \"\"\"\n+        try:\n+            try:\n+                yield\n+\n+            except SocketTimeout:\n+                # FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\n+                # there is yet no clean way to get at it from this context.\n+                raise ReadTimeoutError(self._pool, None, 'Read timed out.')\n+\n+            except BaseSSLError as e:\n+                # FIXME: Is there a better way to differentiate between SSLErrors?\n+                if 'read operation timed out' not in str(e):  # Defensive:\n+                    # This shouldn't happen but just in case we're missing an edge\n+                    # case, let's avoid swallowing SSL errors.\n+                    raise\n+\n+                raise ReadTimeoutError(self._pool, None, 'Read timed out.')\n+\n+            except HTTPException as e:\n+                # This includes IncompleteRead.\n+                raise ProtocolError('Connection broken: %r' % e, e)\n+        except Exception:\n+            # The response may not be closed but we're not going to use it anymore\n+            # so close it now to ensure that the connection is released back to the pool.\n+            if self._original_response and not self._original_response.isclosed():\n+                self._original_response.close()\n+\n+            raise\n+        finally:\n+            if self._original_response and self._original_response.isclosed():\n+                self.release_conn()\n+\n     def read(self, amt=None, decode_content=None, cache_content=False):\n         \"\"\"\n         Similar to :meth:`httplib.HTTPResponse.read`, but with two additional\n@@ -231,45 +273,28 @@ def read(self, amt=None, decode_content=None, cache_content=False):\n             return\n \n         flush_decoder = False\n-\n-        try:\n-            try:\n-                if amt is None:\n-                    # cStringIO doesn't like amt=None\n-                    data = self._fp.read()\n+        data = None\n+\n+        with self._error_catcher():\n+            if amt is None:\n+                # cStringIO doesn't like amt=None\n+                data = self._fp.read()\n+                flush_decoder = True\n+            else:\n+                cache_content = False\n+                data = self._fp.read(amt)\n+                if amt != 0 and not data:  # Platform-specific: Buggy versions of Python.\n+                    # Close the connection when no data is returned\n+                    #\n+                    # This is redundant to what httplib/http.client _should_\n+                    # already do.  However, versions of python released before\n+                    # December 15, 2012 (http://bugs.python.org/issue16298) do\n+                    # not properly close the connection in all cases. There is\n+                    # no harm in redundantly calling close.\n+                    self._fp.close()\n                     flush_decoder = True\n-                else:\n-                    cache_content = False\n-                    data = self._fp.read(amt)\n-                    if amt != 0 and not data:  # Platform-specific: Buggy versions of Python.\n-                        # Close the connection when no data is returned\n-                        #\n-                        # This is redundant to what httplib/http.client _should_\n-                        # already do.  However, versions of python released before\n-                        # December 15, 2012 (http://bugs.python.org/issue16298) do\n-                        # not properly close the connection in all cases. There is\n-                        # no harm in redundantly calling close.\n-                        self._fp.close()\n-                        flush_decoder = True\n-\n-            except SocketTimeout:\n-                # FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\n-                # there is yet no clean way to get at it from this context.\n-                raise ReadTimeoutError(self._pool, None, 'Read timed out.')\n-\n-            except BaseSSLError as e:\n-                # FIXME: Is there a better way to differentiate between SSLErrors?\n-                if 'read operation timed out' not in str(e):  # Defensive:\n-                    # This shouldn't happen but just in case we're missing an edge\n-                    # case, let's avoid swallowing SSL errors.\n-                    raise\n-\n-                raise ReadTimeoutError(self._pool, None, 'Read timed out.')\n-\n-            except HTTPException as e:\n-                # This includes IncompleteRead.\n-                raise ProtocolError('Connection broken: %r' % e, e)\n \n+        if data:\n             self._fp_bytes_read += len(data)\n \n             data = self._decode(data, decode_content, flush_decoder)\n@@ -277,11 +302,8 @@ def read(self, amt=None, decode_content=None, cache_content=False):\n             if cache_content:\n                 self._body = data\n \n-            return data\n+        return data\n \n-        finally:\n-            if self._original_response and self._original_response.isclosed():\n-                self.release_conn()\n \n     def stream(self, amt=2**16, decode_content=None):\n         \"\"\"\n@@ -319,6 +341,7 @@ def from_httplib(ResponseCls, r, **response_kw):\n         with ``original_response=r``.\n         \"\"\"\n         headers = r.msg\n+\n         if not isinstance(headers, HTTPHeaderDict):\n             if PY3: # Python 3\n                 headers = HTTPHeaderDict(headers.items())\n@@ -437,30 +460,29 @@ def read_chunked(self, amt=None, decode_content=None):\n             raise ResponseNotChunked(\"Response is not chunked. \"\n                 \"Header 'transfer-encoding: chunked' is missing.\")\n \n-        if self._original_response and self._original_response._method.upper() == 'HEAD':\n-            # Don't bother reading the body of a HEAD request.\n-            # FIXME: Can we do this somehow without accessing private httplib _method?\n+        # Don't bother reading the body of a HEAD request.\n+        if self._original_response and is_response_to_head(self._original_response):\n             self._original_response.close()\n             return\n \n-        while True:\n-            self._update_chunk_length()\n-            if self.chunk_left == 0:\n-                break\n-            chunk = self._handle_chunk(amt)\n-            yield self._decode(chunk, decode_content=decode_content,\n-                               flush_decoder=True)\n-\n-        # Chunk content ends with \\r\\n: discard it.\n-        while True:\n-            line = self._fp.fp.readline()\n-            if not line:\n-                # Some sites may not end with '\\r\\n'.\n-                break\n-            if line == b'\\r\\n':\n-                break\n-\n-        # We read everything; close the \"file\".\n-        if self._original_response:\n-            self._original_response.close()\n-        self.release_conn()\n+        with self._error_catcher():\n+            while True:\n+                self._update_chunk_length()\n+                if self.chunk_left == 0:\n+                    break\n+                chunk = self._handle_chunk(amt)\n+                yield self._decode(chunk, decode_content=decode_content,\n+                                   flush_decoder=True)\n+\n+            # Chunk content ends with \\r\\n: discard it.\n+            while True:\n+                line = self._fp.fp.readline()\n+                if not line:\n+                    # Some sites may not end with '\\r\\n'.\n+                    break\n+                if line == b'\\r\\n':\n+                    break\n+\n+            # We read everything; close the \"file\".\n+            if self._original_response:\n+                self._original_response.close()\ndiff --git a/requests/packages/urllib3/util/connection.py b/requests/packages/urllib3/util/connection.py\n--- a/requests/packages/urllib3/util/connection.py\n+++ b/requests/packages/urllib3/util/connection.py\n@@ -60,6 +60,8 @@ def create_connection(address, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,\n     \"\"\"\n \n     host, port = address\n+    if host.startswith('['):\n+        host = host.strip('[]')\n     err = None\n     for res in socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM):\n         af, socktype, proto, canonname, sa = res\ndiff --git a/requests/packages/urllib3/util/response.py b/requests/packages/urllib3/util/response.py\n--- a/requests/packages/urllib3/util/response.py\n+++ b/requests/packages/urllib3/util/response.py\n@@ -1,3 +1,11 @@\n+try:\n+    import http.client as httplib\n+except ImportError:\n+    import httplib\n+\n+from ..exceptions import HeaderParsingError\n+\n+\n def is_fp_closed(obj):\n     \"\"\"\n     Checks whether a given file-like object is closed.\n@@ -20,3 +28,49 @@ def is_fp_closed(obj):\n         pass\n \n     raise ValueError(\"Unable to determine whether fp is closed.\")\n+\n+\n+def assert_header_parsing(headers):\n+    \"\"\"\n+    Asserts whether all headers have been successfully parsed.\n+    Extracts encountered errors from the result of parsing headers.\n+\n+    Only works on Python 3.\n+\n+    :param headers: Headers to verify.\n+    :type headers: `httplib.HTTPMessage`.\n+\n+    :raises urllib3.exceptions.HeaderParsingError:\n+        If parsing errors are found.\n+    \"\"\"\n+\n+    # This will fail silently if we pass in the wrong kind of parameter.\n+    # To make debugging easier add an explicit check.\n+    if not isinstance(headers, httplib.HTTPMessage):\n+        raise TypeError('expected httplib.Message, got {}.'.format(\n+            type(headers)))\n+\n+    defects = getattr(headers, 'defects', None)\n+    get_payload = getattr(headers, 'get_payload', None)\n+\n+    unparsed_data = None\n+    if get_payload:  # Platform-specific: Python 3.\n+        unparsed_data = get_payload()\n+\n+    if defects or unparsed_data:\n+        raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)\n+\n+\n+def is_response_to_head(response):\n+    \"\"\"\n+    Checks, wether a the request of a response has been a HEAD-request.\n+    Handles the quirks of AppEngine.\n+\n+    :param conn:\n+    :type conn: :class:`httplib.HTTPResponse`\n+    \"\"\"\n+    # FIXME: Can we do this somehow without accessing private httplib _method?\n+    method = response._method\n+    if isinstance(method, int):  # Platform-specific: Appengine\n+        return method == 3\n+    return method.upper() == 'HEAD'\ndiff --git a/requests/packages/urllib3/util/retry.py b/requests/packages/urllib3/util/retry.py\n--- a/requests/packages/urllib3/util/retry.py\n+++ b/requests/packages/urllib3/util/retry.py\n@@ -94,7 +94,7 @@ class Retry(object):\n \n         seconds. If the backoff_factor is 0.1, then :func:`.sleep` will sleep\n         for [0.1s, 0.2s, 0.4s, ...] between retries. It will never be longer\n-        than :attr:`Retry.MAX_BACKOFF`.\n+        than :attr:`Retry.BACKOFF_MAX`.\n \n         By default, backoff is disabled (set to 0).\n \ndiff --git a/requests/packages/urllib3/util/ssl_.py b/requests/packages/urllib3/util/ssl_.py\n--- a/requests/packages/urllib3/util/ssl_.py\n+++ b/requests/packages/urllib3/util/ssl_.py\n@@ -8,6 +8,13 @@\n HAS_SNI = False\n create_default_context = None\n \n+# Maps the length of a digest to a possible hash function producing this digest\n+HASHFUNC_MAP = {\n+    32: md5,\n+    40: sha1,\n+    64: sha256,\n+}\n+\n import errno\n import warnings\n \n@@ -112,31 +119,21 @@ def assert_fingerprint(cert, fingerprint):\n         Fingerprint as string of hexdigits, can be interspersed by colons.\n     \"\"\"\n \n-    # Maps the length of a digest to a possible hash function producing\n-    # this digest.\n-    hashfunc_map = {\n-        16: md5,\n-        20: sha1,\n-        32: sha256,\n-    }\n-\n     fingerprint = fingerprint.replace(':', '').lower()\n-    digest_length, odd = divmod(len(fingerprint), 2)\n-\n-    if odd or digest_length not in hashfunc_map:\n-        raise SSLError('Fingerprint is of invalid length.')\n+    digest_length = len(fingerprint)\n+    hashfunc = HASHFUNC_MAP.get(digest_length)\n+    if not hashfunc:\n+        raise SSLError(\n+            'Fingerprint of invalid length: {0}'.format(fingerprint))\n \n     # We need encode() here for py32; works on py2 and p33.\n     fingerprint_bytes = unhexlify(fingerprint.encode())\n \n-    hashfunc = hashfunc_map[digest_length]\n-\n     cert_digest = hashfunc(cert).digest()\n \n-    if not cert_digest == fingerprint_bytes:\n+    if cert_digest != fingerprint_bytes:\n         raise SSLError('Fingerprints did not match. Expected \"{0}\", got \"{1}\".'\n-                       .format(hexlify(fingerprint_bytes),\n-                               hexlify(cert_digest)))\n+                       .format(fingerprint, hexlify(cert_digest)))\n \n \n def resolve_cert_reqs(candidate):\n",
  "test_patch": "diff --git a/test_requests.py b/test_requests.py\n--- a/test_requests.py\n+++ b/test_requests.py\n@@ -33,6 +33,11 @@\n except ImportError:\n     import io as StringIO\n \n+try:\n+    from multiprocessing.pool import ThreadPool\n+except ImportError:\n+    ThreadPool = None\n+\n if is_py3:\n     def u(s):\n         return s\n@@ -419,6 +424,21 @@ def test_DIGESTAUTH_QUOTES_QOP_VALUE(self):\n         r = requests.get(url, auth=auth)\n         assert '\"auth\"' in r.request.headers['Authorization']\n \n+    def test_DIGESTAUTH_THREADED(self):\n+\n+        auth = HTTPDigestAuth('user', 'pass')\n+        url = httpbin('digest-auth', 'auth', 'user', 'pass')\n+        session = requests.Session()\n+        session.auth=auth\n+\n+        def do_request(i):\n+            r = session.get(url)\n+            assert '\"auth\"' in r.request.headers['Authorization']\n+            return 1\n+        if ThreadPool is not None:\n+            pool = ThreadPool(processes=50)\n+            pool.map(do_request, range(100))\n+\n     def test_POSTBIN_GET_POST_FILES(self):\n \n         url = httpbin('post')\n@@ -1655,6 +1675,16 @@ def test_urllib3_retries():\n     with pytest.raises(RetryError):\n         s.get(httpbin('status/500'))\n \n+\n+def test_urllib3_pool_connection_closed():\n+    s = requests.Session()\n+    s.mount('http://', HTTPAdapter(pool_connections=0, pool_maxsize=0))\n+\n+    try:\n+        s.get(httpbin('status/200'))\n+    except ConnectionError as e:\n+        assert u\"HTTPConnectionPool(host='httpbin.org', port=80): Pool is closed.\" in str(e)\n+\n def test_vendor_aliases():\n     from requests.packages import urllib3\n     from requests.packages import chardet\n",
  "problem_statement": "urllib3 exceptions passing through requests API\nI don't know if it's a design goal of requests to hide urllib3's exceptions and wrap them around requests.exceptions types.\n\n(If it's not IMHO it should be, but that's another discussion)\n\nIf it is, I have at least two of them passing through that I have to catch in addition to requests' exceptions. They are requests.packages.urllib3.exceptions.DecodeError and requests.packages.urllib3.exceptions.TimeoutError (this one I get when a proxy timeouts)\n\nThanks!\n\n",
  "hints_text": "I definitely agree with you and would agree that these should be wrapped.\n\nCould you give us stack-traces so we can find where they're bleeding through?\n\nSorry I don't have stack traces readily available :/\n\nNo worries. I have ideas as to where the DecodeError might be coming from but I'm not certain where the TimeoutError could be coming from.\n\nIf you run into them again, please save us the stack traces. =) Thanks for reporting them. (We'll never know what we're missing until someone tells us.)\n\n`TimeoutError` is almost certainly being raised from either [`HTTPConnectionPool.urlopen()`](https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L282-L293) or from [`HTTPConnection.putrequest()`](https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L301). Adding a new clause to [here](https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L323-L335) should cover us.\n\nActually, that can't be right, we should be catching and rethrowing as a Requests `Timeout` exception in that block. Hmm, I'll do another spin through the code to see if I can see the problem.\n\nYeah, a quick search of the `urllib3` code reveals that the only place that `TimeoutError`s are thrown is from `HTTPConnectionPool.urlopen()`. These should not be leaking. We really need a stack trace to track this down.\n\nI've added a few logs to get the traces if they happen again. What may have confused me for the TimeoutError is that requests' Timeout actually wraps the urllib3's TimeoutError and we were logging the content of the error as well. \n\nSo DecodeError was definitely being thrown but probably not TimeoutError, sorry for the confusion. I'll report here it I ever see it happening now that we're watching for it.\n\nThanks for the help!\n\nI also got urllib3 exceptions passing through when use Session in several threads, trace:\n\n```\n......\n  File \"C:\\Python27\\lib\\site-packages\\requests\\sessions.py\", line 347, in get\n    return self.request('GET', url, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\requests\\sessions.py\", line 335, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\requests\\sessions.py\", line 438, in send\n    r = adapter.send(request, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\requests\\adapters.py\", line 292, in send\n    timeout=timeout\n  File \"C:\\Python27\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 423, in url\nopen\n    conn = self._get_conn(timeout=pool_timeout)\n  File \"C:\\Python27\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 224, in _ge\nt_conn\n    raise ClosedPoolError(self, \"Pool is closed.\")\nClosedPoolError: HTTPConnectionPool(host='......', port=80): Pool is closed.\n```\n\nAh, we should rewrap that `ClosedPoolError` too.\n\nBut it's still the summer... How can any pool be closed? :smirk_cat: \n\nBut yes :+1:\n\nI've added a fix for the `ClosedPoolError` to #1475. Which apparently broke in the last month for no adequately understandable reason.\n\nIf it's still needed, here is the traceback of DecodeError I got using proxy on requests 2.0.0:\n\n```\nTraceback (most recent call last):\n  File \"/home/krat/Projects/Grubhub/source/Pit/pit/web.py\", line 52, in request\n    response = session.request(method, url, **kw)\n  File \"/home/krat/.virtualenvs/grubhub/local/lib/python2.7/site-packages/requests/sessions.py\", line 357, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/home/krat/.virtualenvs/grubhub/local/lib/python2.7/site-packages/requests/sessions.py\", line 460, in send\n    r = adapter.send(request, **kwargs)\n  File \"/home/krat/.virtualenvs/grubhub/local/lib/python2.7/site-packages/requests/adapters.py\", line 367, in send\n    r.content\n  File \"/home/krat/.virtualenvs/grubhub/local/lib/python2.7/site-packages/requests/models.py\", line 633, in content\n    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()\n  File \"/home/krat/.virtualenvs/grubhub/local/lib/python2.7/site-packages/requests/models.py\", line 572, in generate\n    decode_content=True):\n  File \"/home/krat/.virtualenvs/grubhub/local/lib/python2.7/site-packages/requests/packages/urllib3/response.py\", line 225, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"/home/krat/.virtualenvs/grubhub/local/lib/python2.7/site-packages/requests/packages/urllib3/response.py\", line 193, in read\n    e)\nDecodeError: ('Received response with content-encoding: gzip, but failed to decode it.', error('Error -3 while decompressing: incorrect header check',))\n```\n\nSlightly different to the above, but urllib3's LocationParseError leaks through which could probably do with being wrapped in InvalidURL.\n\n```\nTraceback (most recent call last):\n  File \"/home/oliver/wc/trunk/mtmCore/python/asagent/samplers/net/web.py\", line 255, in process_url\n    resp = self.request(self.params.httpverb, url, data=data)\n  File \"/home/oliver/wc/trunk/mtmCore/python/asagent/samplers/net/web.py\", line 320, in request\n    verb, url, data=data))\n  File \"abilisoft/requests/opt/abilisoft.com/thirdparty/requests/lib/python2.7/site-packages/requests/sessions.py\", line 286, in prepare_request\n  File \"abilisoft/requests/opt/abilisoft.com/thirdparty/requests/lib/python2.7/site-packages/requests/models.py\", line 286, in prepare\n  File \"abilisoft/requests/opt/abilisoft.com/thirdparty/requests/lib/python2.7/site-packages/requests/models.py\", line 333, in prepare_url\n  File \"abilisoft/requests/opt/abilisoft.com/thirdparty/requests/lib/python2.7/site-packages/requests/packages/urllib3/util.py\", line 397, in parse_url\nLocationParseError: Failed to parse: Failed to parse: fe80::5054:ff:fe5a:fc0\n```\n",
  "created_at": "2015-07-18T15:45:52Z",
  "version": "2.7",
  "FAIL_TO_PASS": "[\"test_requests.py::RequestsTestCase::test_HTTP_302_ALLOW_REDIRECT_GET\", \"test_requests.py::RequestsTestCase::test_auth_is_retained_for_redirect_on_host\", \"test_requests.py::RequestsTestCase::test_auth_is_stripped_on_redirect_off_host\", \"test_requests.py::RequestsTestCase::test_different_encodings_dont_break_post\", \"test_requests.py::RequestsTestCase::test_prepared_from_session\", \"test_requests.py::RequestsTestCase::test_user_agent_transfers\", \"test_requests.py::test_urllib3_pool_connection_closed\"]",
  "PASS_TO_PASS": "[\"test_requests.py::RequestsTestCase::test_BASICAUTH_TUPLE_HTTP_200_OK_GET\", \"test_requests.py::RequestsTestCase::test_DIGESTAUTH_QUOTES_QOP_VALUE\", \"test_requests.py::RequestsTestCase::test_DIGESTAUTH_WRONG_HTTP_401_GET\", \"test_requests.py::RequestsTestCase::test_DIGEST_AUTH_RETURNS_COOKIE\", \"test_requests.py::RequestsTestCase::test_DIGEST_AUTH_SETS_SESSION_COOKIES\", \"test_requests.py::RequestsTestCase::test_DIGEST_HTTP_200_OK_GET\", \"test_requests.py::RequestsTestCase::test_DIGEST_STREAM\", \"test_requests.py::RequestsTestCase::test_HTTP_200_OK_GET_ALTERNATIVE\", \"test_requests.py::RequestsTestCase::test_HTTP_200_OK_GET_WITH_MIXED_PARAMS\", \"test_requests.py::RequestsTestCase::test_HTTP_200_OK_GET_WITH_PARAMS\", \"test_requests.py::RequestsTestCase::test_HTTP_200_OK_PUT\", \"test_requests.py::RequestsTestCase::test_LocationParseError\", \"test_requests.py::RequestsTestCase::test_POSTBIN_GET_POST_FILES\", \"test_requests.py::RequestsTestCase::test_POSTBIN_GET_POST_FILES_WITH_DATA\", \"test_requests.py::RequestsTestCase::test_autoset_header_values_are_native\", \"test_requests.py::RequestsTestCase::test_basic_auth_str_is_always_native\", \"test_requests.py::RequestsTestCase::test_basic_building\", \"test_requests.py::RequestsTestCase::test_basicauth_with_netrc\", \"test_requests.py::RequestsTestCase::test_can_send_bytes_bytearray_objects_with_files\", \"test_requests.py::RequestsTestCase::test_can_send_file_object_with_non_string_filename\", \"test_requests.py::RequestsTestCase::test_can_send_nonstring_objects_with_files\", \"test_requests.py::RequestsTestCase::test_cannot_send_unprepared_requests\", \"test_requests.py::RequestsTestCase::test_connection_error_invalid_domain\", \"test_requests.py::RequestsTestCase::test_connection_error_invalid_port\", \"test_requests.py::RequestsTestCase::test_cookie_as_dict_items\", \"test_requests.py::RequestsTestCase::test_cookie_as_dict_keeps_items\", \"test_requests.py::RequestsTestCase::test_cookie_as_dict_keeps_len\", \"test_requests.py::RequestsTestCase::test_cookie_as_dict_keys\", \"test_requests.py::RequestsTestCase::test_cookie_as_dict_values\", \"test_requests.py::RequestsTestCase::test_cookie_parameters\", \"test_requests.py::RequestsTestCase::test_cookie_persists_via_api\", \"test_requests.py::RequestsTestCase::test_cookie_quote_wrapped\", \"test_requests.py::RequestsTestCase::test_cookie_removed_on_expire\", \"test_requests.py::RequestsTestCase::test_cookie_sent_on_redirect\", \"test_requests.py::RequestsTestCase::test_custom_content_type\", \"test_requests.py::RequestsTestCase::test_decompress_gzip\", \"test_requests.py::RequestsTestCase::test_entry_points\", \"test_requests.py::RequestsTestCase::test_fixes_1329\", \"test_requests.py::RequestsTestCase::test_generic_cookiejar_works\", \"test_requests.py::RequestsTestCase::test_get_auth_from_url\", \"test_requests.py::RequestsTestCase::test_get_auth_from_url_encoded_hashes\", \"test_requests.py::RequestsTestCase::test_get_auth_from_url_encoded_spaces\", \"test_requests.py::RequestsTestCase::test_get_auth_from_url_not_encoded_spaces\", \"test_requests.py::RequestsTestCase::test_get_auth_from_url_percent_chars\", \"test_requests.py::RequestsTestCase::test_header_keys_are_native\", \"test_requests.py::RequestsTestCase::test_history_is_always_a_list\", \"test_requests.py::RequestsTestCase::test_hook_receives_request_arguments\", \"test_requests.py::RequestsTestCase::test_http_error\", \"test_requests.py::RequestsTestCase::test_invalid_url\", \"test_requests.py::RequestsTestCase::test_json_param_post_content_type_works\", \"test_requests.py::RequestsTestCase::test_links\", \"test_requests.py::RequestsTestCase::test_long_authinfo_in_url\", \"test_requests.py::RequestsTestCase::test_manual_redirect_with_partial_body_read\", \"test_requests.py::RequestsTestCase::test_mixed_case_scheme_acceptable\", \"test_requests.py::RequestsTestCase::test_no_content_length\", \"test_requests.py::RequestsTestCase::test_nonhttp_schemes_dont_check_URLs\", \"test_requests.py::RequestsTestCase::test_override_content_length\", \"test_requests.py::RequestsTestCase::test_param_cookiejar_works\", \"test_requests.py::RequestsTestCase::test_params_are_added_before_fragment\", \"test_requests.py::RequestsTestCase::test_params_are_merged_case_sensitive\", \"test_requests.py::RequestsTestCase::test_params_original_order_is_preserved_by_default\", \"test_requests.py::RequestsTestCase::test_path_is_not_double_encoded\", \"test_requests.py::RequestsTestCase::test_prepare_request_with_bytestring_url\", \"test_requests.py::RequestsTestCase::test_prepared_request_hook\", \"test_requests.py::RequestsTestCase::test_pyopenssl_redirect\", \"test_requests.py::RequestsTestCase::test_redirect_with_wrong_gzipped_header\", \"test_requests.py::RequestsTestCase::test_request_and_response_are_pickleable\", \"test_requests.py::RequestsTestCase::test_request_cookie_overrides_session_cookie\", \"test_requests.py::RequestsTestCase::test_request_cookies_not_persisted\", \"test_requests.py::RequestsTestCase::test_request_ok_set\", \"test_requests.py::RequestsTestCase::test_requests_history_is_saved\", \"test_requests.py::RequestsTestCase::test_requests_in_history_are_not_overridden\", \"test_requests.py::RequestsTestCase::test_response_decode_unicode\", \"test_requests.py::RequestsTestCase::test_response_is_iterable\", \"test_requests.py::RequestsTestCase::test_response_iter_lines\", \"test_requests.py::RequestsTestCase::test_session_hooks_are_overriden_by_request_hooks\", \"test_requests.py::RequestsTestCase::test_session_hooks_are_used_with_no_request_hooks\", \"test_requests.py::RequestsTestCase::test_session_pickling\", \"test_requests.py::RequestsTestCase::test_set_cookie_on_301\", \"test_requests.py::RequestsTestCase::test_status_raising\", \"test_requests.py::RequestsTestCase::test_time_elapsed_blank\", \"test_requests.py::RequestsTestCase::test_transport_adapter_ordering\", \"test_requests.py::RequestsTestCase::test_unconsumed_session_response_closes_connection\", \"test_requests.py::RequestsTestCase::test_unicode_get\", \"test_requests.py::RequestsTestCase::test_unicode_header_name\", \"test_requests.py::RequestsTestCase::test_unicode_method_name\", \"test_requests.py::RequestsTestCase::test_unicode_multipart_post\", \"test_requests.py::RequestsTestCase::test_unicode_multipart_post_fieldnames\", \"test_requests.py::RequestsTestCase::test_uppercase_scheme_redirect\", \"test_requests.py::RequestsTestCase::test_urlencoded_get_query_multivalued_param\", \"test_requests.py::TestContentEncodingDetection::test_html4_pragma\", \"test_requests.py::TestContentEncodingDetection::test_html_charset\", \"test_requests.py::TestContentEncodingDetection::test_none\", \"test_requests.py::TestContentEncodingDetection::test_precedence\", \"test_requests.py::TestContentEncodingDetection::test_xhtml_pragma\", \"test_requests.py::TestContentEncodingDetection::test_xml\", \"test_requests.py::TestCaseInsensitiveDict::test_contains\", \"test_requests.py::TestCaseInsensitiveDict::test_copy\", \"test_requests.py::TestCaseInsensitiveDict::test_delitem\", \"test_requests.py::TestCaseInsensitiveDict::test_docstring_example\", \"test_requests.py::TestCaseInsensitiveDict::test_equality\", \"test_requests.py::TestCaseInsensitiveDict::test_fixes_649\", \"test_requests.py::TestCaseInsensitiveDict::test_get\", \"test_requests.py::TestCaseInsensitiveDict::test_getitem\", \"test_requests.py::TestCaseInsensitiveDict::test_iter\", \"test_requests.py::TestCaseInsensitiveDict::test_iterable_init\", \"test_requests.py::TestCaseInsensitiveDict::test_kwargs_init\", \"test_requests.py::TestCaseInsensitiveDict::test_len\", \"test_requests.py::TestCaseInsensitiveDict::test_lower_items\", \"test_requests.py::TestCaseInsensitiveDict::test_mapping_init\", \"test_requests.py::TestCaseInsensitiveDict::test_preserve_key_case\", \"test_requests.py::TestCaseInsensitiveDict::test_preserve_last_key_case\", \"test_requests.py::TestCaseInsensitiveDict::test_setdefault\", \"test_requests.py::TestCaseInsensitiveDict::test_update\", \"test_requests.py::TestCaseInsensitiveDict::test_update_retains_unchanged\", \"test_requests.py::UtilsTestCase::test_address_in_network\", \"test_requests.py::UtilsTestCase::test_dotted_netmask\", \"test_requests.py::UtilsTestCase::test_get_auth_from_url\", \"test_requests.py::UtilsTestCase::test_get_environ_proxies\", \"test_requests.py::UtilsTestCase::test_get_environ_proxies_ip_ranges\", \"test_requests.py::UtilsTestCase::test_guess_filename_when_filename_is_an_int\", \"test_requests.py::UtilsTestCase::test_guess_filename_when_int\", \"test_requests.py::UtilsTestCase::test_guess_filename_with_file_like_obj\", \"test_requests.py::UtilsTestCase::test_guess_filename_with_unicode_name\", \"test_requests.py::UtilsTestCase::test_is_ipv4_address\", \"test_requests.py::UtilsTestCase::test_is_valid_cidr\", \"test_requests.py::UtilsTestCase::test_requote_uri_properly_requotes\", \"test_requests.py::UtilsTestCase::test_requote_uri_with_unquoted_percents\", \"test_requests.py::UtilsTestCase::test_super_len_io_streams\", \"test_requests.py::TestMorselToCookieExpires::test_expires_invalid_int\", \"test_requests.py::TestMorselToCookieExpires::test_expires_invalid_str\", \"test_requests.py::TestMorselToCookieExpires::test_expires_none\", \"test_requests.py::TestMorselToCookieExpires::test_expires_valid_str\", \"test_requests.py::TestMorselToCookieMaxAge::test_max_age_invalid_str\", \"test_requests.py::TestMorselToCookieMaxAge::test_max_age_valid_int\", \"test_requests.py::TestTimeout::test_stream_timeout\", \"test_requests.py::TestTimeout::test_invalid_timeout\", \"test_requests.py::TestTimeout::test_none_timeout\", \"test_requests.py::TestTimeout::test_read_timeout\", \"test_requests.py::TestTimeout::test_connect_timeout\", \"test_requests.py::TestTimeout::test_total_timeout_connect\", \"test_requests.py::TestRedirects::test_requests_are_updated_each_time\", \"test_requests.py::test_data_argument_accepts_tuples\", \"test_requests.py::test_prepared_request_empty_copy\", \"test_requests.py::test_prepared_request_no_cookies_copy\", \"test_requests.py::test_prepared_request_complete_copy\", \"test_requests.py::test_prepare_unicode_url\", \"test_requests.py::test_urllib3_retries\", \"test_requests.py::test_vendor_aliases\"]",
  "environment_setup_commit": "bf436ea0a49513bd4e49bb2d1645bd770e470d75",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.857342",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}