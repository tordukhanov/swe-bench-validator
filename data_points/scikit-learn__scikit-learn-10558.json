{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-10558",
  "base_commit": "2ccc946157d40bbb8bb17b70e98df6af49d5f40c",
  "patch": "diff --git a/examples/plot_missing_values.py b/examples/plot_missing_values.py\n--- a/examples/plot_missing_values.py\n+++ b/examples/plot_missing_values.py\n@@ -65,8 +65,7 @@\n X_missing[np.where(missing_samples)[0], missing_features] = 0\n y_missing = y_full.copy()\n estimator = Pipeline([(\"imputer\", Imputer(missing_values=0,\n-                                          strategy=\"mean\",\n-                                          axis=0)),\n+                                          strategy=\"mean\")),\n                       (\"forest\", RandomForestRegressor(random_state=0,\n                                                        n_estimators=100))])\n score = cross_val_score(estimator, X_missing, y_missing).mean()\ndiff --git a/sklearn/preprocessing/imputation.py b/sklearn/preprocessing/imputation.py\n--- a/sklearn/preprocessing/imputation.py\n+++ b/sklearn/preprocessing/imputation.py\n@@ -82,12 +82,19 @@ class Imputer(BaseEstimator, TransformerMixin):\n         - If \"most_frequent\", then replace missing using the most frequent\n           value along the axis.\n \n-    axis : integer, optional (default=0)\n+    axis : integer, optional (default=None)\n         The axis along which to impute.\n \n         - If `axis=0`, then impute along columns.\n         - If `axis=1`, then impute along rows.\n \n+        .. deprecated:: 0.20\n+           Parameter ``axis`` has been deprecated in 0.20 and will be removed\n+           in 0.22. Future (and default) behavior is equivalent to ``axis=0``\n+           (impute along columns). Row-wise imputation can be performed with\n+           FunctionTransformer (e.g.,\n+           ``FunctionTransformer(lambda X: Imputer().fit_transform(X.T).T)``).\n+\n     verbose : integer, optional (default=0)\n         Controls the verbosity of the imputer.\n \n@@ -115,7 +122,7 @@ class Imputer(BaseEstimator, TransformerMixin):\n       contain missing values).\n     \"\"\"\n     def __init__(self, missing_values=\"NaN\", strategy=\"mean\",\n-                 axis=0, verbose=0, copy=True):\n+                 axis=None, verbose=0, copy=True):\n         self.missing_values = missing_values\n         self.strategy = strategy\n         self.axis = axis\n@@ -142,14 +149,24 @@ def fit(self, X, y=None):\n                              \" got strategy={1}\".format(allowed_strategies,\n                                                         self.strategy))\n \n-        if self.axis not in [0, 1]:\n+        if self.axis is None:\n+            self._axis = 0\n+        else:\n+            warnings.warn(\"Parameter 'axis' has been deprecated in 0.20 and \"\n+                          \"will be removed in 0.22. Future (and default) \"\n+                          \"behavior is equivalent to 'axis=0' (impute along \"\n+                          \"columns). Row-wise imputation can be performed \"\n+                          \"with FunctionTransformer.\", DeprecationWarning)\n+            self._axis = self.axis\n+\n+        if self._axis not in [0, 1]:\n             raise ValueError(\"Can only impute missing values on axis 0 and 1, \"\n-                             \" got axis={0}\".format(self.axis))\n+                             \" got axis={0}\".format(self._axis))\n \n         # Since two different arrays can be provided in fit(X) and\n         # transform(X), the imputation data will be computed in transform()\n         # when the imputation is done per sample (i.e., when axis=1).\n-        if self.axis == 0:\n+        if self._axis == 0:\n             X = check_array(X, accept_sparse='csc', dtype=np.float64,\n                             force_all_finite=False)\n \n@@ -157,12 +174,12 @@ def fit(self, X, y=None):\n                 self.statistics_ = self._sparse_fit(X,\n                                                     self.strategy,\n                                                     self.missing_values,\n-                                                    self.axis)\n+                                                    self._axis)\n             else:\n                 self.statistics_ = self._dense_fit(X,\n                                                    self.strategy,\n                                                    self.missing_values,\n-                                                   self.axis)\n+                                                   self._axis)\n \n         return self\n \n@@ -305,7 +322,7 @@ def transform(self, X):\n         X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n             The input data to complete.\n         \"\"\"\n-        if self.axis == 0:\n+        if self._axis == 0:\n             check_is_fitted(self, 'statistics_')\n             X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES,\n                             force_all_finite=False, copy=self.copy)\n@@ -325,27 +342,27 @@ def transform(self, X):\n                 statistics = self._sparse_fit(X,\n                                               self.strategy,\n                                               self.missing_values,\n-                                              self.axis)\n+                                              self._axis)\n \n             else:\n                 statistics = self._dense_fit(X,\n                                              self.strategy,\n                                              self.missing_values,\n-                                             self.axis)\n+                                             self._axis)\n \n         # Delete the invalid rows/columns\n         invalid_mask = np.isnan(statistics)\n         valid_mask = np.logical_not(invalid_mask)\n         valid_statistics = statistics[valid_mask]\n         valid_statistics_indexes = np.where(valid_mask)[0]\n-        missing = np.arange(X.shape[not self.axis])[invalid_mask]\n+        missing = np.arange(X.shape[not self._axis])[invalid_mask]\n \n-        if self.axis == 0 and invalid_mask.any():\n+        if self._axis == 0 and invalid_mask.any():\n             if self.verbose:\n                 warnings.warn(\"Deleting features without \"\n                               \"observed values: %s\" % missing)\n             X = X[:, valid_statistics_indexes]\n-        elif self.axis == 1 and invalid_mask.any():\n+        elif self._axis == 1 and invalid_mask.any():\n             raise ValueError(\"Some rows only contain \"\n                              \"missing values: %s\" % missing)\n \n@@ -362,10 +379,10 @@ def transform(self, X):\n                 X = X.toarray()\n \n             mask = _get_mask(X, self.missing_values)\n-            n_missing = np.sum(mask, axis=self.axis)\n+            n_missing = np.sum(mask, axis=self._axis)\n             values = np.repeat(valid_statistics, n_missing)\n \n-            if self.axis == 0:\n+            if self._axis == 0:\n                 coordinates = np.where(mask.transpose())[::-1]\n             else:\n                 coordinates = mask\n",
  "test_patch": "diff --git a/sklearn/preprocessing/tests/test_imputation.py b/sklearn/preprocessing/tests/test_imputation.py\n--- a/sklearn/preprocessing/tests/test_imputation.py\n+++ b/sklearn/preprocessing/tests/test_imputation.py\n@@ -7,6 +7,8 @@\n from sklearn.utils.testing import assert_array_almost_equal\n from sklearn.utils.testing import assert_raises\n from sklearn.utils.testing import assert_false\n+from sklearn.utils.testing import assert_warns_message\n+from sklearn.utils.testing import ignore_warnings\n \n from sklearn.preprocessing.imputation import Imputer\n from sklearn.pipeline import Pipeline\n@@ -15,6 +17,7 @@\n from sklearn.random_projection import sparse_random_matrix\n \n \n+@ignore_warnings(category=DeprecationWarning)  # To be removed in 0.22\n def _check_statistics(X, X_true,\n                       strategy, statistics, missing_values):\n     \"\"\"Utility function for testing imputation for a given strategy.\n@@ -298,6 +301,7 @@ def test_imputation_pickle():\n         )\n \n \n+@ignore_warnings(category=DeprecationWarning)  # To be removed in 0.22\n def test_imputation_copy():\n     # Test imputation with copy\n     X_orig = sparse_random_matrix(5, 5, density=0.75, random_state=0)\n@@ -364,3 +368,15 @@ def test_imputation_copy():\n \n     # Note: If X is sparse and if missing_values=0, then a (dense) copy of X is\n     # made, even if copy=False.\n+\n+\n+def test_deprecated_imputer_axis():\n+    depr_message = (\"Parameter 'axis' has been deprecated in 0.20 and will \"\n+                    \"be removed in 0.22. Future (and default) behavior is \"\n+                    \"equivalent to 'axis=0' (impute along columns). Row-wise \"\n+                    \"imputation can be performed with FunctionTransformer.\")\n+    X = sparse_random_matrix(5, 5, density=0.75, random_state=0)\n+    imputer = Imputer(missing_values=0, axis=0)\n+    assert_warns_message(DeprecationWarning, depr_message, imputer.fit, X)\n+    imputer = Imputer(missing_values=0, axis=1)\n+    assert_warns_message(DeprecationWarning, depr_message, imputer.fit, X)\n",
  "problem_statement": "Deprecate Imputer with axis=1\nAfter having tried to deal with a few issues related to extending `Imputer` behaviour, I believe we should be removing the `axis` parameter from `Imputer`.\r\n\r\n* It seems a strange feature to support in a machine learning context, except perhaps where the features represent something like a time series.\r\n* It is not stateful and can be performed with a FunctionTransformer. (We could even provide a `row_impute` function, if we felt it necessary, which would roughly be defined as `def row_impute(X, **kwargs): return Imputer(**kwargs).fit_transform(X.T).T`.)\r\n* It complicates the implementation, which already has a bunch of weird edge-cases (to handle sparse data with missing indicated by 0 which is an inefficient use of a sparse data structure; and to handle non-NaN missingness indicators), unnecessarily.\r\n* It is often nonsensical to extend further features to the `axis=1` case.\r\n\r\nDo others agree?\n[MRG+1] Deprecate ``Imputer.axis`` argument\n\r\n#### Reference Issue\r\nFixes: #9463\r\n\r\n#### What does this implement/fix? Explain your changes.\r\nDeprecated the argument `axis` on the `Imputer` class.\r\n\n",
  "hints_text": "It could be stateful for KNN, right? That might not be totally useless. But not sure if that's something that people are doing.\r\nBut yeah, it's a strange feature, and I wouldn't be opposed to removing it.\nI'm not sure what it means in a knn imputation context.\n\nOn 1 Aug 2017 2:22 am, \"Andreas Mueller\" <notifications@github.com> wrote:\n\n> It could be stateful for KNN, right? That might not be totally useless.\n> But not sure if that's something that people are doing.\n> But yeah, it's a strange feature, and I wouldn't be opposed to removing it.\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/9463#issuecomment-319122664>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz65q63aHLWT8YCLqxQeiJLf0HV1Znks5sTf9DgaJpZM4OniQC>\n> .\n>\n\nWell you could learn which feature is most common to which feature is most common to which other feature, and then impute using a distance weighted average of these features.\r\nYou could learn something like \"this feature is always the average of these other two features\" or \"these features are perfectly correlated\".\nsounds like messy code to maintain, because behaviour with axis=1 is subtly\ndifferent: the axis=0 version of KNN gets the query from the test data and\nthe values to average from the training data; the axis=1 version gets the\nquery from the training data, i.e. nearest neighbors can be precomputed and\nthe values from the test data. I would rather see a KNNRowImputer if it's\nwell motivated.\n\nOn 2 Aug 2017 7:46 am, \"Andreas Mueller\" <notifications@github.com> wrote:\n\n> Well you could learn which feature is most common to which feature is most\n> common to which other feature, and then impute using a distance weighted\n> average of these features.\n> You could learn something like \"this feature is always the average of\n> these other two features\" or \"these features are perfectly correlated\".\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/9463#issuecomment-319506442>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz62-RXpPsETqkB5GubxAV4uwFd5wJks5sT5yfgaJpZM4OniQC>\n> .\n>\n\nyeah I agree.\nSince the only other sensible value would be `axis=0`, then this means we should probably deprecate the parameter completely?\n",
  "created_at": "2018-01-31T08:28:29Z",
  "version": "0.20",
  "FAIL_TO_PASS": "[\"sklearn/preprocessing/tests/test_imputation.py::test_deprecated_imputer_axis\"]",
  "PASS_TO_PASS": "[\"sklearn/preprocessing/tests/test_imputation.py::test_imputation_shape\", \"sklearn/preprocessing/tests/test_imputation.py::test_imputation_mean_median_only_zero\", \"sklearn/preprocessing/tests/test_imputation.py::test_imputation_mean_median\", \"sklearn/preprocessing/tests/test_imputation.py::test_imputation_median_special_cases\", \"sklearn/preprocessing/tests/test_imputation.py::test_imputation_most_frequent\", \"sklearn/preprocessing/tests/test_imputation.py::test_imputation_pipeline_grid_search\", \"sklearn/preprocessing/tests/test_imputation.py::test_imputation_pickle\", \"sklearn/preprocessing/tests/test_imputation.py::test_imputation_copy\"]",
  "environment_setup_commit": "55bf5d93e5674f13a1134d93a11fd0cd11aabcd1",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.952759",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}