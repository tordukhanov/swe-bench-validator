{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-13620",
  "base_commit": "f9af18b4e5b9d4b379867d32381296062782dc15",
  "patch": "diff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py\n--- a/sklearn/ensemble/gradient_boosting.py\n+++ b/sklearn/ensemble/gradient_boosting.py\n@@ -1709,17 +1709,26 @@ def feature_importances_(self):\n         Returns\n         -------\n         feature_importances_ : array, shape (n_features,)\n+            The values of this array sum to 1, unless all trees are single node\n+            trees consisting of only the root node, in which case it will be an\n+            array of zeros.\n         \"\"\"\n         self._check_initialized()\n \n-        total_sum = np.zeros((self.n_features_, ), dtype=np.float64)\n-        for stage in self.estimators_:\n-            stage_sum = sum(tree.tree_.compute_feature_importances(\n-                normalize=False) for tree in stage) / len(stage)\n-            total_sum += stage_sum\n-\n-        importances = total_sum / total_sum.sum()\n-        return importances\n+        relevant_trees = [tree\n+                          for stage in self.estimators_ for tree in stage\n+                          if tree.tree_.node_count > 1]\n+        if not relevant_trees:\n+            # degenerate case where all trees have only one node\n+            return np.zeros(shape=self.n_features_, dtype=np.float64)\n+\n+        relevant_feature_importances = [\n+            tree.tree_.compute_feature_importances(normalize=False)\n+            for tree in relevant_trees\n+        ]\n+        avg_feature_importances = np.mean(relevant_feature_importances,\n+                                          axis=0, dtype=np.float64)\n+        return avg_feature_importances / np.sum(avg_feature_importances)\n \n     def _validate_y(self, y, sample_weight):\n         # 'sample_weight' is not utilised but is used for\n",
  "test_patch": "diff --git a/sklearn/ensemble/tests/test_gradient_boosting.py b/sklearn/ensemble/tests/test_gradient_boosting.py\n--- a/sklearn/ensemble/tests/test_gradient_boosting.py\n+++ b/sklearn/ensemble/tests/test_gradient_boosting.py\n@@ -1440,3 +1440,12 @@ def test_early_stopping_n_classes():\n     # No error if we let training data be big enough\n     gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=0,\n                                     validation_fraction=4)\n+\n+\n+def test_gbr_degenerate_feature_importances():\n+    # growing an ensemble of single node trees. See #13620\n+    X = np.zeros((10, 10))\n+    y = np.ones((10,))\n+    gbr = GradientBoostingRegressor().fit(X, y)\n+    assert_array_equal(gbr.feature_importances_,\n+                       np.zeros(10, dtype=np.float64))\n",
  "problem_statement": "Bug in Gradient Boosting: Feature Importances do not sum to 1\n#### Description\r\n\r\nI found conditions when Feature Importance values do not add up to 1 in ensemble tree methods, like Gradient Boosting Trees or AdaBoost Trees.  \r\n\r\nThis error occurs once the ensemble reaches a large number of estimators.  The exact conditions depend variously.  For example, the error shows up sooner with a smaller amount of training samples.  Or, if the depth of the tree is large.  \r\n\r\nWhen this error appears, the predicted value seems to have converged.  But it’s unclear if the error is causing the predicted value not to change with more estimators.  In fact, the feature importance sum goes lower and lower with more estimators thereafter.  \r\n\r\nConsequently, it's questionable if the tree ensemble code is functioning as expected.  \r\n\r\nHere's sample code to reproduce this:\r\n\r\n``` python\r\nimport numpy as np\r\nfrom sklearn import datasets\r\nfrom sklearn.ensemble import GradientBoostingRegressor\r\n\r\nboston = datasets.load_boston()\r\nX, Y = (boston.data, boston.target)\r\n\r\nn_estimators = 720\r\n# Note: From 712 onwards, the feature importance sum is less than 1\r\n\r\nparams = {'n_estimators': n_estimators, 'max_depth': 6, 'learning_rate': 0.1}\r\nclf = GradientBoostingRegressor(**params)\r\nclf.fit(X, Y)\r\n\r\nfeature_importance_sum = np.sum(clf.feature_importances_)\r\nprint(\"At n_estimators = %i, feature importance sum = %f\" % (n_estimators , feature_importance_sum))\r\n```\r\n\r\n_Output:_\r\n\r\n```\r\nAt n_estimators = 720, feature importance sum = 0.987500\r\n```\r\n\r\nIn fact, if we examine the tree at each staged prediction, we'll see that the feature importance goes to 0 after we hit a certain number of estimators.  (For the code above, it's 712.)\r\n\r\nHere's code to describe what I mean:\r\n\r\n``` python\r\nfor i, tree in enumerate(clf.estimators_):\r\n    feature_importance_sum = np.sum(tree[0].feature_importances_)\r\n    print(\"At n_estimators = %i, feature importance sum = %f\" % (i , feature_importance_sum))\r\n```\r\n\r\n_Output:_\r\n\r\n```\r\n...\r\nAt n_estimators = 707, feature importance sum = 1.000000\r\nAt n_estimators = 708, feature importance sum = 1.000000\r\nAt n_estimators = 709, feature importance sum = 1.000000\r\nAt n_estimators = 710, feature importance sum = 1.000000\r\nAt n_estimators = 711, feature importance sum = 0.000000\r\nAt n_estimators = 712, feature importance sum = 0.000000\r\nAt n_estimators = 713, feature importance sum = 0.000000\r\nAt n_estimators = 714, feature importance sum = 0.000000\r\nAt n_estimators = 715, feature importance sum = 0.000000\r\nAt n_estimators = 716, feature importance sum = 0.000000\r\nAt n_estimators = 717, feature importance sum = 0.000000\r\nAt n_estimators = 718, feature importance sum = 0.000000\r\n...\r\n```\r\n\r\nI wonder if we’re hitting some floating point calculation error. \r\n\r\nBTW, I've posted this issue on the mailing list [Link](https://mail.python.org/pipermail/scikit-learn/2016-September/000508.html).  There aren't a lot of discussion, but others seem to think there's a bug here too.\r\n\r\nHope we can get this fixed or clarified.\r\n\r\nThank you!\r\n-Doug\r\n#### Versions\r\n\r\nWindows-7;'Python', '2.7.9 ;'NumPy', '1.9.2';'SciPy', '0.15.1';'Scikit-Learn', '0.16.1' \r\n\n",
  "hints_text": "thanks for the report. ping @pprett ?\n\nObservation:\nThe estimators with feature importance sum 0 have only 1 node which is being caused by the following [code](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_tree.pyx#L228-L229)\n`is_leaf = (is_leaf or (impurity <= min_impurity_split))`\n\n`is_leaf` is changing to 1 in the first iteration for such trees.\n\nFor estimator number 710\n`impurity = 0.00000010088593634382\nmin_impurity_split = 0.00000010000000000000`\n\nFor estimator number 711 (which has 0 feature importance sum)\n`impurity = 0.00000009983122597550\nmin_impurity_split = 0.00000010000000000000`\n\nThe same behavior is there for all subsequent estimators.\n\nVersions:\nUbuntu 16.04, Python: 2.7.10, Numpy: 1.11.2, Scipy: 0.18.1, Scikit-Learn: 0.19.dev0\n\nping @jmschrei maybe?\n\nIf the gini impurity at the first node is that low, you're not gaining anything by having more trees. I am unsure if this is a bug, except that it should maybe only look at the impurity of trees where a split improves things. \n\nSo should these extra trees (containing only one node) be considered while computing feature importance?\n\n@Naereen skipping them would be fine I guess, and would make them sum to one again?\n\nIMO, while computing trees, when gini impurity of the first node of a tree is lower than the threshold, the program shouldn't move forward. The actual number of trees computed should be communicated to the user. In example above, program should stop at 711 and number of estimators should be 710.\n\non master, the first part of the issue seems fixed. This part of the code now returns 1.0 as expected.\r\n```python\r\nfeature_importance_sum = np.sum(clf.feature_importances_)\r\nprint(\"At n_estimators = %i, feature importance sum = %f\" % (n_estimators , feature_importance_sum))\r\n```\r\nIt has been fixed in #11176.\r\nHowever, for the second part of the issue, the problem is still present. the trees that add up to 0.0 actually contain only 1 node and `feature_importances_` does not really make sens for those trees. Maybe something should be done to avoid having those trees.\nBy 1 node, you mean just a single leaf node, or a single split node? Yes, a tree with a single leaf node should not be contributing to feature importances.\n1 is the value returned by `node_count`. According to the docstring, it's internal nodes + leaves. So those trees are just a single leaf node.",
  "created_at": "2019-04-11T16:46:24Z",
  "version": "0.21",
  "FAIL_TO_PASS": "[\"sklearn/ensemble/tests/test_gradient_boosting.py::test_gbr_degenerate_feature_importances\"]",
  "PASS_TO_PASS": "[\"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_toy[deviance-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_toy[deviance-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_toy[deviance-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_toy[exponential-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_toy[exponential-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_toy[exponential-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classifier_parameter_checks\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_regressor_parameter_checks\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_loss_function\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_synthetic[deviance-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_synthetic[deviance-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_synthetic[deviance-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_synthetic[exponential-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_synthetic[exponential-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_synthetic[exponential-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[1.0-ls-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[1.0-ls-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[1.0-ls-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[1.0-lad-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[1.0-lad-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[1.0-lad-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[1.0-huber-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[1.0-huber-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[1.0-huber-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[0.5-ls-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[0.5-ls-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[0.5-ls-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[0.5-lad-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[0.5-lad-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[0.5-lad-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[0.5-huber-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[0.5-huber-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_boston[0.5-huber-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[None-1.0-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[None-1.0-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[None-1.0-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[None-0.5-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[None-0.5-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[None-0.5-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[1-1.0-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[1-1.0-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[1-1.0-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[1-0.5-auto]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[1-0.5-True]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[1-0.5-False]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_regression_synthetic\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_feature_importances\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_probability_log\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_check_inputs\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_check_inputs_predict\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_check_inputs_predict_stages\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_check_max_features\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_max_feature_regression\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_feature_importance_regression\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_max_feature_auto\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_staged_predict\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_staged_predict_proba\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_staged_functions_defensive[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_staged_functions_defensive[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_serialization\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_degenerate_targets\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_quantile_loss\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_symbol_labels\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_float_class_labels\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_shape_y\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_mem_layout\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_oob_improvement\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_oob_improvement_raise\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_oob_multilcass_iris\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_verbose_output\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_more_verbose_output\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_n_estimators[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_n_estimators[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_max_depth[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_max_depth[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_clear[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_clear[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_zero_n_estimators[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_zero_n_estimators[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_smaller_n_estimators[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_smaller_n_estimators[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_equal_n_estimators[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_equal_n_estimators[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_oob_switch[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_oob_switch[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_oob[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_oob[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_sparse[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_sparse[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_fortran[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_fortran[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_monitor_early_stopping[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_monitor_early_stopping[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_complete_classification\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_complete_regression\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_zero_estimator_reg\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_zero_estimator_clf\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_max_leaf_nodes_max_depth[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_max_leaf_nodes_max_depth[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_min_impurity_split[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_min_impurity_split[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_min_impurity_decrease[GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_min_impurity_decrease[GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_wo_nestimators_change\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_probability_exponential\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_non_uniform_weights_toy_edge_case_reg\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_non_uniform_weights_toy_edge_case_clf\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_sparse_input[csr_matrix-GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_sparse_input[csr_matrix-GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_sparse_input[csc_matrix-GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_sparse_input[csc_matrix-GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_sparse_input[coo_matrix-GradientBoostingClassifier]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_sparse_input[coo_matrix-GradientBoostingRegressor]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_early_stopping\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_validation_fraction\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_early_stopping_stratified\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_with_init[binary\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_with_init[multiclass\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_with_init[regression]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_with_init_pipeline\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_init_wrong_methods[estimator0-predict_proba]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_init_wrong_methods[estimator1-predict]\", \"sklearn/ensemble/tests/test_gradient_boosting.py::test_early_stopping_n_classes\"]",
  "environment_setup_commit": "7813f7efb5b2012412888b69e73d76f2df2b50b6",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.997511",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}