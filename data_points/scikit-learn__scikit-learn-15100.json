{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-15100",
  "base_commit": "af8a6e592a1a15d92d77011856d5aa0ec4db4c6c",
  "patch": "diff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py\n--- a/sklearn/feature_extraction/text.py\n+++ b/sklearn/feature_extraction/text.py\n@@ -129,10 +129,13 @@ def strip_accents_unicode(s):\n         Remove accentuated char for any unicode symbol that has a direct\n         ASCII equivalent.\n     \"\"\"\n-    normalized = unicodedata.normalize('NFKD', s)\n-    if normalized == s:\n+    try:\n+        # If `s` is ASCII-compatible, then it does not contain any accented\n+        # characters and we can avoid an expensive list comprehension\n+        s.encode(\"ASCII\", errors=\"strict\")\n         return s\n-    else:\n+    except UnicodeEncodeError:\n+        normalized = unicodedata.normalize('NFKD', s)\n         return ''.join([c for c in normalized if not unicodedata.combining(c)])\n \n \n",
  "test_patch": "diff --git a/sklearn/feature_extraction/tests/test_text.py b/sklearn/feature_extraction/tests/test_text.py\n--- a/sklearn/feature_extraction/tests/test_text.py\n+++ b/sklearn/feature_extraction/tests/test_text.py\n@@ -97,6 +97,21 @@ def test_strip_accents():\n     expected = 'this is a test'\n     assert strip_accents_unicode(a) == expected\n \n+    # strings that are already decomposed\n+    a = \"o\\u0308\"  # o with diaresis\n+    expected = \"o\"\n+    assert strip_accents_unicode(a) == expected\n+\n+    # combining marks by themselves\n+    a = \"\\u0300\\u0301\\u0302\\u0303\"\n+    expected = \"\"\n+    assert strip_accents_unicode(a) == expected\n+\n+    # Multiple combining marks on one character\n+    a = \"o\\u0308\\u0304\"\n+    expected = \"o\"\n+    assert strip_accents_unicode(a) == expected\n+\n \n def test_to_ascii():\n     # check some classical latin accentuated symbols\n",
  "problem_statement": "strip_accents_unicode fails to strip accents from strings that are already in NFKD form\n<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn\r\n- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn\r\nFor more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->\r\n\r\nThe `strip_accents=\"unicode\"` feature of `CountVectorizer` and related does not work as expected when it processes strings that contain accents, if those strings are already in NFKD form.\r\n\r\n#### Steps/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.feature_extraction.text import strip_accents_unicode\r\n\r\n# This string contains one code point, \"LATIN SMALL LETTER N WITH TILDE\"\r\ns1 = chr(241)\r\n\r\n# This string contains two code points, \"LATIN SMALL LETTER N\" followed by \"COMBINING TILDE\"\r\ns2 = chr(110) + chr(771)\r\n\r\n# They are visually identical, as expected\r\nprint(s1) # => ñ\r\nprint(s2) # => ñ\r\n\r\n# The tilde is removed from s1, as expected\r\nprint(strip_accents_unicode(s1)) # => n\r\n\r\n# But strip_accents_unicode returns s2 unchanged\r\nprint(strip_accents_unicode(s2) == s2) # => True\r\n```\r\n\r\n#### Expected Results\r\n\r\n`s1` and `s2` should both be normalized to the same string, `\"n\"`.\r\n\r\n#### Actual Results\r\n`s2` is not changed, because `strip_accent_unicode` does nothing if the string is already in NFKD form.\r\n\r\n#### Versions\r\n```\r\nSystem:\r\n    python: 3.7.4 (default, Jul  9 2019, 15:11:16)  [GCC 7.4.0]\r\nexecutable: /home/dgrady/.local/share/virtualenvs/profiling-data-exploration--DO1bU6C/bin/python3.7\r\n   machine: Linux-4.4.0-17763-Microsoft-x86_64-with-Ubuntu-18.04-bionic\r\n\r\nPython deps:\r\n       pip: 19.2.2\r\nsetuptools: 41.2.0\r\n   sklearn: 0.21.3\r\n     numpy: 1.17.2\r\n     scipy: 1.3.1\r\n    Cython: None\r\n    pandas: 0.25.1\r\n```\r\n\n",
  "hints_text": "Good catch. Are you able to provide a fix?\nIt looks like we should just remove the `if` branch from `strip_accents_unicode`:\r\n\r\n```python\r\ndef strip_accents_unicode(s):\r\n    normalized = unicodedata.normalize('NFKD', s)\r\n    return ''.join([c for c in normalized if not unicodedata.combining(c)])\r\n```\r\n\r\nIf that sounds good to you I can put together a PR shortly.\nA pr with that fix and some tests sounds very welcome.\n\nIndeed this is a bug and the solution proposed seems correct. +1 for a PR with a non-regression test.",
  "created_at": "2019-09-26T19:21:38Z",
  "version": "0.22",
  "FAIL_TO_PASS": "[\"sklearn/feature_extraction/tests/test_text.py::test_strip_accents\"]",
  "PASS_TO_PASS": "[\"sklearn/feature_extraction/tests/test_text.py::test_to_ascii\", \"sklearn/feature_extraction/tests/test_text.py::test_word_analyzer_unigrams[CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_word_analyzer_unigrams[HashingVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_word_analyzer_unigrams_and_bigrams\", \"sklearn/feature_extraction/tests/test_text.py::test_unicode_decode_error\", \"sklearn/feature_extraction/tests/test_text.py::test_char_ngram_analyzer\", \"sklearn/feature_extraction/tests/test_text.py::test_char_wb_ngram_analyzer\", \"sklearn/feature_extraction/tests/test_text.py::test_word_ngram_analyzer\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_custom_vocabulary\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_custom_vocabulary_pipeline\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_custom_vocabulary_repeated_indices\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_custom_vocabulary_gap_index\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_stop_words\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_empty_vocabulary\", \"sklearn/feature_extraction/tests/test_text.py::test_fit_countvectorizer_twice\", \"sklearn/feature_extraction/tests/test_text.py::test_tf_idf_smoothing\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_no_smoothing\", \"sklearn/feature_extraction/tests/test_text.py::test_sublinear_tf\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_setters\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_deprecationwarning\", \"sklearn/feature_extraction/tests/test_text.py::test_hashing_vectorizer\", \"sklearn/feature_extraction/tests/test_text.py::test_feature_names\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_max_features[CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_max_features[TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_count_vectorizer_max_features\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_max_df\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_min_df\", \"sklearn/feature_extraction/tests/test_text.py::test_count_binary_occurrences\", \"sklearn/feature_extraction/tests/test_text.py::test_hashed_binary_occurrences\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_inverse_transform[CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_inverse_transform[TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_count_vectorizer_pipeline_grid_selection\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_pipeline_grid_selection\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_pipeline_cross_validation\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_unicode\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_with_fixed_vocabulary\", \"sklearn/feature_extraction/tests/test_text.py::test_pickling_vectorizer\", \"sklearn/feature_extraction/tests/test_text.py::test_pickling_built_processors[build_analyzer]\", \"sklearn/feature_extraction/tests/test_text.py::test_pickling_built_processors[build_preprocessor]\", \"sklearn/feature_extraction/tests/test_text.py::test_pickling_built_processors[build_tokenizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_vocab_sets_when_pickling\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_vocab_dicts_when_pickling\", \"sklearn/feature_extraction/tests/test_text.py::test_stop_words_removal\", \"sklearn/feature_extraction/tests/test_text.py::test_pickling_transformer\", \"sklearn/feature_extraction/tests/test_text.py::test_transformer_idf_setter\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_setter\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidfvectorizer_invalid_idf_attr\", \"sklearn/feature_extraction/tests/test_text.py::test_non_unique_vocab\", \"sklearn/feature_extraction/tests/test_text.py::test_hashingvectorizer_nan_in_docs\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidfvectorizer_binary\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidfvectorizer_export_idf\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_vocab_clone\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_string_object_as_input[CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_string_object_as_input[TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_string_object_as_input[HashingVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_transformer_type[float32]\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_transformer_type[float64]\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_transformer_sparse\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_type[int32-float64-True]\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_type[int64-float64-True]\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_type[float32-float32-False]\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_type[float64-float64-False]\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizers_invalid_ngram_range[vec1]\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizers_invalid_ngram_range[vec2]\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_stop_words_inconsistent\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_sort_features_64bit_sparse_indices\", \"sklearn/feature_extraction/tests/test_text.py::test_stop_word_validation_custom_preprocessor[CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_stop_word_validation_custom_preprocessor[TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_stop_word_validation_custom_preprocessor[HashingVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_error[filename-FileNotFoundError--CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_error[filename-FileNotFoundError--TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_error[file-AttributeError-'str'\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[file-<lambda>0-CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[file-<lambda>0-TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[file-<lambda>0-HashingVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[file-<lambda>1-CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[file-<lambda>1-TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[file-<lambda>1-HashingVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[filename-<lambda>0-CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[filename-<lambda>0-TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[filename-<lambda>0-HashingVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[filename-<lambda>1-CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[filename-<lambda>1-TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[filename-<lambda>1-HashingVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_reraise_error[CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_reraise_error[TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_unused_parameters_warn[stop_words0-None-None-ngram_range0-None-char-'stop_words'-'analyzer'-!=\", \"sklearn/feature_extraction/tests/test_text.py::test_unused_parameters_warn[None-<lambda>-None-ngram_range1-None-char-'tokenizer'-'analyzer'-!=\", \"sklearn/feature_extraction/tests/test_text.py::test_unused_parameters_warn[None-<lambda>-None-ngram_range2-\\\\\\\\w+-word-'token_pattern'-'tokenizer'-is\", \"sklearn/feature_extraction/tests/test_text.py::test_unused_parameters_warn[None-None-<lambda>-ngram_range3-\\\\\\\\w+-<lambda>-'preprocessor'-'analyzer'-is\", \"sklearn/feature_extraction/tests/test_text.py::test_unused_parameters_warn[None-None-None-ngram_range4-None-<lambda>-'ngram_range'-'analyzer'-is\", \"sklearn/feature_extraction/tests/test_text.py::test_unused_parameters_warn[None-None-None-ngram_range5-\\\\\\\\w+-char-'token_pattern'-'analyzer'-!=\"]",
  "environment_setup_commit": "7e85a6d1f038bbb932b36f18d75df6be937ed00d",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:31.011466",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}