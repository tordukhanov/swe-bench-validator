{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-10397",
  "base_commit": "2eb731b375fa0b48f6902daa839ff6a8477b48fd",
  "patch": "diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py\n--- a/sklearn/linear_model/ridge.py\n+++ b/sklearn/linear_model/ridge.py\n@@ -778,6 +778,7 @@ class RidgeClassifier(LinearClassifierMixin, _BaseRidge):\n     a one-versus-all approach. Concretely, this is implemented by taking\n     advantage of the multi-variate response support in Ridge.\n     \"\"\"\n+\n     def __init__(self, alpha=1.0, fit_intercept=True, normalize=False,\n                  copy_X=True, max_iter=None, tol=1e-3, class_weight=None,\n                  solver=\"auto\", random_state=None):\n@@ -1041,11 +1042,16 @@ def fit(self, X, y, sample_weight=None):\n         scorer = check_scoring(self, scoring=self.scoring, allow_none=True)\n         error = scorer is None\n \n+        if np.any(self.alphas < 0):\n+            raise ValueError(\"alphas cannot be negative. \"\n+                             \"Got {} containing some \"\n+                             \"negative value instead.\".format(self.alphas))\n+\n         for i, alpha in enumerate(self.alphas):\n             if error:\n-                out, c = _errors(alpha, y, v, Q, QT_y)\n+                out, c = _errors(float(alpha), y, v, Q, QT_y)\n             else:\n-                out, c = _values(alpha, y, v, Q, QT_y)\n+                out, c = _values(float(alpha), y, v, Q, QT_y)\n             cv_values[:, i] = out.ravel()\n             C.append(c)\n \n@@ -1085,7 +1091,7 @@ def __init__(self, alphas=(0.1, 1.0, 10.0),\n                  fit_intercept=True, normalize=False, scoring=None,\n                  cv=None, gcv_mode=None,\n                  store_cv_values=False):\n-        self.alphas = alphas\n+        self.alphas = np.asarray(alphas)\n         self.fit_intercept = fit_intercept\n         self.normalize = normalize\n         self.scoring = scoring\n@@ -1328,6 +1334,7 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n     a one-versus-all approach. Concretely, this is implemented by taking\n     advantage of the multi-variate response support in Ridge.\n     \"\"\"\n+\n     def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,\n                  normalize=False, scoring=None, cv=None, class_weight=None):\n         super(RidgeClassifierCV, self).__init__(\n",
  "test_patch": "diff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py\n--- a/sklearn/linear_model/tests/test_ridge.py\n+++ b/sklearn/linear_model/tests/test_ridge.py\n@@ -11,6 +11,7 @@\n from sklearn.utils.testing import assert_greater\n from sklearn.utils.testing import assert_raises\n from sklearn.utils.testing import assert_raise_message\n+from sklearn.utils.testing import assert_raises_regex\n from sklearn.utils.testing import ignore_warnings\n from sklearn.utils.testing import assert_warns\n \n@@ -51,6 +52,7 @@\n X_iris = sp.csr_matrix(iris.data)\n y_iris = iris.target\n \n+\n DENSE_FILTER = lambda X: X\n SPARSE_FILTER = lambda X: sp.csr_matrix(X)\n \n@@ -704,6 +706,34 @@ def test_sparse_design_with_sample_weights():\n                                       decimal=6)\n \n \n+def test_ridgecv_int_alphas():\n+    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n+                  [1.0, 1.0], [1.0, 0.0]])\n+    y = [1, 1, 1, -1, -1]\n+\n+    # Integers\n+    ridge = RidgeCV(alphas=(1, 10, 100))\n+    ridge.fit(X, y)\n+\n+\n+def test_ridgecv_negative_alphas():\n+    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n+                  [1.0, 1.0], [1.0, 0.0]])\n+    y = [1, 1, 1, -1, -1]\n+\n+    # Negative integers\n+    ridge = RidgeCV(alphas=(-1, -10, -100))\n+    assert_raises_regex(ValueError,\n+                        \"alphas cannot be negative.\",\n+                        ridge.fit, X, y)\n+\n+    # Negative floats\n+    ridge = RidgeCV(alphas=(-0.1, -1.0, -10.0))\n+    assert_raises_regex(ValueError,\n+                        \"alphas cannot be negative.\",\n+                        ridge.fit, X, y)\n+\n+\n def test_raises_value_error_if_solver_not_supported():\n     # Tests whether a ValueError is raised if a non-identified solver\n     # is passed to ridge_regression\n",
  "problem_statement": "integers in RidgeCV alpha\n```python\r\nfrom sklearn.linear_model import RidgeCV\r\nfrom sklearn.datasets import make_regression\r\n\r\nX, y = make_regression()\r\nridge = RidgeCV(alphas=[1, 10, 100, 1000]).fit(X, y)\r\n```\r\n\r\n> ValueError: Integers to negative integer powers are not allowed.\r\n\r\nmaking one of the alphas a float fixes the problem. This should be handled internally.\r\nPython3.6\nintegers in RidgeCV alpha\n```python\r\nfrom sklearn.linear_model import RidgeCV\r\nfrom sklearn.datasets import make_regression\r\n\r\nX, y = make_regression()\r\nridge = RidgeCV(alphas=[1, 10, 100, 1000]).fit(X, y)\r\n```\r\n\r\n> ValueError: Integers to negative integer powers are not allowed.\r\n\r\nmaking one of the alphas a float fixes the problem. This should be handled internally.\r\nPython3.6\n",
  "hints_text": "Can I take this?\nI think so, but maybe after that you should have a go at non \"good first issue\"s!\nCan I take this?\nI think so, but maybe after that you should have a go at non \"good first issue\"s!",
  "created_at": "2018-01-03T18:27:12Z",
  "version": "0.20",
  "FAIL_TO_PASS": "[\"sklearn/linear_model/tests/test_ridge.py::test_ridgecv_int_alphas\", \"sklearn/linear_model/tests/test_ridge.py::test_ridgecv_negative_alphas\"]",
  "PASS_TO_PASS": "[\"sklearn/linear_model/tests/test_ridge.py::test_ridge\", \"sklearn/linear_model/tests/test_ridge.py::test_primal_dual_relationship\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_singular\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_sample_weights\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_convergence_fail\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_sample_weights\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_shapes\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_intercept\", \"sklearn/linear_model/tests/test_ridge.py::test_toy_ridge_object\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_vs_lstsq\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_individual_penalties\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_cv_sparse_svd\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_sparse_svd\", \"sklearn/linear_model/tests/test_ridge.py::test_class_weights\", \"sklearn/linear_model/tests/test_ridge.py::test_class_weight_vs_sample_weight\", \"sklearn/linear_model/tests/test_ridge.py::test_class_weights_cv\", \"sklearn/linear_model/tests/test_ridge.py::test_ridgecv_store_cv_values\", \"sklearn/linear_model/tests/test_ridge.py::test_ridgecv_sample_weight\", \"sklearn/linear_model/tests/test_ridge.py::test_raises_value_error_if_sample_weights_greater_than_1d\", \"sklearn/linear_model/tests/test_ridge.py::test_sparse_design_with_sample_weights\", \"sklearn/linear_model/tests/test_ridge.py::test_raises_value_error_if_solver_not_supported\", \"sklearn/linear_model/tests/test_ridge.py::test_sparse_cg_max_iter\", \"sklearn/linear_model/tests/test_ridge.py::test_n_iter\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_fit_intercept_sparse\", \"sklearn/linear_model/tests/test_ridge.py::test_errors_and_values_helper\", \"sklearn/linear_model/tests/test_ridge.py::test_errors_and_values_svd_helper\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_classifier_no_support_multilabel\", \"sklearn/linear_model/tests/test_ridge.py::test_dtype_match\", \"sklearn/linear_model/tests/test_ridge.py::test_dtype_match_cholesky\"]",
  "environment_setup_commit": "55bf5d93e5674f13a1134d93a11fd0cd11aabcd1",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.950206",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}