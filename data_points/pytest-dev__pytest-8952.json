{
  "repo": "pytest-dev/pytest",
  "instance_id": "pytest-dev__pytest-8952",
  "base_commit": "6d6bc97231f2d9a68002f1d191828fd3476ca8b8",
  "patch": "diff --git a/src/_pytest/pytester.py b/src/_pytest/pytester.py\n--- a/src/_pytest/pytester.py\n+++ b/src/_pytest/pytester.py\n@@ -588,6 +588,7 @@ def assert_outcomes(\n         errors: int = 0,\n         xpassed: int = 0,\n         xfailed: int = 0,\n+        warnings: int = 0,\n     ) -> None:\n         \"\"\"Assert that the specified outcomes appear with the respective\n         numbers (0 means it didn't occur) in the text output from a test run.\"\"\"\n@@ -603,6 +604,7 @@ def assert_outcomes(\n             errors=errors,\n             xpassed=xpassed,\n             xfailed=xfailed,\n+            warnings=warnings,\n         )\n \n \ndiff --git a/src/_pytest/pytester_assertions.py b/src/_pytest/pytester_assertions.py\n--- a/src/_pytest/pytester_assertions.py\n+++ b/src/_pytest/pytester_assertions.py\n@@ -42,6 +42,7 @@ def assert_outcomes(\n     errors: int = 0,\n     xpassed: int = 0,\n     xfailed: int = 0,\n+    warnings: int = 0,\n ) -> None:\n     \"\"\"Assert that the specified outcomes appear with the respective\n     numbers (0 means it didn't occur) in the text output from a test run.\"\"\"\n@@ -54,6 +55,7 @@ def assert_outcomes(\n         \"errors\": outcomes.get(\"errors\", 0),\n         \"xpassed\": outcomes.get(\"xpassed\", 0),\n         \"xfailed\": outcomes.get(\"xfailed\", 0),\n+        \"warnings\": outcomes.get(\"warnings\", 0),\n     }\n     expected = {\n         \"passed\": passed,\n@@ -62,5 +64,6 @@ def assert_outcomes(\n         \"errors\": errors,\n         \"xpassed\": xpassed,\n         \"xfailed\": xfailed,\n+        \"warnings\": warnings,\n     }\n     assert obtained == expected\n",
  "test_patch": "diff --git a/testing/test_nose.py b/testing/test_nose.py\n--- a/testing/test_nose.py\n+++ b/testing/test_nose.py\n@@ -335,7 +335,7 @@ def test_failing():\n         \"\"\"\n     )\n     result = pytester.runpytest(p)\n-    result.assert_outcomes(skipped=1)\n+    result.assert_outcomes(skipped=1, warnings=1)\n \n \n def test_SkipTest_in_test(pytester: Pytester) -> None:\ndiff --git a/testing/test_pytester.py b/testing/test_pytester.py\n--- a/testing/test_pytester.py\n+++ b/testing/test_pytester.py\n@@ -847,3 +847,17 @@ def test_testdir_makefile_ext_empty_string_makes_file(testdir) -> None:\n     \"\"\"For backwards compat #8192\"\"\"\n     p1 = testdir.makefile(\"\", \"\")\n     assert \"test_testdir_makefile\" in str(p1)\n+\n+\n+@pytest.mark.filterwarnings(\"default\")\n+def test_pytester_assert_outcomes_warnings(pytester: Pytester) -> None:\n+    pytester.makepyfile(\n+        \"\"\"\n+        import warnings\n+\n+        def test_with_warning():\n+            warnings.warn(UserWarning(\"some custom warning\"))\n+        \"\"\"\n+    )\n+    result = pytester.runpytest()\n+    result.assert_outcomes(passed=1, warnings=1)\n",
  "problem_statement": "Enhance `RunResult` warning assertion capabilities\nwhile writing some other bits and pieces, I had a use case for checking the `warnings` omitted, `RunResult` has a `assert_outcomes()` that doesn't quite offer `warnings=` yet the information is already available in there, I suspect there is a good reason why we don't have `assert_outcomes(warnings=...)` so I propose some additional capabilities on `RunResult` to handle warnings in isolation.\r\n\r\nWith `assert_outcomes()` the full dict comparison may get a bit intrusive as far as warning capture is concerned.\r\n\r\nsomething simple like:\r\n\r\n```python\r\nresult = pytester.runpytest(...)\r\nresult.assert_warnings(count=1)\r\n```\r\n\r\nThoughts?\n",
  "hints_text": "",
  "created_at": "2021-07-28T21:11:34Z",
  "version": "7.0",
  "FAIL_TO_PASS": "[\"testing/test_pytester.py::test_pytester_assert_outcomes_warnings\"]",
  "PASS_TO_PASS": "[\"testing/test_pytester.py::test_hookrecorder_basic[apiclass]\", \"testing/test_pytester.py::test_hookrecorder_basic[api]\", \"testing/test_pytester.py::TestSysModulesSnapshot::test_remove_added\", \"testing/test_pytester.py::TestSysModulesSnapshot::test_add_removed\", \"testing/test_pytester.py::TestSysModulesSnapshot::test_restore_reloaded\", \"testing/test_pytester.py::TestSysModulesSnapshot::test_preserve_modules\", \"testing/test_pytester.py::TestSysModulesSnapshot::test_preserve_container\", \"testing/test_pytester.py::TestSysPathsSnapshot::test_restore[path]\", \"testing/test_pytester.py::TestSysPathsSnapshot::test_restore[meta_path]\", \"testing/test_pytester.py::TestSysPathsSnapshot::test_preserve_container[path]\", \"testing/test_pytester.py::TestSysPathsSnapshot::test_preserve_container[meta_path]\", \"testing/test_pytester.py::test_linematcher_with_nonlist\", \"testing/test_pytester.py::test_linematcher_match_failure\", \"testing/test_pytester.py::test_linematcher_consecutive\", \"testing/test_pytester.py::test_linematcher_no_matching[no_fnmatch_line]\", \"testing/test_pytester.py::test_linematcher_no_matching[no_re_match_line]\", \"testing/test_pytester.py::test_linematcher_no_matching_after_match\", \"testing/test_pytester.py::test_linematcher_string_api\", \"testing/test_pytester.py::test_pytest_addopts_before_pytester\", \"testing/test_pytester.py::test_run_result_repr\", \"testing/test_pytester.py::test_parse_summary_line_always_plural\", \"testing/test_pytester.py::test_parseconfig\", \"testing/test_pytester.py::test_pytester_runs_with_plugin\", \"testing/test_pytester.py::test_pytester_with_doctest\", \"testing/test_pytester.py::test_runresult_assertion_on_xfail\", \"testing/test_pytester.py::test_runresult_assertion_on_xpassed\", \"testing/test_pytester.py::test_xpassed_with_strict_is_considered_a_failure\", \"testing/test_pytester.py::test_makepyfile_unicode\", \"testing/test_pytester.py::test_makepyfile_utf8\", \"testing/test_pytester.py::TestInlineRunModulesCleanup::test_inline_run_test_module_not_cleaned_up\", \"testing/test_pytester.py::TestInlineRunModulesCleanup::test_inline_run_taking_and_restoring_a_sys_modules_snapshot\", \"testing/test_pytester.py::TestInlineRunModulesCleanup::test_inline_run_sys_modules_snapshot_restore_preserving_modules\", \"testing/test_pytester.py::TestInlineRunModulesCleanup::test_external_test_module_imports_not_cleaned_up\", \"testing/test_pytester.py::test_assert_outcomes_after_pytest_error\", \"testing/test_pytester.py::test_cwd_snapshot\", \"testing/test_pytester.py::test_pytester_subprocess_via_runpytest_arg\", \"testing/test_pytester.py::test_unicode_args\", \"testing/test_pytester.py::test_run_stdin\", \"testing/test_pytester.py::test_popen_stdin_pipe\", \"testing/test_pytester.py::test_popen_stdin_bytes\", \"testing/test_pytester.py::test_popen_default_stdin_stderr_and_stdin_None\", \"testing/test_pytester.py::test_pytester_outcomes_with_multiple_errors\", \"testing/test_pytester.py::test_makefile_joins_absolute_path\", \"testing/test_pytester.py::test_testtmproot\", \"testing/test_pytester.py::test_testdir_makefile_dot_prefixes_extension_silently\", \"testing/test_pytester.py::test_pytester_makefile_dot_prefixes_extension_with_warning\", \"testing/test_pytester.py::test_testdir_makefile_ext_none_raises_type_error\", \"testing/test_pytester.py::test_testdir_makefile_ext_empty_string_makes_file\", \"testing/test_pytester.py::test_pytester_subprocess\", \"testing/test_pytester.py::test_pytester_run_no_timeout\", \"testing/test_pytester.py::test_pytester_run_with_timeout\", \"testing/test_pytester.py::test_pytester_run_timeout_expires\"]",
  "environment_setup_commit": "e2ee3144ed6e241dea8d96215fcdca18b3892551",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.945088",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}