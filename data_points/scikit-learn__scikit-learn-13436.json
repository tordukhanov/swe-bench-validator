{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-13436",
  "base_commit": "77b73d63d05bc198ba89193582aee93cae1f69a4",
  "patch": "diff --git a/sklearn/base.py b/sklearn/base.py\n--- a/sklearn/base.py\n+++ b/sklearn/base.py\n@@ -8,6 +8,7 @@\n from collections import defaultdict\n import platform\n import inspect\n+import re\n \n import numpy as np\n \n@@ -233,10 +234,13 @@ def set_params(self, **params):\n \n         return self\n \n-    def __repr__(self):\n+    def __repr__(self, N_CHAR_MAX=700):\n+        # N_CHAR_MAX is the (approximate) maximum number of non-blank\n+        # characters to render. We pass it as an optional parameter to ease\n+        # the tests.\n+\n         from .utils._pprint import _EstimatorPrettyPrinter\n \n-        N_CHAR_MAX = 700  # number of non-whitespace or newline chars\n         N_MAX_ELEMENTS_TO_SHOW = 30  # number of elements to show in sequences\n \n         # use ellipsis for sequences with a lot of elements\n@@ -246,10 +250,37 @@ def __repr__(self):\n \n         repr_ = pp.pformat(self)\n \n-        # Use bruteforce ellipsis if string is very long\n-        if len(''.join(repr_.split())) > N_CHAR_MAX:  # check non-blank chars\n-            lim = N_CHAR_MAX // 2\n-            repr_ = repr_[:lim] + '...' + repr_[-lim:]\n+        # Use bruteforce ellipsis when there are a lot of non-blank characters\n+        n_nonblank = len(''.join(repr_.split()))\n+        if n_nonblank > N_CHAR_MAX:\n+            lim = N_CHAR_MAX // 2  # apprx number of chars to keep on both ends\n+            regex = r'^(\\s*\\S){%d}' % lim\n+            # The regex '^(\\s*\\S){%d}' % n\n+            # matches from the start of the string until the nth non-blank\n+            # character:\n+            # - ^ matches the start of string\n+            # - (pattern){n} matches n repetitions of pattern\n+            # - \\s*\\S matches a non-blank char following zero or more blanks\n+            left_lim = re.match(regex, repr_).end()\n+            right_lim = re.match(regex, repr_[::-1]).end()\n+\n+            if '\\n' in repr_[left_lim:-right_lim]:\n+                # The left side and right side aren't on the same line.\n+                # To avoid weird cuts, e.g.:\n+                # categoric...ore',\n+                # we need to start the right side with an appropriate newline\n+                # character so that it renders properly as:\n+                # categoric...\n+                # handle_unknown='ignore',\n+                # so we add [^\\n]*\\n which matches until the next \\n\n+                regex += r'[^\\n]*\\n'\n+                right_lim = re.match(regex, repr_[::-1]).end()\n+\n+            ellipsis = '...'\n+            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n+                # Only add ellipsis if it results in a shorter repr\n+                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n+\n         return repr_\n \n     def __getstate__(self):\n",
  "test_patch": "diff --git a/sklearn/utils/tests/test_pprint.py b/sklearn/utils/tests/test_pprint.py\n--- a/sklearn/utils/tests/test_pprint.py\n+++ b/sklearn/utils/tests/test_pprint.py\n@@ -459,16 +459,78 @@ def test_n_max_elements_to_show():\n     assert  pp.pformat(gs) == expected\n \n \n-def test_length_constraint():\n-    # When repr is still too long, use bruteforce ellipsis\n-    # repr is a very long line so we don't check for equality here, just that\n-    # ellipsis has been done. It's not the ellipsis from before because the\n-    # number of elements in the dict is only 1.\n-    vocabulary = {0: 'hello' * 1000}\n-    vectorizer = CountVectorizer(vocabulary=vocabulary)\n-    repr_ = vectorizer.__repr__()\n-    assert '...' in repr_\n+def test_bruteforce_ellipsis():\n+    # Check that the bruteforce ellipsis (used when the number of non-blank\n+    # characters exceeds N_CHAR_MAX) renders correctly.\n+\n+    lr = LogisticRegression()\n+\n+    # test when the left and right side of the ellipsis aren't on the same\n+    # line.\n+    expected = \"\"\"\n+LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n+                   in...\n+                   multi_class='warn', n_jobs=None, penalty='l2',\n+                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n+                   warm_start=False)\"\"\"\n+\n+    expected = expected[1:]  # remove first \\n\n+    assert expected == lr.__repr__(N_CHAR_MAX=150)\n+\n+    # test with very small N_CHAR_MAX\n+    # Note that N_CHAR_MAX is not strictly enforced, but it's normal: to avoid\n+    # weird reprs we still keep the whole line of the right part (after the\n+    # ellipsis).\n+    expected = \"\"\"\n+Lo...\n+                   warm_start=False)\"\"\"\n+\n+    expected = expected[1:]  # remove first \\n\n+    assert expected == lr.__repr__(N_CHAR_MAX=4)\n+\n+    # test with N_CHAR_MAX == number of non-blank characters: In this case we\n+    # don't want ellipsis\n+    full_repr = lr.__repr__(N_CHAR_MAX=float('inf'))\n+    n_nonblank = len(''.join(full_repr.split()))\n+    assert lr.__repr__(N_CHAR_MAX=n_nonblank) == full_repr\n+    assert '...' not in full_repr\n+\n+    # test with N_CHAR_MAX == number of non-blank characters - 10: the left and\n+    # right side of the ellispsis are on different lines. In this case we\n+    # want to expend the whole line of the right side\n+    expected = \"\"\"\n+LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n+                   intercept_scaling=1, l1_ratio=None, max_i...\n+                   multi_class='warn', n_jobs=None, penalty='l2',\n+                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n+                   warm_start=False)\"\"\"\n+    expected = expected[1:]  # remove first \\n\n+    assert expected == lr.__repr__(N_CHAR_MAX=n_nonblank - 10)\n+\n+    # test with N_CHAR_MAX == number of non-blank characters - 10: the left and\n+    # right side of the ellispsis are on the same line. In this case we don't\n+    # want to expend the whole line of the right side, just add the ellispsis\n+    # between the 2 sides.\n+    expected = \"\"\"\n+LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n+                   intercept_scaling=1, l1_ratio=None, max_iter...,\n+                   multi_class='warn', n_jobs=None, penalty='l2',\n+                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n+                   warm_start=False)\"\"\"\n+    expected = expected[1:]  # remove first \\n\n+    assert expected == lr.__repr__(N_CHAR_MAX=n_nonblank - 4)\n \n+    # test with N_CHAR_MAX == number of non-blank characters - 2: the left and\n+    # right side of the ellispsis are on the same line, but adding the ellipsis\n+    # would actually make the repr longer. So we don't add the ellipsis.\n+    expected = \"\"\"\n+LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n+                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n+                   multi_class='warn', n_jobs=None, penalty='l2',\n+                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n+                   warm_start=False)\"\"\"\n+    expected = expected[1:]  # remove first \\n\n+    assert expected == lr.__repr__(N_CHAR_MAX=n_nonblank - 2)\n \n def test_builtin_prettyprinter():\n     # non regression test than ensures we can still use the builtin\n",
  "problem_statement": "Confusing pretty print repr for nested Pipeline\nTaking the examples from the docs (https://scikit-learn.org/dev/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py) that involves some nested pipelines in columntransformer in pipeline\r\n\r\n```py\r\nfrom sklearn.compose import ColumnTransformer\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.impute import SimpleImputer\r\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\r\nfrom sklearn.linear_model import LogisticRegression\r\n\r\nnumeric_features = ['age', 'fare']\r\nnumeric_transformer = Pipeline(steps=[\r\n    ('imputer', SimpleImputer(strategy='median')),\r\n    ('scaler', StandardScaler())])\r\n\r\ncategorical_features = ['embarked', 'sex', 'pclass']\r\ncategorical_transformer = Pipeline(steps=[\r\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\r\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\r\n\r\npreprocessor = ColumnTransformer(\r\n    transformers=[\r\n        ('num', numeric_transformer, numeric_features),\r\n        ('cat', categorical_transformer, categorical_features)])\r\n\r\nclf = Pipeline(steps=[('preprocessor', preprocessor),\r\n                      ('classifier', LogisticRegression(solver='lbfgs'))])\r\n```\r\n\r\nThe repr that you get for this pipeline:\r\n\r\n```py\r\nIn [8]: clf\r\nOut[8]: \r\nPipeline(memory=None,\r\n         steps=[('preprocessor',\r\n                 ColumnTransformer(n_jobs=None, remainder='drop',\r\n                                   sparse_threshold=0.3,\r\n                                   transformer_weights=None,\r\n                                   transformers=[('num',\r\n                                                  Pipe...cept_scaling=1,\r\n                                    l1_ratio=None, max_iter=100,\r\n                                    multi_class='warn', n_jobs=None,\r\n                                    penalty='l2', random_state=None,\r\n                                    solver='lbfgs', tol=0.0001, verbose=0,\r\n                                    warm_start=False))])\r\n```\r\n\r\nwhich I found very confusing: the outer pipeline seems to have only 1 step (the 'preprocessor', as the 'classifier' disappeared in the `...`).\r\n\r\nIt's probably certainly not easy to get a good repr in all cases, and for sure the old behaviour was even worse (it would show the first 'imputer' step of the pipeline inside the column transformer as if it was the second step of the outer pipeline ..). But just opening this issue as a data point for possible improvements.\r\n\r\nWithout knowing how the current repr is determined: ideally I would expect that, if the full repr is too long, we first try to trim it step per step of the outer pipeline, so that the structure of that outer pipeline is still visible. But that is easier to write than to code .. :)\r\n\r\ncc @NicolasHug \n",
  "hints_text": "",
  "created_at": "2019-03-12T14:40:46Z",
  "version": "0.21",
  "FAIL_TO_PASS": "[\"sklearn/utils/tests/test_pprint.py::test_bruteforce_ellipsis\"]",
  "PASS_TO_PASS": "[\"sklearn/utils/tests/test_pprint.py::test_basic\", \"sklearn/utils/tests/test_pprint.py::test_changed_only\", \"sklearn/utils/tests/test_pprint.py::test_pipeline\", \"sklearn/utils/tests/test_pprint.py::test_deeply_nested\", \"sklearn/utils/tests/test_pprint.py::test_gridsearch\", \"sklearn/utils/tests/test_pprint.py::test_gridsearch_pipeline\", \"sklearn/utils/tests/test_pprint.py::test_n_max_elements_to_show\", \"sklearn/utils/tests/test_pprint.py::test_builtin_prettyprinter\"]",
  "environment_setup_commit": "7813f7efb5b2012412888b69e73d76f2df2b50b6",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.993794",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}