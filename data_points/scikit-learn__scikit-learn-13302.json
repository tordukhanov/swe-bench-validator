{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-13302",
  "base_commit": "4de404d46d24805ff48ad255ec3169a5155986f0",
  "patch": "diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py\n--- a/sklearn/linear_model/ridge.py\n+++ b/sklearn/linear_model/ridge.py\n@@ -226,9 +226,17 @@ def _solve_svd(X, y, alpha):\n     return np.dot(Vt.T, d_UT_y).T\n \n \n+def _get_valid_accept_sparse(is_X_sparse, solver):\n+    if is_X_sparse and solver in ['auto', 'sag', 'saga']:\n+        return 'csr'\n+    else:\n+        return ['csr', 'csc', 'coo']\n+\n+\n def ridge_regression(X, y, alpha, sample_weight=None, solver='auto',\n                      max_iter=None, tol=1e-3, verbose=0, random_state=None,\n-                     return_n_iter=False, return_intercept=False):\n+                     return_n_iter=False, return_intercept=False,\n+                     check_input=True):\n     \"\"\"Solve the ridge equation by the method of normal equations.\n \n     Read more in the :ref:`User Guide <ridge_regression>`.\n@@ -332,6 +340,11 @@ def ridge_regression(X, y, alpha, sample_weight=None, solver='auto',\n \n         .. versionadded:: 0.17\n \n+    check_input : boolean, default True\n+        If False, the input arrays X and y will not be checked.\n+\n+        .. versionadded:: 0.21\n+\n     Returns\n     -------\n     coef : array, shape = [n_features] or [n_targets, n_features]\n@@ -360,13 +373,14 @@ def ridge_regression(X, y, alpha, sample_weight=None, solver='auto',\n                              return_n_iter=return_n_iter,\n                              return_intercept=return_intercept,\n                              X_scale=None,\n-                             X_offset=None)\n+                             X_offset=None,\n+                             check_input=check_input)\n \n \n def _ridge_regression(X, y, alpha, sample_weight=None, solver='auto',\n                       max_iter=None, tol=1e-3, verbose=0, random_state=None,\n                       return_n_iter=False, return_intercept=False,\n-                      X_scale=None, X_offset=None):\n+                      X_scale=None, X_offset=None, check_input=True):\n \n     has_sw = sample_weight is not None\n \n@@ -388,17 +402,12 @@ def _ridge_regression(X, y, alpha, sample_weight=None, solver='auto',\n                          \"intercept. Please change solver to 'sag' or set \"\n                          \"return_intercept=False.\")\n \n-    _dtype = [np.float64, np.float32]\n-\n-    # SAG needs X and y columns to be C-contiguous and np.float64\n-    if solver in ['sag', 'saga']:\n-        X = check_array(X, accept_sparse=['csr'],\n-                        dtype=np.float64, order='C')\n-        y = check_array(y, dtype=np.float64, ensure_2d=False, order='F')\n-    else:\n-        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'],\n-                        dtype=_dtype)\n-        y = check_array(y, dtype=X.dtype, ensure_2d=False)\n+    if check_input:\n+        _dtype = [np.float64, np.float32]\n+        _accept_sparse = _get_valid_accept_sparse(sparse.issparse(X), solver)\n+        X = check_array(X, accept_sparse=_accept_sparse, dtype=_dtype,\n+                        order=\"C\")\n+        y = check_array(y, dtype=X.dtype, ensure_2d=False, order=\"C\")\n     check_consistent_length(X, y)\n \n     n_samples, n_features = X.shape\n@@ -417,8 +426,6 @@ def _ridge_regression(X, y, alpha, sample_weight=None, solver='auto',\n         raise ValueError(\"Number of samples in X and y does not correspond:\"\n                          \" %d != %d\" % (n_samples, n_samples_))\n \n-\n-\n     if has_sw:\n         if np.atleast_1d(sample_weight).ndim > 1:\n             raise ValueError(\"Sample weights must be 1D array or scalar\")\n@@ -438,7 +445,6 @@ def _ridge_regression(X, y, alpha, sample_weight=None, solver='auto',\n     if alpha.size == 1 and n_targets > 1:\n         alpha = np.repeat(alpha, n_targets)\n \n-\n     n_iter = None\n     if solver == 'sparse_cg':\n         coef = _solve_sparse_cg(X, y, alpha,\n@@ -461,7 +467,6 @@ def _ridge_regression(X, y, alpha, sample_weight=None, solver='auto',\n             except linalg.LinAlgError:\n                 # use SVD solver if matrix is singular\n                 solver = 'svd'\n-\n         else:\n             try:\n                 coef = _solve_cholesky(X, y, alpha)\n@@ -473,11 +478,12 @@ def _ridge_regression(X, y, alpha, sample_weight=None, solver='auto',\n         # precompute max_squared_sum for all targets\n         max_squared_sum = row_norms(X, squared=True).max()\n \n-        coef = np.empty((y.shape[1], n_features))\n+        coef = np.empty((y.shape[1], n_features), dtype=X.dtype)\n         n_iter = np.empty(y.shape[1], dtype=np.int32)\n-        intercept = np.zeros((y.shape[1], ))\n+        intercept = np.zeros((y.shape[1], ), dtype=X.dtype)\n         for i, (alpha_i, target) in enumerate(zip(alpha, y.T)):\n-            init = {'coef': np.zeros((n_features + int(return_intercept), 1))}\n+            init = {'coef': np.zeros((n_features + int(return_intercept), 1),\n+                                     dtype=X.dtype)}\n             coef_, n_iter_, _ = sag_solver(\n                 X, target.ravel(), sample_weight, 'squared', alpha_i, 0,\n                 max_iter, tol, verbose, random_state, False, max_squared_sum,\n@@ -530,13 +536,13 @@ def __init__(self, alpha=1.0, fit_intercept=True, normalize=False,\n \n     def fit(self, X, y, sample_weight=None):\n \n-        if self.solver in ('sag', 'saga'):\n-            _dtype = np.float64\n-        else:\n-            # all other solvers work at both float precision levels\n-            _dtype = [np.float64, np.float32]\n-\n-        X, y = check_X_y(X, y, ['csr', 'csc', 'coo'], dtype=_dtype,\n+        # all other solvers work at both float precision levels\n+        _dtype = [np.float64, np.float32]\n+        _accept_sparse = _get_valid_accept_sparse(sparse.issparse(X),\n+                                                  self.solver)\n+        X, y = check_X_y(X, y,\n+                         accept_sparse=_accept_sparse,\n+                         dtype=_dtype,\n                          multi_output=True, y_numeric=True)\n \n         if ((sample_weight is not None) and\n@@ -555,7 +561,7 @@ def fit(self, X, y, sample_weight=None):\n                 X, y, alpha=self.alpha, sample_weight=sample_weight,\n                 max_iter=self.max_iter, tol=self.tol, solver=self.solver,\n                 random_state=self.random_state, return_n_iter=True,\n-                return_intercept=True)\n+                return_intercept=True, check_input=False)\n             # add the offset which was subtracted by _preprocess_data\n             self.intercept_ += y_offset\n         else:\n@@ -570,8 +576,7 @@ def fit(self, X, y, sample_weight=None):\n                 X, y, alpha=self.alpha, sample_weight=sample_weight,\n                 max_iter=self.max_iter, tol=self.tol, solver=self.solver,\n                 random_state=self.random_state, return_n_iter=True,\n-                return_intercept=False, **params)\n-\n+                return_intercept=False, check_input=False, **params)\n             self._set_intercept(X_offset, y_offset, X_scale)\n \n         return self\n@@ -893,8 +898,9 @@ def fit(self, X, y, sample_weight=None):\n         -------\n         self : returns an instance of self.\n         \"\"\"\n-        check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n-                  multi_output=True)\n+        _accept_sparse = _get_valid_accept_sparse(sparse.issparse(X),\n+                                                  self.solver)\n+        check_X_y(X, y, accept_sparse=_accept_sparse, multi_output=True)\n \n         self._label_binarizer = LabelBinarizer(pos_label=1, neg_label=-1)\n         Y = self._label_binarizer.fit_transform(y)\n@@ -1077,10 +1083,13 @@ def fit(self, X, y, sample_weight=None):\n         -------\n         self : object\n         \"\"\"\n-        X, y = check_X_y(X, y, ['csr', 'csc', 'coo'], dtype=np.float64,\n+        X, y = check_X_y(X, y,\n+                         accept_sparse=['csr', 'csc', 'coo'],\n+                         dtype=[np.float64, np.float32],\n                          multi_output=True, y_numeric=True)\n         if sample_weight is not None and not isinstance(sample_weight, float):\n-            sample_weight = check_array(sample_weight, ensure_2d=False)\n+            sample_weight = check_array(sample_weight, ensure_2d=False,\n+                                        dtype=X.dtype)\n         n_samples, n_features = X.shape\n \n         X, y, X_offset, y_offset, X_scale = LinearModel._preprocess_data(\n",
  "test_patch": "diff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py\n--- a/sklearn/linear_model/tests/test_ridge.py\n+++ b/sklearn/linear_model/tests/test_ridge.py\n@@ -1,3 +1,4 @@\n+import os\n import numpy as np\n import scipy.sparse as sp\n from scipy import linalg\n@@ -6,6 +7,7 @@\n import pytest\n \n from sklearn.utils.testing import assert_almost_equal\n+from sklearn.utils.testing import assert_allclose\n from sklearn.utils.testing import assert_array_almost_equal\n from sklearn.utils.testing import assert_allclose\n from sklearn.utils.testing import assert_equal\n@@ -38,7 +40,7 @@\n from sklearn.model_selection import GridSearchCV\n from sklearn.model_selection import KFold\n \n-from sklearn.utils import check_random_state\n+from sklearn.utils import check_random_state, _IS_32BIT\n from sklearn.datasets import make_multilabel_classification\n \n diabetes = datasets.load_diabetes()\n@@ -934,7 +936,9 @@ def test_ridge_classifier_no_support_multilabel():\n     assert_raises(ValueError, RidgeClassifier().fit, X, y)\n \n \n-def test_dtype_match():\n+@pytest.mark.parametrize(\n+    \"solver\", [\"svd\", \"sparse_cg\", \"cholesky\", \"lsqr\", \"sag\", \"saga\"])\n+def test_dtype_match(solver):\n     rng = np.random.RandomState(0)\n     alpha = 1.0\n \n@@ -944,25 +948,22 @@ def test_dtype_match():\n     X_32 = X_64.astype(np.float32)\n     y_32 = y_64.astype(np.float32)\n \n-    solvers = [\"svd\", \"sparse_cg\", \"cholesky\", \"lsqr\"]\n-    for solver in solvers:\n-\n-        # Check type consistency 32bits\n-        ridge_32 = Ridge(alpha=alpha, solver=solver)\n-        ridge_32.fit(X_32, y_32)\n-        coef_32 = ridge_32.coef_\n+    # Check type consistency 32bits\n+    ridge_32 = Ridge(alpha=alpha, solver=solver, max_iter=500, tol=1e-10,)\n+    ridge_32.fit(X_32, y_32)\n+    coef_32 = ridge_32.coef_\n \n-        # Check type consistency 64 bits\n-        ridge_64 = Ridge(alpha=alpha, solver=solver)\n-        ridge_64.fit(X_64, y_64)\n-        coef_64 = ridge_64.coef_\n+    # Check type consistency 64 bits\n+    ridge_64 = Ridge(alpha=alpha, solver=solver, max_iter=500, tol=1e-10,)\n+    ridge_64.fit(X_64, y_64)\n+    coef_64 = ridge_64.coef_\n \n-        # Do the actual checks at once for easier debug\n-        assert coef_32.dtype == X_32.dtype\n-        assert coef_64.dtype == X_64.dtype\n-        assert ridge_32.predict(X_32).dtype == X_32.dtype\n-        assert ridge_64.predict(X_64).dtype == X_64.dtype\n-        assert_almost_equal(ridge_32.coef_, ridge_64.coef_, decimal=5)\n+    # Do the actual checks at once for easier debug\n+    assert coef_32.dtype == X_32.dtype\n+    assert coef_64.dtype == X_64.dtype\n+    assert ridge_32.predict(X_32).dtype == X_32.dtype\n+    assert ridge_64.predict(X_64).dtype == X_64.dtype\n+    assert_allclose(ridge_32.coef_, ridge_64.coef_, rtol=1e-4)\n \n \n def test_dtype_match_cholesky():\n@@ -993,3 +994,32 @@ def test_dtype_match_cholesky():\n     assert ridge_32.predict(X_32).dtype == X_32.dtype\n     assert ridge_64.predict(X_64).dtype == X_64.dtype\n     assert_almost_equal(ridge_32.coef_, ridge_64.coef_, decimal=5)\n+\n+\n+@pytest.mark.parametrize(\n+    'solver', ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'])\n+def test_ridge_regression_dtype_stability(solver):\n+    random_state = np.random.RandomState(0)\n+    n_samples, n_features = 6, 5\n+    X = random_state.randn(n_samples, n_features)\n+    coef = random_state.randn(n_features)\n+    y = np.dot(X, coef) + 0.01 * rng.randn(n_samples)\n+    alpha = 1.0\n+    rtol = 1e-2 if os.name == 'nt' and _IS_32BIT else 1e-5\n+\n+    results = dict()\n+    for current_dtype in (np.float32, np.float64):\n+        results[current_dtype] = ridge_regression(X.astype(current_dtype),\n+                                                  y.astype(current_dtype),\n+                                                  alpha=alpha,\n+                                                  solver=solver,\n+                                                  random_state=random_state,\n+                                                  sample_weight=None,\n+                                                  max_iter=500,\n+                                                  tol=1e-10,\n+                                                  return_n_iter=False,\n+                                                  return_intercept=False)\n+\n+    assert results[np.float32].dtype == np.float32\n+    assert results[np.float64].dtype == np.float64\n+    assert_allclose(results[np.float32], results[np.float64], rtol=rtol)\n",
  "problem_statement": "[WIP] EHN: Ridge with solver SAG/SAGA does not cast to float64\ncloses #11642 \r\n\r\nbuild upon #11155 \r\n\r\nTODO:\r\n\r\n- [ ] Merge #11155 to reduce the diff.\r\n- [ ] Ensure that the casting rule is clear between base classes, classes and functions. I suspect that we have some copy which are not useful.\r\n\n",
  "hints_text": "",
  "created_at": "2019-02-27T10:28:25Z",
  "version": "0.22",
  "FAIL_TO_PASS": "[\"sklearn/linear_model/tests/test_ridge.py::test_dtype_match[sag]\", \"sklearn/linear_model/tests/test_ridge.py::test_dtype_match[saga]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_dtype_stability[sag]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_dtype_stability[saga]\"]",
  "PASS_TO_PASS": "[\"sklearn/linear_model/tests/test_ridge.py::test_ridge[svd]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge[sparse_cg]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge[cholesky]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge[lsqr]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge[sag]\", \"sklearn/linear_model/tests/test_ridge.py::test_primal_dual_relationship\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_singular\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_sample_weights\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_convergence_fail\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_sample_weights\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_shapes\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_intercept\", \"sklearn/linear_model/tests/test_ridge.py::test_toy_ridge_object\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_vs_lstsq\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_individual_penalties\", \"sklearn/linear_model/tests/test_ridge.py::test_dense_sparse[_test_ridge_loo]\", \"sklearn/linear_model/tests/test_ridge.py::test_dense_sparse[_test_ridge_cv]\", \"sklearn/linear_model/tests/test_ridge.py::test_dense_sparse[_test_ridge_cv_normalize]\", \"sklearn/linear_model/tests/test_ridge.py::test_dense_sparse[_test_ridge_diabetes]\", \"sklearn/linear_model/tests/test_ridge.py::test_dense_sparse[_test_multi_ridge_diabetes]\", \"sklearn/linear_model/tests/test_ridge.py::test_dense_sparse[_test_ridge_classifiers]\", \"sklearn/linear_model/tests/test_ridge.py::test_dense_sparse[_test_tolerance]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_cv_sparse_svd\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_sparse_svd\", \"sklearn/linear_model/tests/test_ridge.py::test_class_weights\", \"sklearn/linear_model/tests/test_ridge.py::test_class_weight_vs_sample_weight[RidgeClassifier]\", \"sklearn/linear_model/tests/test_ridge.py::test_class_weight_vs_sample_weight[RidgeClassifierCV]\", \"sklearn/linear_model/tests/test_ridge.py::test_class_weights_cv\", \"sklearn/linear_model/tests/test_ridge.py::test_ridgecv_store_cv_values\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_classifier_cv_store_cv_values\", \"sklearn/linear_model/tests/test_ridge.py::test_ridgecv_sample_weight\", \"sklearn/linear_model/tests/test_ridge.py::test_raises_value_error_if_sample_weights_greater_than_1d\", \"sklearn/linear_model/tests/test_ridge.py::test_sparse_design_with_sample_weights\", \"sklearn/linear_model/tests/test_ridge.py::test_ridgecv_int_alphas\", \"sklearn/linear_model/tests/test_ridge.py::test_ridgecv_negative_alphas\", \"sklearn/linear_model/tests/test_ridge.py::test_raises_value_error_if_solver_not_supported\", \"sklearn/linear_model/tests/test_ridge.py::test_sparse_cg_max_iter\", \"sklearn/linear_model/tests/test_ridge.py::test_n_iter\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_fit_intercept_sparse\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[auto-array-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[auto-array-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[auto-array-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[auto-array-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[auto-csr_matrix-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[auto-csr_matrix-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[auto-csr_matrix-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[auto-csr_matrix-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sparse_cg-array-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sparse_cg-array-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sparse_cg-array-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sparse_cg-array-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sparse_cg-csr_matrix-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sparse_cg-csr_matrix-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sparse_cg-csr_matrix-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sparse_cg-csr_matrix-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[cholesky-array-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[cholesky-array-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[cholesky-array-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[cholesky-array-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[cholesky-csr_matrix-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[cholesky-csr_matrix-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[cholesky-csr_matrix-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[cholesky-csr_matrix-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[lsqr-array-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[lsqr-array-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[lsqr-array-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[lsqr-array-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[lsqr-csr_matrix-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[lsqr-csr_matrix-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[lsqr-csr_matrix-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[lsqr-csr_matrix-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sag-array-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sag-array-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sag-array-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sag-array-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sag-csr_matrix-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sag-csr_matrix-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sag-csr_matrix-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[sag-csr_matrix-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[saga-array-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[saga-array-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[saga-array-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[saga-array-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[saga-csr_matrix-None-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[saga-csr_matrix-None-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[saga-csr_matrix-sample_weight1-False]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_check_arguments_validity[saga-csr_matrix-sample_weight1-True]\", \"sklearn/linear_model/tests/test_ridge.py::test_errors_and_values_helper\", \"sklearn/linear_model/tests/test_ridge.py::test_errors_and_values_svd_helper\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_classifier_no_support_multilabel\", \"sklearn/linear_model/tests/test_ridge.py::test_dtype_match[svd]\", \"sklearn/linear_model/tests/test_ridge.py::test_dtype_match[sparse_cg]\", \"sklearn/linear_model/tests/test_ridge.py::test_dtype_match[cholesky]\", \"sklearn/linear_model/tests/test_ridge.py::test_dtype_match[lsqr]\", \"sklearn/linear_model/tests/test_ridge.py::test_dtype_match_cholesky\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_dtype_stability[svd]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_dtype_stability[cholesky]\", \"sklearn/linear_model/tests/test_ridge.py::test_ridge_regression_dtype_stability[lsqr]\"]",
  "environment_setup_commit": "7e85a6d1f038bbb932b36f18d75df6be937ed00d",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.992112",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}