{
  "repo": "sympy/sympy",
  "instance_id": "sympy__sympy-24353",
  "base_commit": "900caa3f53cb160612928afe8699d2c9bcd27691",
  "patch": "diff --git a/setup.py b/setup.py\n--- a/setup.py\n+++ b/setup.py\n@@ -16,10 +16,7 @@\n \n In addition, there are some other commands:\n \n-    python setup.py clean -> will clean all trash (*.pyc and stuff)\n     python setup.py test  -> will run the complete test suite\n-    python setup.py bench -> will run the complete benchmark suite\n-    python setup.py audit -> will run pyflakes checker on source code\n \n To get a full list of available commands, read the output of:\n \n@@ -168,75 +165,6 @@\n     'sympy.vector',\n ]\n \n-class audit(Command):\n-    \"\"\"Audits SymPy's source code for following issues:\n-        - Names which are used but not defined or used before they are defined.\n-        - Names which are redefined without having been used.\n-    \"\"\"\n-\n-    description = \"Audit SymPy source with PyFlakes\"\n-    user_options = []\n-\n-    def initialize_options(self):\n-        self.all = None\n-\n-    def finalize_options(self):\n-        pass\n-\n-    def run(self):\n-        try:\n-            import pyflakes.scripts.pyflakes as flakes\n-        except ImportError:\n-            print(\"In order to run the audit, you need to have PyFlakes installed.\")\n-            sys.exit(-1)\n-        dirs = (os.path.join(*d) for d in (m.split('.') for m in modules))\n-        warns = 0\n-        for dir in dirs:\n-            for filename in os.listdir(dir):\n-                if filename.endswith('.py') and filename != '__init__.py':\n-                    warns += flakes.checkPath(os.path.join(dir, filename))\n-        if warns > 0:\n-            print(\"Audit finished with total %d warnings\" % warns)\n-\n-\n-class clean(Command):\n-    \"\"\"Cleans *.pyc and debian trashs, so you should get the same copy as\n-    is in the VCS.\n-    \"\"\"\n-\n-    description = \"remove build files\"\n-    user_options = [(\"all\", \"a\", \"the same\")]\n-\n-    def initialize_options(self):\n-        self.all = None\n-\n-    def finalize_options(self):\n-        pass\n-\n-    def run(self):\n-        curr_dir = os.getcwd()\n-        for root, dirs, files in os.walk(dir_setup):\n-            for file in files:\n-                if file.endswith('.pyc') and os.path.isfile:\n-                    os.remove(os.path.join(root, file))\n-\n-        os.chdir(dir_setup)\n-        names = [\"python-build-stamp-2.4\", \"MANIFEST\", \"build\",\n-                 \"dist\", \"doc/_build\", \"sample.tex\"]\n-\n-        for f in names:\n-            if os.path.isfile(f):\n-                os.remove(f)\n-            elif os.path.isdir(f):\n-                shutil.rmtree(f)\n-\n-        for name in glob.glob(os.path.join(dir_setup, \"doc\", \"src\", \"modules\",\n-                                           \"physics\", \"vector\", \"*.pdf\")):\n-            if os.path.isfile(name):\n-                os.remove(name)\n-\n-        os.chdir(curr_dir)\n-\n \n class test_sympy(Command):\n     \"\"\"Runs all tests under the sympy/ folder\n@@ -260,34 +188,6 @@ def run(self):\n         runtests.run_all_tests()\n \n \n-class run_benchmarks(Command):\n-    \"\"\"Runs all SymPy benchmarks\"\"\"\n-\n-    description = \"run all benchmarks\"\n-    user_options = []  # distutils complains if this is not here.\n-\n-    def __init__(self, *args):\n-        self.args = args[0]  # so we can pass it to other classes\n-        Command.__init__(self, *args)\n-\n-    def initialize_options(self):  # distutils wants this\n-        pass\n-\n-    def finalize_options(self):    # this too\n-        pass\n-\n-    # we use py.test like architecture:\n-    #\n-    # o collector   -- collects benchmarks\n-    # o runner      -- executes benchmarks\n-    # o presenter   -- displays benchmarks results\n-    #\n-    # this is done in sympy.utilities.benchmarking on top of py.test\n-    def run(self):\n-        from sympy.utilities import benchmarking\n-        benchmarking.main(['sympy'])\n-\n-\n class antlr(Command):\n     \"\"\"Generate code with antlr4\"\"\"\n     description = \"generate parser code from antlr grammars\"\n@@ -449,9 +349,6 @@ def run(self):\n               },\n           data_files=[('share/man/man1', ['doc/man/isympy.1'])],\n           cmdclass={'test': test_sympy,\n-                    'bench': run_benchmarks,\n-                    'clean': clean,\n-                    'audit': audit,\n                     'antlr': antlr,\n                     'sdist': sdist_sympy,\n                     },\ndiff --git a/sympy/utilities/benchmarking.py b/sympy/utilities/benchmarking.py\ndeleted file mode 100644\n--- a/sympy/utilities/benchmarking.py\n+++ /dev/null\n@@ -1,12 +0,0 @@\n-\"\"\"\n-.. deprecated:: 1.6\n-\n-   sympy.utilities.benchmarking has been renamed to sympy.testing.benchmarking.\n-\"\"\"\n-from sympy.utilities.exceptions import sympy_deprecation_warning\n-\n-sympy_deprecation_warning(\"The sympy.utilities.benchmarking submodule is deprecated. Use sympy.testing.benchmarking instead.\",\n-    deprecated_since_version=\"1.6\",\n-    active_deprecations_target=\"deprecated-sympy-utilities-submodules\")\n-\n-from sympy.testing.benchmarking import *  # noqa:F401\n",
  "test_patch": "diff --git a/sympy/testing/benchmarking.py b/sympy/testing/benchmarking.py\ndeleted file mode 100644\n--- a/sympy/testing/benchmarking.py\n+++ /dev/null\n@@ -1,221 +0,0 @@\n-\"\"\"benchmarking through py.test\"\"\"\n-\n-import py\n-from py.__.test.item import Item\n-from py.__.test.terminal.terminal import TerminalSession\n-\n-from math import ceil as _ceil, floor as _floor, log10\n-import timeit\n-\n-from inspect import getsource\n-\n-\n-# from IPython.Magic.magic_timeit\n-units = [\"s\", \"ms\", \"us\", \"ns\"]\n-scaling = [1, 1e3, 1e6, 1e9]\n-\n-unitn = {s: i for i, s in enumerate(units)}\n-\n-precision = 3\n-\n-\n-# like py.test Directory but scan for 'bench_<smth>.py'\n-class Directory(py.test.collect.Directory):\n-\n-    def filefilter(self, path):\n-        b = path.purebasename\n-        ext = path.ext\n-        return b.startswith('bench_') and ext == '.py'\n-\n-\n-# like py.test Module but scane for 'bench_<smth>' and 'timeit_<smth>'\n-class Module(py.test.collect.Module):\n-\n-    def funcnamefilter(self, name):\n-        return name.startswith('bench_') or name.startswith('timeit_')\n-\n-\n-# Function level benchmarking driver\n-class Timer(timeit.Timer):\n-\n-    def __init__(self, stmt, setup='pass', timer=timeit.default_timer, globals=globals()):\n-        # copy of timeit.Timer.__init__\n-        # similarity index 95%\n-        self.timer = timer\n-        stmt = timeit.reindent(stmt, 8)\n-        setup = timeit.reindent(setup, 4)\n-        src = timeit.template % {'stmt': stmt, 'setup': setup}\n-        self.src = src  # Save for traceback display\n-        code = compile(src, timeit.dummy_src_name, \"exec\")\n-        ns = {}\n-        #exec(code, globals(), ns)      -- original timeit code\n-        exec(code, globals, ns)  # -- we use caller-provided globals instead\n-        self.inner = ns[\"inner\"]\n-\n-\n-class Function(py.__.test.item.Function):\n-\n-    def __init__(self, *args, **kw):\n-        super().__init__(*args, **kw)\n-        self.benchtime = None\n-        self.benchtitle = None\n-\n-    def execute(self, target, *args):\n-        # get func source without first 'def func(...):' line\n-        src = getsource(target)\n-        src = '\\n'.join( src.splitlines()[1:] )\n-\n-        # extract benchmark title\n-        if target.func_doc is not None:\n-            self.benchtitle = target.func_doc\n-        else:\n-            self.benchtitle = src.splitlines()[0].strip()\n-\n-        # XXX we ignore args\n-        timer = Timer(src, globals=target.func_globals)\n-\n-        if self.name.startswith('timeit_'):\n-            # from IPython.Magic.magic_timeit\n-            repeat = 3\n-            number = 1\n-            for i in range(1, 10):\n-                t = timer.timeit(number)\n-\n-                if t >= 0.2:\n-                    number *= (0.2 / t)\n-                    number = int(_ceil(number))\n-                    break\n-\n-                if t <= 0.02:\n-                    # we are not close enough to that 0.2s\n-                    number *= 10\n-\n-                else:\n-                    # since we are very close to be > 0.2s we'd better adjust number\n-                    # so that timing time is not too high\n-                    number *= (0.2 / t)\n-                    number = int(_ceil(number))\n-                    break\n-\n-            self.benchtime = min(timer.repeat(repeat, number)) / number\n-\n-        # 'bench_<smth>'\n-        else:\n-            self.benchtime = timer.timeit(1)\n-\n-\n-class BenchSession(TerminalSession):\n-\n-    def header(self, colitems):\n-        super().header(colitems)\n-\n-    def footer(self, colitems):\n-        super().footer(colitems)\n-\n-        self.out.write('\\n')\n-        self.print_bench_results()\n-\n-    def print_bench_results(self):\n-        self.out.write('==============================\\n')\n-        self.out.write(' *** BENCHMARKING RESULTS *** \\n')\n-        self.out.write('==============================\\n')\n-        self.out.write('\\n')\n-\n-        # benchname, time, benchtitle\n-        results = []\n-\n-        for item, outcome in self._memo:\n-            if isinstance(item, Item):\n-\n-                best = item.benchtime\n-\n-                if best is None:\n-                    # skipped or failed benchmarks\n-                    tstr = '---'\n-\n-                else:\n-                    # from IPython.Magic.magic_timeit\n-                    if best > 0.0:\n-                        order = min(-int(_floor(log10(best)) // 3), 3)\n-                    else:\n-                        order = 3\n-\n-                    tstr = \"%.*g %s\" % (\n-                        precision, best * scaling[order], units[order])\n-\n-                results.append( [item.name, tstr, item.benchtitle] )\n-\n-        # dot/unit align second column\n-        # FIXME simpler? this is crappy -- shame on me...\n-        wm = [0]*len(units)\n-        we = [0]*len(units)\n-\n-        for s in results:\n-            tstr = s[1]\n-            n, u = tstr.split()\n-\n-            # unit n\n-            un = unitn[u]\n-\n-            try:\n-                m, e = n.split('.')\n-            except ValueError:\n-                m, e = n, ''\n-\n-            wm[un] = max(len(m), wm[un])\n-            we[un] = max(len(e), we[un])\n-\n-        for s in results:\n-            tstr = s[1]\n-            n, u = tstr.split()\n-\n-            un = unitn[u]\n-\n-            try:\n-                m, e = n.split('.')\n-            except ValueError:\n-                m, e = n, ''\n-\n-            m = m.rjust(wm[un])\n-            e = e.ljust(we[un])\n-\n-            if e.strip():\n-                n = '.'.join((m, e))\n-            else:\n-                n = ' '.join((m, e))\n-\n-            # let's put the number into the right place\n-            txt = ''\n-            for i in range(len(units)):\n-                if i == un:\n-                    txt += n\n-                else:\n-                    txt += ' '*(wm[i] + we[i] + 1)\n-\n-            s[1] = '%s %s' % (txt, u)\n-\n-        # align all columns besides the last one\n-        for i in range(2):\n-            w = max(len(s[i]) for s in results)\n-\n-            for s in results:\n-                s[i] = s[i].ljust(w)\n-\n-        # show results\n-        for s in results:\n-            self.out.write('%s  |  %s  |  %s\\n' % tuple(s))\n-\n-\n-def main(args=None):\n-    # hook our Directory/Module/Function as defaults\n-    from py.__.test import defaultconftest\n-\n-    defaultconftest.Directory = Directory\n-    defaultconftest.Module = Module\n-    defaultconftest.Function = Function\n-\n-    # hook BenchSession as py.test session\n-    config = py.test.config\n-    config._getsessionclass = lambda: BenchSession\n-\n-    py.test.cmdline.main(args)\ndiff --git a/sympy/testing/runtests.py b/sympy/testing/runtests.py\n--- a/sympy/testing/runtests.py\n+++ b/sympy/testing/runtests.py\n@@ -748,15 +748,12 @@ def _get_doctest_blacklist():\n         \"examples/advanced/autowrap_ufuncify.py\"\n         ])\n \n-    # blacklist these modules until issue 4840 is resolved\n     blacklist.extend([\n         \"sympy/conftest.py\", # Depends on pytest\n-        \"sympy/testing/benchmarking.py\",\n     ])\n \n     # These are deprecated stubs to be removed:\n     blacklist.extend([\n-        \"sympy/utilities/benchmarking.py\",\n         \"sympy/utilities/tmpfiles.py\",\n         \"sympy/utilities/pytest.py\",\n         \"sympy/utilities/runtests.py\",\ndiff --git a/sympy/testing/tests/test_code_quality.py b/sympy/testing/tests/test_code_quality.py\n--- a/sympy/testing/tests/test_code_quality.py\n+++ b/sympy/testing/tests/test_code_quality.py\n@@ -308,7 +308,6 @@ def test_this_file(fname, test_file):\n         \"%(sep)sutilities%(sep)srandtest.py\" % sepd,\n         \"%(sep)sutilities%(sep)stmpfiles.py\" % sepd,\n         \"%(sep)sutilities%(sep)squality_unicode.py\" % sepd,\n-        \"%(sep)sutilities%(sep)sbenchmarking.py\" % sepd,\n     }\n     check_files(top_level_files, test)\n     check_directory_tree(BIN_PATH, test, {\"~\", \".pyc\", \".sh\", \".mjs\"}, \"*\")\ndiff --git a/sympy/utilities/tests/test_deprecated.py b/sympy/utilities/tests/test_deprecated.py\n--- a/sympy/utilities/tests/test_deprecated.py\n+++ b/sympy/utilities/tests/test_deprecated.py\n@@ -1,4 +1,4 @@\n-from sympy.testing.pytest import warns_deprecated_sympy, XFAIL\n+from sympy.testing.pytest import warns_deprecated_sympy\n \n # See https://github.com/sympy/sympy/pull/18095\n \n@@ -11,9 +11,3 @@ def test_deprecated_utilities():\n         import sympy.utilities.randtest  # noqa:F401\n     with warns_deprecated_sympy():\n         import sympy.utilities.tmpfiles  # noqa:F401\n-\n-# This fails because benchmarking isn't importable...\n-@XFAIL\n-def test_deprecated_benchmarking():\n-    with warns_deprecated_sympy():\n-        import sympy.utilities.benchmarking  # noqa:F401\n",
  "problem_statement": "py.bench broken\n```\n$ bin/py.bench \nTraceback (most recent call last):\n  File \"bin/py.bench\", line 15, in <module>\n    from sympy.utilities import benchmarking\n  File \"sympy/utilities/benchmarking.py\", line 4, in <module>\n    from py.__.test.item import Item\nImportError: No module named __.test.item\n$ python -c \"import py; print py.__version__\"\n1.1.1\n```\n\nOriginal issue for #4840: http://code.google.com/p/sympy/issues/detail?id=1741\nOriginal author: https://code.google.com/u/Vinzent.Steinberg@gmail.com/\n\n",
  "hints_text": "```\nIt works for me, but I have 0.9.2.  So something in py must have broken it.\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1741#c1\nOriginal author: https://code.google.com/u/asmeurer@gmail.com/\n\n```\nIt's kinda written on the tin that this would break with any new release. Surely, a\nmodule named '__' is meant to be private!\n\nNB: I get the same traceback as Vinzent with version 1.0.0.\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1741#c2\nOriginal author: https://code.google.com/u/101272611947379421629/\n\n```\nWe should use the new plug-in architecture, if possible. Maybe we should talk to\nupstream about possible abuses for benchmarking.\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1741#c3\nOriginal author: https://code.google.com/u/Vinzent.Steinberg@gmail.com/\n\n```\nIs there a reasonable work-around for this? I'm facing the same problem...\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1741#c4\nOriginal author: https://code.google.com/u/christian.muise/\n\n```\nBasically I think we have to rewrite it. It should be much easier with the new plugin \ninfrastructure. Or we even use our own test runner. We just need someone to do it. The \neasiest work-around would be to use mpmath's test runner, that also prints the time \ntaken: http://code.google.com/p/mpmath/source/browse/trunk/mpmath/tests/runtests.py Much simpler than sympy's runner and thus easier to adapt.\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1741#c5\nOriginal author: https://code.google.com/u/Vinzent.Steinberg@gmail.com/\n\n```\n**Labels:** Milestone-Release0.7.0  \n\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1741#c6\nOriginal author: https://code.google.com/u/asmeurer@gmail.com/\n\n```\nIs it possible to adapt IPython's timeit to do benchmarking?\n\n**Cc:** elliso...@gmail.com  \n\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1741#c7\nOriginal author: https://code.google.com/u/asmeurer@gmail.com/\n\n```\nSee timeutils.\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1741#c8\nOriginal author: https://code.google.com/u/101069955704897915480/\n\n```\nThis is non-trivial to fix, so unless someone wants to do it soon, I am going to postpone the release milestone.\n\n**Labels:** -Milestone-Release0.7.0 Milestone-Release0.7.1  \n\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1741#c9\nOriginal author: https://code.google.com/u/asmeurer@gmail.com/\n\n```\n**Blocking:** 5641  \n\n```\n\nReferenced issues: #5641\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1741#c10\nOriginal author: https://code.google.com/u/101272611947379421629/\n\n```\nI don't think anybody uses it any more, so I think we should just consider it broken and not let that hold back other changes (e.g. issue 5641 ). \n\nAlso, I think that we should just remove all the timeit_* microbenchmarks: there's no guarantee that they're actually relevant and they're more likely than not to push us towards premature optimisation whenever we pay attention to them.\n\n**Labels:** Testing  \n\n```\n\nReferenced issues: #5641\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1741#c11\nOriginal author: https://code.google.com/u/101272611947379421629/\n\n```\nI'd like to do a small 0.7.1 release with IPython 0.11 support, so these will be postponed until 0.7.2.\n\n**Labels:** Milestone-Release0.7.2  \n\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1741#c12\nOriginal author: https://code.google.com/u/asmeurer@gmail.com/\n\n```\nWhat does PyPy use for their extensive benchmarking ( http://speed.pypy.org/)?\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1741#c13\nOriginal author: https://code.google.com/u/asmeurer@gmail.com/\n\n```\nApparently something called Codespeed: https://github.com/tobami/codespeed/wiki/ I think if we want to work on benchmarking, we need to first think up appropriate benchmarks, independent of any test runner. You said on the PyPy issue that we have various benchmarks, but they probably aren't relevant anymore. So, first order of business should be that, and then it will be easier to switch to whatever benchmarking solution we decide on.\n\nI'll look into this a bit more when I get the chance.\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1741#c14\nOriginal author: https://code.google.com/u/108713607268198052411/\n\n```\nThis is hardly a release blocker, but I'm going to raise the priority to high because we really should have some kind of benchmarking.\n\n**Labels:** -Priority-Medium -Milestone-Release0.7.2 Priority-High  \n\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1741#c15\nOriginal author: https://code.google.com/u/108713607268198052411/\n\n```\n**Status:** Valid  \n\n```\n\nOriginal comment: http://code.google.com/p/sympy/issues/detail?id=1741#c16\nOriginal author: https://code.google.com/u/asmeurer@gmail.com/\n",
  "created_at": "2022-12-07T04:35:36Z",
  "version": "1.12",
  "FAIL_TO_PASS": "[\"test_files\"]",
  "PASS_TO_PASS": "[\"test_raise_statement_regular_expression\", \"test_implicit_imports_regular_expression\", \"test_test_suite_defs\", \"test_test_duplicate_defs\", \"test_find_self_assignments\"]",
  "environment_setup_commit": "c6cb7c5602fa48034ab1bd43c2347a7e8488f12e",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:31.155919",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}