{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-14024",
  "base_commit": "4a6264db68b28a2e65efdecc459233911c9aee95",
  "patch": "diff --git a/sklearn/ensemble/_hist_gradient_boosting/grower.py b/sklearn/ensemble/_hist_gradient_boosting/grower.py\n--- a/sklearn/ensemble/_hist_gradient_boosting/grower.py\n+++ b/sklearn/ensemble/_hist_gradient_boosting/grower.py\n@@ -16,6 +16,10 @@\n from .predictor import TreePredictor\n from .utils import sum_parallel\n from .types import PREDICTOR_RECORD_DTYPE\n+from .types import Y_DTYPE\n+\n+\n+EPS = np.finfo(Y_DTYPE).eps  # to avoid zero division errors\n \n \n class TreeNode:\n@@ -398,7 +402,7 @@ def _finalize_leaf(self, node):\n         https://arxiv.org/abs/1603.02754\n         \"\"\"\n         node.value = -self.shrinkage * node.sum_gradients / (\n-            node.sum_hessians + self.splitter.l2_regularization)\n+            node.sum_hessians + self.splitter.l2_regularization + EPS)\n         self.finalized_leaves.append(node)\n \n     def _finalize_splittable_nodes(self):\ndiff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py\n--- a/sklearn/utils/estimator_checks.py\n+++ b/sklearn/utils/estimator_checks.py\n@@ -2400,8 +2400,11 @@ def check_decision_proba_consistency(name, estimator_orig):\n             hasattr(estimator, \"predict_proba\")):\n \n         estimator.fit(X, y)\n-        a = estimator.predict_proba(X_test)[:, 1]\n-        b = estimator.decision_function(X_test)\n+        # Since the link function from decision_function() to predict_proba()\n+        # is sometimes not precise enough (typically expit), we round to the\n+        # 10th decimal to avoid numerical issues.\n+        a = estimator.predict_proba(X_test)[:, 1].round(decimals=10)\n+        b = estimator.decision_function(X_test).round(decimals=10)\n         assert_array_equal(rankdata(a), rankdata(b))\n \n \n",
  "test_patch": "diff --git a/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py b/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py\n--- a/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py\n+++ b/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py\n@@ -172,3 +172,20 @@ def test_binning_train_validation_are_separated():\n                   int((1 - validation_fraction) * n_samples))\n     assert np.all(mapper_training_data.actual_n_bins_ !=\n                   mapper_whole_data.actual_n_bins_)\n+\n+\n+@pytest.mark.parametrize('data', [\n+    make_classification(random_state=0, n_classes=2),\n+    make_classification(random_state=0, n_classes=3, n_informative=3)\n+], ids=['binary_crossentropy', 'categorical_crossentropy'])\n+def test_zero_division_hessians(data):\n+    # non regression test for issue #14018\n+    # make sure we avoid zero division errors when computing the leaves values.\n+\n+    # If the learning rate is too high, the raw predictions are bad and will\n+    # saturate the softmax (or sigmoid in binary classif). This leads to\n+    # probabilities being exactly 0 or 1, gradients being constant, and\n+    # hessians being zero.\n+    X, y = data\n+    gb = HistGradientBoostingClassifier(learning_rate=100, max_iter=10)\n+    gb.fit(X, y)\n",
  "problem_statement": "Zero division error in HistGradientBoosting\n```python\r\nfrom sklearn.datasets import fetch_openml\r\nfrom sklearn.model_selection import cross_val_score\r\nfrom sklearn.experimental import enable_hist_gradient_boosting\r\nfrom sklearn.ensemble import HistGradientBoostingClassifier\r\n\r\nimport numpy as np\r\n\r\n# one hundred plants - margin\r\nbunch = fetch_openml(data_id=1491)\r\nX = bunch.data\r\ny = bunch.target\r\n\r\n\r\nres = cross_val_score(HistGradientBoostingClassifier(max_iter=100, min_samples_leaf=5), X, y)\r\nnp.mean(res)\r\n```\r\nNaN\r\n\r\nThis dataset is a bit weird in that it has 100 classes with 16 samples each. The default parameter don't work very well but we should fail more gacefully.\r\n\r\ncc @NicolasHug \n",
  "hints_text": "I am just adding the traceback\r\n\r\n\r\n```pytb\r\n---------------------------------------------------------------------------\r\nZeroDivisionError                         Traceback (most recent call last)\r\n<ipython-input-3-b0953fbb1d6e> in <module>\r\n----> 1 clf.fit(X, y)\r\n\r\n~/Documents/code/toolbox/scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py in fit(self, X, y)\r\n    247                     min_samples_leaf=self.min_samples_leaf,\r\n    248                     l2_regularization=self.l2_regularization,\r\n--> 249                     shrinkage=self.learning_rate)\r\n    250                 grower.grow()\r\n    251 \r\n\r\n~/Documents/code/toolbox/scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py in __init__(self, X_binned, gradients, hessians, max_leaf_nodes, max_depth, min_samples_leaf, min_gain_to_split, max_bins, actual_n_bins, l2_regularization, min_hessian_to_split, shrinkage)\r\n    195         self.total_compute_hist_time = 0.  # time spent computing histograms\r\n    196         self.total_apply_split_time = 0.  # time spent splitting nodes\r\n--> 197         self._intilialize_root(gradients, hessians, hessians_are_constant)\r\n    198         self.n_nodes = 1\r\n    199 \r\n\r\n~/Documents/code/toolbox/scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py in _intilialize_root(self, gradients, hessians, hessians_are_constant)\r\n    260             return\r\n    261         if sum_hessians < self.splitter.min_hessian_to_split:\r\n--> 262             self._finalize_leaf(self.root)\r\n    263             return\r\n    264 \r\n\r\n~/Documents/code/toolbox/scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py in _finalize_leaf(self, node)\r\n    399         \"\"\"\r\n    400         node.value = -self.shrinkage * node.sum_gradients / (\r\n--> 401             node.sum_hessians + self.splitter.l2_regularization)\r\n    402         self.finalized_leaves.append(node)\r\n    403 \r\n\r\nZeroDivisionError: float division by zero\r\n```\nAt a glance, the softmax is bringing probabilities close enough to zero, which causes hessians to be zero for the cross entropy loss.\nI think the right fix is just to add a small epsilon to the denominator to avoid the zero division.\r\n\r\nAs Thomas noted these cases happen when the trees are overly confident in their predictions and will always predict a probability of 1 or 0, leading to stationary gradients and zero hessians.\r\n\r\nWe hit this part of the code in `initialize_root()`:\r\n```py\r\n        if sum_hessians < self.splitter.min_hessian_to_split:\r\n            self._finalize_leaf(self.root)\r\n            return\r\n```\r\n\r\nand since hessians are zero `finalize_leaf()` fails.\r\n\r\nThat's because the learning rate is too high BTW.\r\nChanging the learning rate to .05 I get a .67 accuracy (not bad over 100 classes).\r\n\r\n\r\nWill submit a PR soon",
  "created_at": "2019-06-04T15:15:31Z",
  "version": "0.22",
  "FAIL_TO_PASS": "[\"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_zero_division_hessians[binary_crossentropy]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_zero_division_hessians[categorical_crossentropy]\"]",
  "PASS_TO_PASS": "[\"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_init_parameters_validation[params0-Loss\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_init_parameters_validation[params1-learning_rate=0\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_init_parameters_validation[params2-learning_rate=-1\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_init_parameters_validation[params3-max_iter=0\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_init_parameters_validation[params4-max_leaf_nodes=0\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_init_parameters_validation[params5-max_leaf_nodes=1\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_init_parameters_validation[params6-max_depth=0\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_init_parameters_validation[params7-max_depth=1\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_init_parameters_validation[params8-min_samples_leaf=0\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_init_parameters_validation[params9-l2_regularization=-1\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_init_parameters_validation[params10-max_bins=1\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_init_parameters_validation[params11-max_bins=257\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_init_parameters_validation[params12-n_iter_no_change=-1\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_init_parameters_validation[params13-validation_fraction=-1\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_init_parameters_validation[params14-validation_fraction=0\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_init_parameters_validation[params15-tol=-1\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_invalid_classification_loss\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_regression[neg_mean_squared_error-0.1-5-1e-07]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_regression[neg_mean_squared_error-None-5-0.1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_regression[None-0.1-5-1e-07]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_regression[None-None-5-0.1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_regression[loss-0.1-5-1e-07]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_regression[loss-None-5-0.1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_regression[None-None-None-None]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_classification[accuracy-0.1-5-1e-07-data0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_classification[accuracy-0.1-5-1e-07-data1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_classification[accuracy-None-5-0.1-data0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_classification[accuracy-None-5-0.1-data1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_classification[None-0.1-5-1e-07-data0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_classification[None-0.1-5-1e-07-data1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_classification[None-None-5-0.1-data0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_classification[None-None-5-0.1-data1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_classification[loss-0.1-5-1e-07-data0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_classification[loss-0.1-5-1e-07-data1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_classification[loss-None-5-0.1-data0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_classification[loss-None-5-0.1-data1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_classification[None-None-None-None-data0]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_early_stopping_classification[None-None-None-None-data1]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_should_stop[scores0-1-0.001-False]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_should_stop[scores1-5-0.001-False]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_should_stop[scores2-5-0.001-False]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_should_stop[scores3-5-0.001-False]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_should_stop[scores4-5-0.0-False]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_should_stop[scores5-5-0.999-False]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_should_stop[scores6-5-4.99999-False]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_should_stop[scores7-5-0.0-True]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_should_stop[scores8-5-0.001-True]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_should_stop[scores9-5-5-True]\", \"sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py::test_binning_train_validation_are_separated\"]",
  "environment_setup_commit": "7e85a6d1f038bbb932b36f18d75df6be937ed00d",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:31.001578",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}