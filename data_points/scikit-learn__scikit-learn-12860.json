{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-12860",
  "base_commit": "4223633b0d64c75fef1230f66cfb1d50fb5a8d04",
  "patch": "diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py\n--- a/sklearn/linear_model/logistic.py\n+++ b/sklearn/linear_model/logistic.py\n@@ -437,13 +437,13 @@ def _check_solver(solver, penalty, dual):\n         raise ValueError(\"Logistic Regression supports only solvers in %s, got\"\n                          \" %s.\" % (all_solvers, solver))\n \n-    all_penalties = ['l1', 'l2', 'elasticnet']\n+    all_penalties = ['l1', 'l2', 'elasticnet', 'none']\n     if penalty not in all_penalties:\n         raise ValueError(\"Logistic Regression supports only penalties in %s,\"\n                          \" got %s.\" % (all_penalties, penalty))\n \n-    if solver not in ['liblinear', 'saga'] and penalty != 'l2':\n-        raise ValueError(\"Solver %s supports only l2 penalties, \"\n+    if solver not in ['liblinear', 'saga'] and penalty not in ('l2', 'none'):\n+        raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n                          \"got %s penalty.\" % (solver, penalty))\n     if solver != 'liblinear' and dual:\n         raise ValueError(\"Solver %s supports only \"\n@@ -452,6 +452,12 @@ def _check_solver(solver, penalty, dual):\n     if penalty == 'elasticnet' and solver != 'saga':\n         raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n                          \" got solver={}.\".format(solver))\n+\n+    if solver == 'liblinear' and penalty == 'none':\n+        raise ValueError(\n+            \"penalty='none' is not supported for the liblinear solver\"\n+        )\n+\n     return solver\n \n \n@@ -1205,24 +1211,27 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,\n     'sag', 'saga' and 'newton-cg' solvers.)\n \n     This class implements regularized logistic regression using the\n-    'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. It can\n-    handle both dense and sparse input. Use C-ordered arrays or CSR matrices\n-    containing 64-bit floats for optimal performance; any other input format\n-    will be converted (and copied).\n+    'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n+    that regularization is applied by default**. It can handle both dense\n+    and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n+    floats for optimal performance; any other input format will be converted\n+    (and copied).\n \n     The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n-    with primal formulation. The 'liblinear' solver supports both L1 and L2\n-    regularization, with a dual formulation only for the L2 penalty. The\n-    Elastic-Net regularization is only supported by the 'saga' solver.\n+    with primal formulation, or no regularization. The 'liblinear' solver\n+    supports both L1 and L2 regularization, with a dual formulation only for\n+    the L2 penalty. The Elastic-Net regularization is only supported by the\n+    'saga' solver.\n \n     Read more in the :ref:`User Guide <logistic_regression>`.\n \n     Parameters\n     ----------\n-    penalty : str, 'l1', 'l2', or 'elasticnet', optional (default='l2')\n+    penalty : str, 'l1', 'l2', 'elasticnet' or 'none', optional (default='l2')\n         Used to specify the norm used in the penalization. The 'newton-cg',\n         'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n-        only supported by the 'saga' solver.\n+        only supported by the 'saga' solver. If 'none' (not supported by the\n+        liblinear solver), no regularization is applied.\n \n         .. versionadded:: 0.19\n            l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n@@ -1289,8 +1298,10 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,\n         - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n           handle multinomial loss; 'liblinear' is limited to one-versus-rest\n           schemes.\n-        - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n-          'liblinear' and 'saga' handle L1 penalty.\n+        - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n+        - 'liblinear' and 'saga' also handle L1 penalty\n+        - 'saga' also supports 'elasticnet' penalty\n+        - 'liblinear' does not handle no penalty\n \n         Note that 'sag' and 'saga' fast convergence is only guaranteed on\n         features with approximately the same scale. You can\n@@ -1491,6 +1502,18 @@ def fit(self, X, y, sample_weight=None):\n             warnings.warn(\"l1_ratio parameter is only used when penalty is \"\n                           \"'elasticnet'. Got \"\n                           \"(penalty={})\".format(self.penalty))\n+        if self.penalty == 'none':\n+            if self.C != 1.0:  # default values\n+                warnings.warn(\n+                    \"Setting penalty='none' will ignore the C and l1_ratio \"\n+                    \"parameters\"\n+                )\n+                # Note that check for l1_ratio is done right above\n+            C_ = np.inf\n+            penalty = 'l2'\n+        else:\n+            C_ = self.C\n+            penalty = self.penalty\n         if not isinstance(self.max_iter, numbers.Number) or self.max_iter < 0:\n             raise ValueError(\"Maximum number of iteration must be positive;\"\n                              \" got (max_iter=%r)\" % self.max_iter)\n@@ -1570,13 +1593,13 @@ def fit(self, X, y, sample_weight=None):\n             prefer = 'processes'\n         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n                                **_joblib_parallel_args(prefer=prefer))(\n-            path_func(X, y, pos_class=class_, Cs=[self.C],\n+            path_func(X, y, pos_class=class_, Cs=[C_],\n                       l1_ratio=self.l1_ratio, fit_intercept=self.fit_intercept,\n                       tol=self.tol, verbose=self.verbose, solver=solver,\n                       multi_class=multi_class, max_iter=self.max_iter,\n                       class_weight=self.class_weight, check_input=False,\n                       random_state=self.random_state, coef=warm_start_coef_,\n-                      penalty=self.penalty, max_squared_sum=max_squared_sum,\n+                      penalty=penalty, max_squared_sum=max_squared_sum,\n                       sample_weight=sample_weight)\n             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n \n@@ -1968,6 +1991,12 @@ def fit(self, X, y, sample_weight=None):\n \n             l1_ratios_ = [None]\n \n+        if self.penalty == 'none':\n+            raise ValueError(\n+                \"penalty='none' is not useful and not supported by \"\n+                \"LogisticRegressionCV.\"\n+            )\n+\n         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,\n                          order=\"C\",\n                          accept_large_sparse=solver != 'liblinear')\n",
  "test_patch": "diff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py\n--- a/sklearn/linear_model/tests/test_logistic.py\n+++ b/sklearn/linear_model/tests/test_logistic.py\n@@ -234,7 +234,7 @@ def test_check_solver_option(LR):\n \n     # all solvers except 'liblinear' and 'saga'\n     for solver in ['newton-cg', 'lbfgs', 'sag']:\n-        msg = (\"Solver %s supports only l2 penalties, got l1 penalty.\" %\n+        msg = (\"Solver %s supports only 'l2' or 'none' penalties,\" %\n                solver)\n         lr = LR(solver=solver, penalty='l1', multi_class='ovr')\n         assert_raise_message(ValueError, msg, lr.fit, X, y)\n@@ -253,6 +253,11 @@ def test_check_solver_option(LR):\n         lr = LR(solver=solver, penalty='elasticnet')\n         assert_raise_message(ValueError, msg, lr.fit, X, y)\n \n+    # liblinear does not support penalty='none'\n+    msg = \"penalty='none' is not supported for the liblinear solver\"\n+    lr = LR(penalty='none', solver='liblinear')\n+    assert_raise_message(ValueError, msg, lr.fit, X, y)\n+\n \n @pytest.mark.parametrize('model, params, warn_solver',\n                          [(LogisticRegression, {}, True),\n@@ -1754,3 +1759,32 @@ def test_logistic_regression_path_deprecation():\n     assert_warns_message(DeprecationWarning,\n                          \"logistic_regression_path was deprecated\",\n                          logistic_regression_path, X, Y1)\n+\n+\n+@pytest.mark.parametrize('solver', ('lbfgs', 'newton-cg', 'sag', 'saga'))\n+def test_penalty_none(solver):\n+    # - Make sure warning is raised if penalty='none' and C is set to a\n+    #   non-default value.\n+    # - Make sure setting penalty='none' is equivalent to setting C=np.inf with\n+    #   l2 penalty.\n+    X, y = make_classification(n_samples=1000, random_state=0)\n+\n+    msg = \"Setting penalty='none' will ignore the C\"\n+    lr = LogisticRegression(penalty='none', solver=solver, C=4)\n+    assert_warns_message(UserWarning, msg, lr.fit, X, y)\n+\n+    lr_none = LogisticRegression(penalty='none', solver=solver,\n+                                 random_state=0)\n+    lr_l2_C_inf = LogisticRegression(penalty='l2', C=np.inf, solver=solver,\n+                                     random_state=0)\n+    pred_none = lr_none.fit(X, y).predict(X)\n+    pred_l2_C_inf = lr_l2_C_inf.fit(X, y).predict(X)\n+    assert_array_equal(pred_none, pred_l2_C_inf)\n+\n+    lr = LogisticRegressionCV(penalty='none')\n+    assert_raise_message(\n+        ValueError,\n+        \"penalty='none' is not useful and not supported by \"\n+        \"LogisticRegressionCV\",\n+        lr.fit, X, y\n+    )\n",
  "problem_statement": "Suggestion: Add support for unpenalized logistic regression\n`LinearRegression` provides unpenalized OLS, and `SGDClassifier`, which supports `loss=\"log\"`, also supports `penalty=\"none\"`. But if you want plain old unpenalized logistic regression, you have to fake it by setting `C` in `LogisticRegression` to a large number, or use `Logit` from `statsmodels` instead.\n\n",
  "hints_text": "> you have to fake it by setting C in LogisticRegression to a large number\n\nWhat's the problem with that approach?\n\nI assumed that it's inexact and slower than a direct implementation of unpenalized logistic regression. Am I wrong?\n\nI notice that setting `C` too high, as in the following, will cause `LogisticRegression.fit` to hang. But I don't know if this is a bug or just an inherent property of the algorithm and its implementation on a 64-bit computer.\n\n``` python\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\n\nx = np.matrix([0, 0, 0, 0,  1, 1, 1, 1]).T\ny =           [1, 0, 0, 0,  1, 1, 1, 0]\n\nm = LogisticRegression(C = 1e200)\nm.fit(x, y)\nprint m.intercept_, m.coef_\n```\n\n> I notice that setting C too high, as in the following, will cause LogisticRegression.fit to hang\n\nYes this is to be expected as the problem becomes ill-posed when C is large. Iterative solvers are slow with ill-posed problems.\n\nIn your example, the algorithm takes forever to reach the desired tolerance. You either need to increase `tol` or hardcode `max_iter`.\n\n@mblondel is there an alternative to \"iterative solvers\"?\nYou won't get exactly the unregularized option, right?\n\n@Kodiologist why do you want this?\n\nYou're asking why would I want to do logistic regression without regularization? Because (1) sometimes the sample is large enough in proportion to the number of features that regularization won't buy one anything and (2) sometimes the best-fitting coefficients are of interest, as opposed to maximizing predictive accuracy.\n\nYes, that was my question.\n\n(1) is not true. It will always buy you a faster solver.\n\n(2) is more in the realms of statistical analysis, which is not really the focus of scikit-learn. I guess we could add this but I don't know what solver we would use. As a non-statistician, I wonder what good any coefficients are that change with a bit of regularization.\n\nI can't say much about (1) since computation isn't my forte. For (2), I am a data analyst with a background in statistics. I know that scikit-learn focuses on traditional machine learning, but it is in my opinion the best Python package for data analysis right now, and I think it will benefit from not limiting itself _too_ much. (I also think, following Larry Wasserman and Andrew Gelman, that statistics and machine learning would mutually benefit from intermingling more, but I guess that's its own can of worms.) All coefficients will change with regularization; that's what regularization does.\n\nI'm not opposed to adding a solver without regularization. We can check what would be good, or just bail and use l-bfgs and check before-hand if it's ill-conditioned?\n\nYes, all coefficients change with regularization. I'm just honestly curious what you want to do with them afterwards.\n\nHey,\r\nWhat is the status on this topic? I'd be really interested in an unpenalized Logistic Regression. This way p-values will mean something statistically speaking. Otherwise I will have to continue using R 😢 for such use cases...\r\nThanks,\r\nAlex\nOr statsmodels?\n\nWhat solvers do you suggest to implement? How would that be different from the solvers we already have with C -> infty ?\n> What solvers do you suggest to implement? How would that be different from the solvers we already have with C -> infty ?\r\n\r\nYou could try looking at R or statsmodels for ideas. I'm not familiar with their methods, but they're reasonably fast and use no regularization at all.\nYeah statsmodels does the job too if you use the QR algorithm for matrix inversion. My use case is around model interpretability. For performance, I would definitely use regularization. \nI don't think we need to add any new solver... Logistic regression doesn't enjoy a closed form solution, which means that statsmodel must use an iterative solver of some kind too (my guess would be iterative reweighted least squares, but I haven't checked). Setting `C=np.inf` (or equivalently [alpha=0](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/logistic.py#L748)) should in principle work with our current solvers. My recommendation would be to switch to the L-BFGS or Newton-CG solver, since liblinear can indeed be very slow in this setting. Perhaps we can add a `solver=\"auto\"` option and automatically switch to one of these when  `C=np.inf` or equivalently `penalty=\"none\"`?\nwe're changing the default solver to lbfgs in #10001 fwiw\n\nFor the folks that really want unregularised logistic regression (like myself). I've been having to settle with using statsmodels and making a wrapper class that mimics SKLearn API. \nAny updates on this?  This is a big blocker for my willingness to recommend scikit-learn to people.  It's also [not at all obvious](https://www.reddit.com/r/datascience/comments/8kne2r/different_coefficients_scikitlearn_vs_statsmodels/) to people coming from other libraries that scikit-learn does regularization by default and that there's no way to disable it.\n@shermstats suggestions how to improve the documentation on that? I agree that it might not be very obvious.\r\nDoes l-bfgs allow ``C=np.inf``?\nYou can specify ``C=np.inf``, though it'll give you the same result as ``C=large value``. On the example I tried, it gave a better fit than statsmodel and statsmodel failed to converge with most other random seeds:\r\n\r\n```python\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.linear_model import LogisticRegression\r\nimport statsmodels.api as sm\r\n\r\nX, y = make_classification(random_state=2)\r\nlr = LogisticRegression(C=np.inf, solver='lbfgs').fit(X, y)\r\n\r\n\r\nlogit = sm.Logit(y, X)\r\nres = logit.fit()\r\n```\r\n\r\n```\r\nOptimization terminated successfully.\r\n         Current function value: 0.167162\r\n         Iterations 10\r\n```\r\n\r\n```python\r\nfrom sklearn.metrics import log_loss\r\nlog_loss(y, lr.predict_proba(X))\r\nlog_loss(y, res.predict(X))\r\n```\r\n```\r\n0.16197793224715606\r\n0.16716164149746823\r\n```\r\n\r\n\r\nSo I would argue we should just document that you can get an unpenalized model by setting C large or to np.inf.\nI'd suggest adding to the docstring and the user guide\r\n\"The LogisticRegregression model is penalized by default. You can obtain an unpenalized model by setting C=np.inf and solver='lbfgs'.\"\n> it gave a better fit than statsmodel and statsmodel failed to converge with most other random seeds\r\n\r\nR's `glm` is more mature and may make for a better comparison.\r\n\r\n> I'd suggest adding to the docstring and the user guide\r\n\"The LogisticRegregression model is penalized by default. You can obtain an unpenalized model by setting C=np.inf and solver='lbfgs'.\"\r\n\r\nWhy not add allow `penalty = \"none\"` a la `SGDClassifier`?\n@Kodiologist I'm not opposed to adding ``penalty=\"none\"`` but I'm not sure what the benefit is to adding a redundant option.\r\nAnd I think we'd welcome comparisons to glm. I'm not very familiar with glm so I'm probably not a good person to perform the comparison. However, we are optimizing the log-loss so there should really be no difference. Maybe they implement different solvers so having a benchmark would be nice.\n> I'm not opposed to adding `penalty=\"none\"` but I'm not sure what the benefit is to adding a redundant option.\r\n\r\n1. It becomes clearer how to get an unpenalized model.\r\n2. It becomes clearer to the reader what code that's using an unpenalized model is trying to do.\r\n3. It allows sklearn to change its implementation of unregularized models in the future without breaking people's code.\nIf you feel it adds to discoverability then we can add it, and 3 is a valid point (though we can actually not really change that without deprecations probably, see current change of the solver).\r\nDo you want to send a PR?\nI don't have the round tuits for it; sorry.\n@Kodiologist at least you taught me an idiom I didn't know about ;)\nSo open for contributors: add ``penalty='none'`` as an option. Also possibly check what solvers support this / are efficient with this (liblinear is probably not) and restrict to those solvers.\n> I'd suggest adding to the docstring and the user guide\r\n> \"The LogisticRegregression model is penalized by default. You can obtain an unpenalized model by setting C=np.inf and solver='lbfgs'.\"\r\n\r\nThis sounds reasonable to me.  I'd also suggest bolding the first sentence because it's legitimately that surprising for people coming from other machine learning or data analysis environments.\n@shermstats So @Kodiologist suggested adding ``penalty=\"none\"`` to make it more explicit, which would just be an alias for ``C=np.inf``. It makes sense for me to make this more explicit in this way. Do you have thoughts on that?\r\nThen that would be what's in the documentation. And I agree that bold might be a good idea.\r\nI think for someone with a ML background this is (maybe?) expected, for someone with a stats background, this is seems very surprising.\nExactly!  I have a stats background and have worked with many statistics people coming from R or even point and click interfaces, and this behavior is very surprising to us.  I think for now that `penalty=None` (not sure about `\"none\"` vs. `None`) is a good solution.  In the future, we should have a separate solver that's called automatically for unpenalized logistic regression to prevent the issues that @mblondel described.\nSorry, which issue do you mean? We're switching to l-bfgs by default, and we can also  internally switch the solver to l-bfgs automatically if someone specifies ``penalty='none'`` (often None is a special token we use for deprecated parameters, but we have stopped doing that. Still 'none' would be more consistent with the rest of the library).\r\nWe need ``solver=\"auto\"`` anyway so changing the solver based on the penalty shouldn't be an issue.\n[This issue](https://github.com/scikit-learn/scikit-learn/issues/6738#issuecomment-216245049), which refers to the iterative algorithm becoming very slow for large C.  I'm not a numerical analysis expert, but if l-bfgs prevents it from slowing down then that sounds like the right solution.  `penalty='none'` also sounds like the right way to handle this.\n@shermstats yes, with l-bfgs this doesn't seem to be an issue. I haven't run extensive benchmarks, though, and won't have time to. If anyone wants to run benchmarks, that would be a great help.",
  "created_at": "2018-12-24T20:07:42Z",
  "version": "0.21",
  "FAIL_TO_PASS": "[\"sklearn/linear_model/tests/test_logistic.py::test_check_solver_option[LogisticRegression]\", \"sklearn/linear_model/tests/test_logistic.py::test_check_solver_option[LogisticRegressionCV]\", \"sklearn/linear_model/tests/test_logistic.py::test_penalty_none[lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_penalty_none[newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_penalty_none[sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_penalty_none[saga]\"]",
  "PASS_TO_PASS": "[\"sklearn/linear_model/tests/test_logistic.py::test_predict_2_classes\", \"sklearn/linear_model/tests/test_logistic.py::test_error\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv_mock_scorer\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv_score_does_not_warn_by_default\", \"sklearn/linear_model/tests/test_logistic.py::test_lr_liblinear_warning\", \"sklearn/linear_model/tests/test_logistic.py::test_predict_3_classes\", \"sklearn/linear_model/tests/test_logistic.py::test_predict_iris\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_validation[lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_validation[newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_validation[sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_validation[saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_warnings[LogisticRegression-params0-True]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_warnings[LogisticRegressionCV-params1-False]\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_binary[lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_binary[newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_binary[sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_binary[saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_binary_probabilities\", \"sklearn/linear_model/tests/test_logistic.py::test_sparsify\", \"sklearn/linear_model/tests/test_logistic.py::test_inconsistent_input\", \"sklearn/linear_model/tests/test_logistic.py::test_write_parameters\", \"sklearn/linear_model/tests/test_logistic.py::test_nan\", \"sklearn/linear_model/tests/test_logistic.py::test_consistency_path\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_path_convergence_fail\", \"sklearn/linear_model/tests/test_logistic.py::test_liblinear_dual_random_state\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_loss_and_grad\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_grad_hess\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv_multinomial_score[accuracy-multiclass_agg_list0]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv_multinomial_score[precision-multiclass_agg_list1]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv_multinomial_score[f1-multiclass_agg_list2]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv_multinomial_score[neg_log_loss-multiclass_agg_list3]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv_multinomial_score[recall-multiclass_agg_list4]\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_logistic_regression_string_inputs\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_cv_sparse\", \"sklearn/linear_model/tests/test_logistic.py::test_intercept_logistic_helper\", \"sklearn/linear_model/tests/test_logistic.py::test_ovr_multinomial_iris\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_solvers\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_solvers_multiclass\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regressioncv_class_weights\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_sample_weights\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_class_weights\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multinomial\", \"sklearn/linear_model/tests/test_logistic.py::test_multinomial_grad_hess\", \"sklearn/linear_model/tests/test_logistic.py::test_liblinear_decision_function_zero\", \"sklearn/linear_model/tests/test_logistic.py::test_liblinear_logregcv_sparse\", \"sklearn/linear_model/tests/test_logistic.py::test_saga_sparse\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_intercept_scaling\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_intercept_scaling_zero\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_l1\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_l1_sparse_data\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_cv_refit[l1-42]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_cv_refit[l2-42]\", \"sklearn/linear_model/tests/test_logistic.py::test_logreg_predict_proba_multinomial\", \"sklearn/linear_model/tests/test_logistic.py::test_max_iter\", \"sklearn/linear_model/tests/test_logistic.py::test_n_iter[newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_n_iter[liblinear]\", \"sklearn/linear_model/tests/test_logistic.py::test_n_iter[sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_n_iter[saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_n_iter[lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-True-True-newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-True-True-sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-True-True-saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-True-True-lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-True-False-newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-True-False-sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-True-False-saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-True-False-lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-False-True-newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-False-True-sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-False-True-saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-False-True-lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-False-False-newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-False-False-sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-False-False-saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[ovr-False-False-lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-True-True-newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-True-True-sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-True-True-saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-True-True-lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-True-False-newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-True-False-sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-True-False-saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-True-False-lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-False-True-newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-False-True-sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-False-True-saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-False-True-lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-False-False-newton-cg]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-False-False-sag]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-False-False-saga]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start[multinomial-False-False-lbfgs]\", \"sklearn/linear_model/tests/test_logistic.py::test_saga_vs_liblinear\", \"sklearn/linear_model/tests/test_logistic.py::test_dtype_match[ovr]\", \"sklearn/linear_model/tests/test_logistic.py::test_dtype_match[multinomial]\", \"sklearn/linear_model/tests/test_logistic.py::test_warm_start_converge_LR\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_coeffs\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_l1_l2_equivalence[l1-1-0.001]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_l1_l2_equivalence[l1-1-0.1]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_l1_l2_equivalence[l1-1-1]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_l1_l2_equivalence[l1-1-10]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_l1_l2_equivalence[l1-1-100]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_l1_l2_equivalence[l1-1-1000]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_l1_l2_equivalence[l1-1-1000000.0]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_l1_l2_equivalence[l2-0-0.001]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_l1_l2_equivalence[l2-0-0.1]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_l1_l2_equivalence[l2-0-1]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_l1_l2_equivalence[l2-0-10]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_l1_l2_equivalence[l2-0-100]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_l1_l2_equivalence[l2-0-1000]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_l1_l2_equivalence[l2-0-1000000.0]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_vs_l1_l2[0.001]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_vs_l1_l2[1]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_vs_l1_l2[100]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_vs_l1_l2[1000000.0]\", \"sklearn/linear_model/tests/test_logistic.py::test_LogisticRegression_elastic_net_objective[0.1-0.001]\", \"sklearn/linear_model/tests/test_logistic.py::test_LogisticRegression_elastic_net_objective[0.1-0.046415888336127795]\", \"sklearn/linear_model/tests/test_logistic.py::test_LogisticRegression_elastic_net_objective[0.1-2.1544346900318843]\", \"sklearn/linear_model/tests/test_logistic.py::test_LogisticRegression_elastic_net_objective[0.1-100.0]\", \"sklearn/linear_model/tests/test_logistic.py::test_LogisticRegression_elastic_net_objective[0.5-0.001]\", \"sklearn/linear_model/tests/test_logistic.py::test_LogisticRegression_elastic_net_objective[0.5-0.046415888336127795]\", \"sklearn/linear_model/tests/test_logistic.py::test_LogisticRegression_elastic_net_objective[0.5-2.1544346900318843]\", \"sklearn/linear_model/tests/test_logistic.py::test_LogisticRegression_elastic_net_objective[0.5-100.0]\", \"sklearn/linear_model/tests/test_logistic.py::test_LogisticRegression_elastic_net_objective[0.9-0.001]\", \"sklearn/linear_model/tests/test_logistic.py::test_LogisticRegression_elastic_net_objective[0.9-0.046415888336127795]\", \"sklearn/linear_model/tests/test_logistic.py::test_LogisticRegression_elastic_net_objective[0.9-2.1544346900318843]\", \"sklearn/linear_model/tests/test_logistic.py::test_LogisticRegression_elastic_net_objective[0.9-100.0]\", \"sklearn/linear_model/tests/test_logistic.py::test_LogisticRegressionCV_GridSearchCV_elastic_net[ovr]\", \"sklearn/linear_model/tests/test_logistic.py::test_LogisticRegressionCV_GridSearchCV_elastic_net[multinomial]\", \"sklearn/linear_model/tests/test_logistic.py::test_LogisticRegressionCV_GridSearchCV_elastic_net_ovr\", \"sklearn/linear_model/tests/test_logistic.py::test_LogisticRegressionCV_no_refit[ovr]\", \"sklearn/linear_model/tests/test_logistic.py::test_LogisticRegressionCV_no_refit[multinomial]\", \"sklearn/linear_model/tests/test_logistic.py::test_LogisticRegressionCV_elasticnet_attribute_shapes\", \"sklearn/linear_model/tests/test_logistic.py::test_l1_ratio_param[-1]\", \"sklearn/linear_model/tests/test_logistic.py::test_l1_ratio_param[2]\", \"sklearn/linear_model/tests/test_logistic.py::test_l1_ratio_param[None]\", \"sklearn/linear_model/tests/test_logistic.py::test_l1_ratio_param[something_wrong]\", \"sklearn/linear_model/tests/test_logistic.py::test_l1_ratios_param[l1_ratios0]\", \"sklearn/linear_model/tests/test_logistic.py::test_l1_ratios_param[l1_ratios1]\", \"sklearn/linear_model/tests/test_logistic.py::test_l1_ratios_param[None]\", \"sklearn/linear_model/tests/test_logistic.py::test_l1_ratios_param[something_wrong]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_versus_sgd[0.1-0.001]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_versus_sgd[0.1-0.046415888336127795]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_versus_sgd[0.1-2.1544346900318843]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_versus_sgd[0.1-100.0]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_versus_sgd[0.5-0.001]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_versus_sgd[0.5-0.046415888336127795]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_versus_sgd[0.5-2.1544346900318843]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_versus_sgd[0.5-100.0]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_versus_sgd[0.9-0.001]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_versus_sgd[0.9-0.046415888336127795]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_versus_sgd[0.9-2.1544346900318843]\", \"sklearn/linear_model/tests/test_logistic.py::test_elastic_net_versus_sgd[0.9-100.0]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_path_coefs_multinomial\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[liblinear-est0]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[liblinear-est1]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[lbfgs-est0]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[lbfgs-est1]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[newton-cg-est0]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[newton-cg-est1]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[sag-est0]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[sag-est1]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[saga-est0]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_multi_class_auto[saga-est1]\", \"sklearn/linear_model/tests/test_logistic.py::test_logistic_regression_path_deprecation\"]",
  "environment_setup_commit": "7813f7efb5b2012412888b69e73d76f2df2b50b6",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.968859",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}