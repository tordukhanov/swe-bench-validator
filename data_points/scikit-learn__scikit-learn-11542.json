{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-11542",
  "base_commit": "cd7d9d985e1bbe2dbbbae17da0e9fbbba7e8c8c6",
  "patch": "diff --git a/examples/applications/plot_prediction_latency.py b/examples/applications/plot_prediction_latency.py\n--- a/examples/applications/plot_prediction_latency.py\n+++ b/examples/applications/plot_prediction_latency.py\n@@ -285,7 +285,7 @@ def plot_benchmark_throughput(throughputs, configuration):\n          'complexity_label': 'non-zero coefficients',\n          'complexity_computer': lambda clf: np.count_nonzero(clf.coef_)},\n         {'name': 'RandomForest',\n-         'instance': RandomForestRegressor(),\n+         'instance': RandomForestRegressor(n_estimators=100),\n          'complexity_label': 'estimators',\n          'complexity_computer': lambda clf: clf.n_estimators},\n         {'name': 'SVR',\ndiff --git a/examples/ensemble/plot_ensemble_oob.py b/examples/ensemble/plot_ensemble_oob.py\n--- a/examples/ensemble/plot_ensemble_oob.py\n+++ b/examples/ensemble/plot_ensemble_oob.py\n@@ -45,15 +45,18 @@\n # error trajectory during training.\n ensemble_clfs = [\n     (\"RandomForestClassifier, max_features='sqrt'\",\n-        RandomForestClassifier(warm_start=True, oob_score=True,\n+        RandomForestClassifier(n_estimators=100,\n+                               warm_start=True, oob_score=True,\n                                max_features=\"sqrt\",\n                                random_state=RANDOM_STATE)),\n     (\"RandomForestClassifier, max_features='log2'\",\n-        RandomForestClassifier(warm_start=True, max_features='log2',\n+        RandomForestClassifier(n_estimators=100,\n+                               warm_start=True, max_features='log2',\n                                oob_score=True,\n                                random_state=RANDOM_STATE)),\n     (\"RandomForestClassifier, max_features=None\",\n-        RandomForestClassifier(warm_start=True, max_features=None,\n+        RandomForestClassifier(n_estimators=100,\n+                               warm_start=True, max_features=None,\n                                oob_score=True,\n                                random_state=RANDOM_STATE))\n ]\ndiff --git a/examples/ensemble/plot_random_forest_regression_multioutput.py b/examples/ensemble/plot_random_forest_regression_multioutput.py\n--- a/examples/ensemble/plot_random_forest_regression_multioutput.py\n+++ b/examples/ensemble/plot_random_forest_regression_multioutput.py\n@@ -44,11 +44,13 @@\n                                                     random_state=4)\n \n max_depth = 30\n-regr_multirf = MultiOutputRegressor(RandomForestRegressor(max_depth=max_depth,\n+regr_multirf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100,\n+                                                          max_depth=max_depth,\n                                                           random_state=0))\n regr_multirf.fit(X_train, y_train)\n \n-regr_rf = RandomForestRegressor(max_depth=max_depth, random_state=2)\n+regr_rf = RandomForestRegressor(n_estimators=100, max_depth=max_depth,\n+                                random_state=2)\n regr_rf.fit(X_train, y_train)\n \n # Predict on new data\ndiff --git a/examples/ensemble/plot_voting_probas.py b/examples/ensemble/plot_voting_probas.py\n--- a/examples/ensemble/plot_voting_probas.py\n+++ b/examples/ensemble/plot_voting_probas.py\n@@ -30,7 +30,7 @@\n from sklearn.ensemble import VotingClassifier\n \n clf1 = LogisticRegression(random_state=123)\n-clf2 = RandomForestClassifier(random_state=123)\n+clf2 = RandomForestClassifier(n_estimators=100, random_state=123)\n clf3 = GaussianNB()\n X = np.array([[-1.0, -1.0], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n y = np.array([1, 1, 2, 2])\ndiff --git a/sklearn/ensemble/forest.py b/sklearn/ensemble/forest.py\n--- a/sklearn/ensemble/forest.py\n+++ b/sklearn/ensemble/forest.py\n@@ -135,7 +135,7 @@ class BaseForest(six.with_metaclass(ABCMeta, BaseEnsemble)):\n     @abstractmethod\n     def __init__(self,\n                  base_estimator,\n-                 n_estimators=10,\n+                 n_estimators=100,\n                  estimator_params=tuple(),\n                  bootstrap=False,\n                  oob_score=False,\n@@ -242,6 +242,12 @@ def fit(self, X, y, sample_weight=None):\n         -------\n         self : object\n         \"\"\"\n+\n+        if self.n_estimators == 'warn':\n+            warnings.warn(\"The default value of n_estimators will change from \"\n+                          \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n+            self.n_estimators = 10\n+\n         # Validate or convert input data\n         X = check_array(X, accept_sparse=\"csc\", dtype=DTYPE)\n         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)\n@@ -399,7 +405,7 @@ class ForestClassifier(six.with_metaclass(ABCMeta, BaseForest,\n     @abstractmethod\n     def __init__(self,\n                  base_estimator,\n-                 n_estimators=10,\n+                 n_estimators=100,\n                  estimator_params=tuple(),\n                  bootstrap=False,\n                  oob_score=False,\n@@ -408,7 +414,6 @@ def __init__(self,\n                  verbose=0,\n                  warm_start=False,\n                  class_weight=None):\n-\n         super(ForestClassifier, self).__init__(\n             base_estimator,\n             n_estimators=n_estimators,\n@@ -638,7 +643,7 @@ class ForestRegressor(six.with_metaclass(ABCMeta, BaseForest, RegressorMixin)):\n     @abstractmethod\n     def __init__(self,\n                  base_estimator,\n-                 n_estimators=10,\n+                 n_estimators=100,\n                  estimator_params=tuple(),\n                  bootstrap=False,\n                  oob_score=False,\n@@ -758,6 +763,10 @@ class RandomForestClassifier(ForestClassifier):\n     n_estimators : integer, optional (default=10)\n         The number of trees in the forest.\n \n+        .. versionchanged:: 0.20\n+           The default value of ``n_estimators`` will change from 10 in\n+           version 0.20 to 100 in version 0.22.\n+\n     criterion : string, optional (default=\"gini\")\n         The function to measure the quality of a split. Supported criteria are\n         \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n@@ -971,7 +980,7 @@ class labels (multi-output problem).\n     DecisionTreeClassifier, ExtraTreesClassifier\n     \"\"\"\n     def __init__(self,\n-                 n_estimators=10,\n+                 n_estimators='warn',\n                  criterion=\"gini\",\n                  max_depth=None,\n                  min_samples_split=2,\n@@ -1032,6 +1041,10 @@ class RandomForestRegressor(ForestRegressor):\n     n_estimators : integer, optional (default=10)\n         The number of trees in the forest.\n \n+        .. versionchanged:: 0.20\n+           The default value of ``n_estimators`` will change from 10 in\n+           version 0.20 to 100 in version 0.22.\n+\n     criterion : string, optional (default=\"mse\")\n         The function to measure the quality of a split. Supported criteria\n         are \"mse\" for the mean squared error, which is equal to variance\n@@ -1211,7 +1224,7 @@ class RandomForestRegressor(ForestRegressor):\n     DecisionTreeRegressor, ExtraTreesRegressor\n     \"\"\"\n     def __init__(self,\n-                 n_estimators=10,\n+                 n_estimators='warn',\n                  criterion=\"mse\",\n                  max_depth=None,\n                  min_samples_split=2,\n@@ -1268,6 +1281,10 @@ class ExtraTreesClassifier(ForestClassifier):\n     n_estimators : integer, optional (default=10)\n         The number of trees in the forest.\n \n+        .. versionchanged:: 0.20\n+           The default value of ``n_estimators`` will change from 10 in\n+           version 0.20 to 100 in version 0.22.\n+\n     criterion : string, optional (default=\"gini\")\n         The function to measure the quality of a split. Supported criteria are\n         \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n@@ -1454,7 +1471,7 @@ class labels (multi-output problem).\n         splits.\n     \"\"\"\n     def __init__(self,\n-                 n_estimators=10,\n+                 n_estimators='warn',\n                  criterion=\"gini\",\n                  max_depth=None,\n                  min_samples_split=2,\n@@ -1513,6 +1530,10 @@ class ExtraTreesRegressor(ForestRegressor):\n     n_estimators : integer, optional (default=10)\n         The number of trees in the forest.\n \n+        .. versionchanged:: 0.20\n+           The default value of ``n_estimators`` will change from 10 in\n+           version 0.20 to 100 in version 0.22.\n+\n     criterion : string, optional (default=\"mse\")\n         The function to measure the quality of a split. Supported criteria\n         are \"mse\" for the mean squared error, which is equal to variance\n@@ -1666,7 +1687,7 @@ class ExtraTreesRegressor(ForestRegressor):\n     RandomForestRegressor: Ensemble regressor using trees with optimal splits.\n     \"\"\"\n     def __init__(self,\n-                 n_estimators=10,\n+                 n_estimators='warn',\n                  criterion=\"mse\",\n                  max_depth=None,\n                  min_samples_split=2,\n@@ -1728,6 +1749,10 @@ class RandomTreesEmbedding(BaseForest):\n     n_estimators : integer, optional (default=10)\n         Number of trees in the forest.\n \n+        .. versionchanged:: 0.20\n+           The default value of ``n_estimators`` will change from 10 in\n+           version 0.20 to 100 in version 0.22.\n+\n     max_depth : integer, optional (default=5)\n         The maximum depth of each tree. If None, then nodes are expanded until\n         all leaves are pure or until all leaves contain less than\n@@ -1833,7 +1858,7 @@ class RandomTreesEmbedding(BaseForest):\n     \"\"\"\n \n     def __init__(self,\n-                 n_estimators=10,\n+                 n_estimators='warn',\n                  max_depth=5,\n                  min_samples_split=2,\n                  min_samples_leaf=1,\ndiff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py\n--- a/sklearn/utils/estimator_checks.py\n+++ b/sklearn/utils/estimator_checks.py\n@@ -340,7 +340,13 @@ def set_checking_parameters(estimator):\n         estimator.set_params(n_resampling=5)\n     if \"n_estimators\" in params:\n         # especially gradient boosting with default 100\n-        estimator.set_params(n_estimators=min(5, estimator.n_estimators))\n+        # FIXME: The default number of trees was changed and is set to 'warn'\n+        # for some of the ensemble methods. We need to catch this case to avoid\n+        # an error during the comparison. To be reverted in 0.22.\n+        if estimator.n_estimators == 'warn':\n+            estimator.set_params(n_estimators=5)\n+        else:\n+            estimator.set_params(n_estimators=min(5, estimator.n_estimators))\n     if \"max_trials\" in params:\n         # RANSAC\n         estimator.set_params(max_trials=10)\n",
  "test_patch": "diff --git a/sklearn/ensemble/tests/test_forest.py b/sklearn/ensemble/tests/test_forest.py\n--- a/sklearn/ensemble/tests/test_forest.py\n+++ b/sklearn/ensemble/tests/test_forest.py\n@@ -31,6 +31,7 @@\n from sklearn.utils.testing import assert_raises\n from sklearn.utils.testing import assert_warns\n from sklearn.utils.testing import assert_warns_message\n+from sklearn.utils.testing import assert_no_warnings\n from sklearn.utils.testing import ignore_warnings\n \n from sklearn import datasets\n@@ -186,6 +187,7 @@ def check_regressor_attributes(name):\n     assert_false(hasattr(r, \"n_classes_\"))\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n @pytest.mark.parametrize('name', FOREST_REGRESSORS)\n def test_regressor_attributes(name):\n     check_regressor_attributes(name)\n@@ -432,6 +434,7 @@ def check_oob_score_raise_error(name):\n                                                   bootstrap=False).fit, X, y)\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n @pytest.mark.parametrize('name', FOREST_ESTIMATORS)\n def test_oob_score_raise_error(name):\n     check_oob_score_raise_error(name)\n@@ -489,6 +492,7 @@ def check_pickle(name, X, y):\n     assert_equal(score, score2)\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n @pytest.mark.parametrize('name', FOREST_CLASSIFIERS_REGRESSORS)\n def test_pickle(name):\n     if name in FOREST_CLASSIFIERS:\n@@ -526,6 +530,7 @@ def check_multioutput(name):\n             assert_equal(log_proba[1].shape, (4, 4))\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n @pytest.mark.parametrize('name', FOREST_CLASSIFIERS_REGRESSORS)\n def test_multioutput(name):\n     check_multioutput(name)\n@@ -549,6 +554,7 @@ def check_classes_shape(name):\n     assert_array_equal(clf.classes_, [[-1, 1], [-2, 2]])\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n @pytest.mark.parametrize('name', FOREST_CLASSIFIERS)\n def test_classes_shape(name):\n     check_classes_shape(name)\n@@ -738,6 +744,7 @@ def check_min_samples_split(name):\n                    \"Failed with {0}\".format(name))\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n @pytest.mark.parametrize('name', FOREST_ESTIMATORS)\n def test_min_samples_split(name):\n     check_min_samples_split(name)\n@@ -775,6 +782,7 @@ def check_min_samples_leaf(name):\n                    \"Failed with {0}\".format(name))\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n @pytest.mark.parametrize('name', FOREST_ESTIMATORS)\n def test_min_samples_leaf(name):\n     check_min_samples_leaf(name)\n@@ -842,6 +850,7 @@ def check_sparse_input(name, X, X_sparse, y):\n                                   dense.fit_transform(X).toarray())\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n @pytest.mark.parametrize('name', FOREST_ESTIMATORS)\n @pytest.mark.parametrize('sparse_matrix',\n                          (csr_matrix, csc_matrix, coo_matrix))\n@@ -899,6 +908,7 @@ def check_memory_layout(name, dtype):\n     assert_array_almost_equal(est.fit(X, y).predict(X), y)\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n @pytest.mark.parametrize('name', FOREST_CLASSIFIERS_REGRESSORS)\n @pytest.mark.parametrize('dtype', (np.float64, np.float32))\n def test_memory_layout(name, dtype):\n@@ -977,6 +987,7 @@ def check_class_weights(name):\n     clf.fit(iris.data, iris.target, sample_weight=sample_weight)\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n @pytest.mark.parametrize('name', FOREST_CLASSIFIERS)\n def test_class_weights(name):\n     check_class_weights(name)\n@@ -996,6 +1007,7 @@ def check_class_weight_balanced_and_bootstrap_multi_output(name):\n     clf.fit(X, _y)\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n @pytest.mark.parametrize('name', FOREST_CLASSIFIERS)\n def test_class_weight_balanced_and_bootstrap_multi_output(name):\n     check_class_weight_balanced_and_bootstrap_multi_output(name)\n@@ -1026,6 +1038,7 @@ def check_class_weight_errors(name):\n     assert_raises(ValueError, clf.fit, X, _y)\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n @pytest.mark.parametrize('name', FOREST_CLASSIFIERS)\n def test_class_weight_errors(name):\n     check_class_weight_errors(name)\n@@ -1163,6 +1176,7 @@ def test_warm_start_oob(name):\n     check_warm_start_oob(name)\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_dtype_convert(n_classes=15):\n     classifier = RandomForestClassifier(random_state=0, bootstrap=False)\n \n@@ -1201,6 +1215,7 @@ def test_decision_path(name):\n     check_decision_path(name)\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_min_impurity_split():\n     # Test if min_impurity_split of base estimators is set\n     # Regression test for #8006\n@@ -1216,6 +1231,7 @@ def test_min_impurity_split():\n             assert_equal(tree.min_impurity_split, 0.1)\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_min_impurity_decrease():\n     X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)\n     all_estimators = [RandomForestClassifier, RandomForestRegressor,\n@@ -1228,3 +1244,21 @@ def test_min_impurity_decrease():\n             # Simply check if the parameter is passed on correctly. Tree tests\n             # will suffice for the actual working of this param\n             assert_equal(tree.min_impurity_decrease, 0.1)\n+\n+\n+@pytest.mark.parametrize('forest',\n+                         [RandomForestClassifier, RandomForestRegressor,\n+                          ExtraTreesClassifier, ExtraTreesRegressor,\n+                          RandomTreesEmbedding])\n+def test_nestimators_future_warning(forest):\n+    # FIXME: to be removed 0.22\n+\n+    # When n_estimators default value is used\n+    msg_future = (\"The default value of n_estimators will change from \"\n+                  \"10 in version 0.20 to 100 in 0.22.\")\n+    est = forest()\n+    est = assert_warns_message(FutureWarning, msg_future, est.fit, X, y)\n+\n+    # When n_estimators is a valid value not equal to the default\n+    est = forest(n_estimators=100)\n+    est = assert_no_warnings(est.fit, X, y)\ndiff --git a/sklearn/ensemble/tests/test_voting_classifier.py b/sklearn/ensemble/tests/test_voting_classifier.py\n--- a/sklearn/ensemble/tests/test_voting_classifier.py\n+++ b/sklearn/ensemble/tests/test_voting_classifier.py\n@@ -1,6 +1,8 @@\n \"\"\"Testing for the VotingClassifier\"\"\"\n \n+import pytest\n import numpy as np\n+\n from sklearn.utils.testing import assert_almost_equal, assert_array_equal\n from sklearn.utils.testing import assert_array_almost_equal\n from sklearn.utils.testing import assert_equal, assert_true, assert_false\n@@ -74,6 +76,7 @@ def test_notfitted():\n     assert_raise_message(NotFittedError, msg, eclf.predict_proba, X)\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_majority_label_iris():\n     \"\"\"Check classification by majority label on dataset iris.\"\"\"\n     clf1 = LogisticRegression(random_state=123)\n@@ -86,6 +89,7 @@ def test_majority_label_iris():\n     assert_almost_equal(scores.mean(), 0.95, decimal=2)\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_tie_situation():\n     \"\"\"Check voting classifier selects smaller class label in tie situation.\"\"\"\n     clf1 = LogisticRegression(random_state=123)\n@@ -97,6 +101,7 @@ def test_tie_situation():\n     assert_equal(eclf.fit(X, y).predict(X)[73], 1)\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_weights_iris():\n     \"\"\"Check classification by average probabilities on dataset iris.\"\"\"\n     clf1 = LogisticRegression(random_state=123)\n@@ -110,6 +115,7 @@ def test_weights_iris():\n     assert_almost_equal(scores.mean(), 0.93, decimal=2)\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_predict_on_toy_problem():\n     \"\"\"Manually check predicted class labels for toy dataset.\"\"\"\n     clf1 = LogisticRegression(random_state=123)\n@@ -142,6 +148,7 @@ def test_predict_on_toy_problem():\n     assert_equal(all(eclf.fit(X, y).predict(X)), all([1, 1, 1, 2, 2, 2]))\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_predict_proba_on_toy_problem():\n     \"\"\"Calculate predicted probabilities on toy dataset.\"\"\"\n     clf1 = LogisticRegression(random_state=123)\n@@ -209,6 +216,7 @@ def test_multilabel():\n         return\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_gridsearch():\n     \"\"\"Check GridSearch support.\"\"\"\n     clf1 = LogisticRegression(random_state=1)\n@@ -226,6 +234,7 @@ def test_gridsearch():\n     grid.fit(iris.data, iris.target)\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_parallel_fit():\n     \"\"\"Check parallel backend of VotingClassifier on toy dataset.\"\"\"\n     clf1 = LogisticRegression(random_state=123)\n@@ -247,6 +256,7 @@ def test_parallel_fit():\n     assert_array_almost_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_sample_weight():\n     \"\"\"Tests sample_weight parameter of VotingClassifier\"\"\"\n     clf1 = LogisticRegression(random_state=123)\n@@ -290,6 +300,7 @@ def fit(self, X, y, *args, **sample_weight):\n     eclf.fit(X, y, sample_weight=np.ones((len(y),)))\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_set_params():\n     \"\"\"set_params should be able to set estimators\"\"\"\n     clf1 = LogisticRegression(random_state=123, C=1.0)\n@@ -324,6 +335,7 @@ def test_set_params():\n                  eclf1.get_params()[\"lr\"].get_params()['C'])\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_set_estimator_none():\n     \"\"\"VotingClassifier set_params should be able to set estimators as None\"\"\"\n     # Test predict\n@@ -376,6 +388,7 @@ def test_set_estimator_none():\n     assert_array_equal(eclf2.transform(X1), np.array([[0], [1]]))\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_estimator_weights_format():\n     # Test estimator weights inputs as list and array\n     clf1 = LogisticRegression(random_state=123)\n@@ -393,6 +406,7 @@ def test_estimator_weights_format():\n     assert_array_almost_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_transform():\n     \"\"\"Check transform method of VotingClassifier on toy dataset.\"\"\"\n     clf1 = LogisticRegression(random_state=123)\ndiff --git a/sklearn/ensemble/tests/test_weight_boosting.py b/sklearn/ensemble/tests/test_weight_boosting.py\n--- a/sklearn/ensemble/tests/test_weight_boosting.py\n+++ b/sklearn/ensemble/tests/test_weight_boosting.py\n@@ -1,6 +1,8 @@\n \"\"\"Testing for the boost module (sklearn.ensemble.boost).\"\"\"\n \n+import pytest\n import numpy as np\n+\n from sklearn.utils.testing import assert_array_equal, assert_array_less\n from sklearn.utils.testing import assert_array_almost_equal\n from sklearn.utils.testing import assert_equal, assert_true, assert_greater\n@@ -277,6 +279,7 @@ def test_error():\n                   X, y_class, sample_weight=np.asarray([-1]))\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_base_estimator():\n     # Test different base estimators.\n     from sklearn.ensemble import RandomForestClassifier\ndiff --git a/sklearn/feature_selection/tests/test_from_model.py b/sklearn/feature_selection/tests/test_from_model.py\n--- a/sklearn/feature_selection/tests/test_from_model.py\n+++ b/sklearn/feature_selection/tests/test_from_model.py\n@@ -1,3 +1,4 @@\n+import pytest\n import numpy as np\n \n from sklearn.utils.testing import assert_true\n@@ -32,6 +33,7 @@ def test_invalid_input():\n         assert_raises(ValueError, model.transform, data)\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_input_estimator_unchanged():\n     # Test that SelectFromModel fits on a clone of the estimator.\n     est = RandomForestClassifier()\n@@ -119,6 +121,7 @@ def test_2d_coef():\n             assert_array_almost_equal(X_new, X[:, feature_mask])\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_partial_fit():\n     est = PassiveAggressiveClassifier(random_state=0, shuffle=False,\n                                       max_iter=5, tol=None)\ndiff --git a/sklearn/feature_selection/tests/test_rfe.py b/sklearn/feature_selection/tests/test_rfe.py\n--- a/sklearn/feature_selection/tests/test_rfe.py\n+++ b/sklearn/feature_selection/tests/test_rfe.py\n@@ -1,6 +1,7 @@\n \"\"\"\n Testing Recursive feature elimination\n \"\"\"\n+import pytest\n import numpy as np\n from numpy.testing import assert_array_almost_equal, assert_array_equal\n from scipy import sparse\n@@ -336,6 +337,7 @@ def test_rfe_cv_n_jobs():\n     assert_array_almost_equal(rfecv.grid_scores_, rfecv_grid_scores)\n \n \n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_rfe_cv_groups():\n     generator = check_random_state(0)\n     iris = load_iris()\ndiff --git a/sklearn/tests/test_calibration.py b/sklearn/tests/test_calibration.py\n--- a/sklearn/tests/test_calibration.py\n+++ b/sklearn/tests/test_calibration.py\n@@ -2,6 +2,7 @@\n # License: BSD 3 clause\n \n from __future__ import division\n+import pytest\n import numpy as np\n from scipy import sparse\n from sklearn.model_selection import LeaveOneOut\n@@ -24,7 +25,7 @@\n from sklearn.calibration import calibration_curve\n \n \n-@ignore_warnings\n+@pytest.mark.filterwarnings('ignore:The default value of n_estimators')\n def test_calibration():\n     \"\"\"Test calibration objects with isotonic and sigmoid\"\"\"\n     n_samples = 100\n",
  "problem_statement": "Change default n_estimators in RandomForest (to 100?)\nAnalysis of code on github shows that people use default parameters when they shouldn't. We can make that a little bit less bad by providing reasonable defaults. The default for n_estimators is not great imho and I think we should change it. I suggest 100.\r\nWe could probably run benchmarks with openml if we want to do something empirical, but I think anything is better than 10.\r\n\r\nI'm not sure if I want to tag this 1.0 because really no-one should ever run a random forest with 10 trees imho and therefore deprecation of the current default will show people they have a bug.\n",
  "hints_text": "I would like to give it a shot. Is the default value 100 final? \n@ArihantJain456 I didn't tag this one as \"help wanted\" because I wanted to wait for other core devs to chime in before we do anything.\nI agree. Bad defaults should be deprecated. The warning doesn't hurt.​\n\nI'm also +1 for n_estimators=100 by default\nBoth for this and #11129 I would suggest that the deprecation warning includes a message to encourage the user to change the value manually.\nHas someone taken this up? Can I jump on it?\nBagging* also have n_estimators=10. Is this also a bad default?\n@jnothman I would think so, but I have less experience and it depends on the base estimator, I think? With trees, probably?\nI am fine with changing the default to 100 for random forests and bagging. (with the usual FutureWarning cycle).\r\n\r\n@jnothman I would rather reserve the Blocker label for things that are really harmful if not fixed. To me, fixing this issue is an easy nice-to-heave but does not justify delaying the release cycle.\nOkay. But given the rate at which we release, I think it's worth making\nsure that simple deprecations make it into the release.​ It's not a big\nenough feature to *not* delay the release for it.\n\nI agree.\nI am currently working on this issue.",
  "created_at": "2018-07-15T22:29:04Z",
  "version": "0.20",
  "FAIL_TO_PASS": "[\"sklearn/ensemble/tests/test_forest.py::test_nestimators_future_warning[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_nestimators_future_warning[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_nestimators_future_warning[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_nestimators_future_warning[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_nestimators_future_warning[RandomTreesEmbedding]\"]",
  "PASS_TO_PASS": "[\"sklearn/ensemble/tests/test_forest.py::test_classification_toy[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_classification_toy[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_iris[gini-ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_iris[gini-RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_iris[entropy-ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_iris[entropy-RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_boston[mse-ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_boston[mse-RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_boston[mae-ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_boston[mae-RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_boston[friedman_mse-ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_boston[friedman_mse-RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_regressor_attributes[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_regressor_attributes[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_probability[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_probability[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[ExtraTreesClassifier-gini-float64]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[ExtraTreesClassifier-gini-float32]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[ExtraTreesClassifier-entropy-float64]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[ExtraTreesClassifier-entropy-float32]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[RandomForestClassifier-gini-float64]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[RandomForestClassifier-gini-float32]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[RandomForestClassifier-entropy-float64]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[RandomForestClassifier-entropy-float32]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[ExtraTreesRegressor-mse-float64]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[ExtraTreesRegressor-mse-float32]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[ExtraTreesRegressor-friedman_mse-float64]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[ExtraTreesRegressor-friedman_mse-float32]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[ExtraTreesRegressor-mae-float64]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[ExtraTreesRegressor-mae-float32]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[RandomForestRegressor-mse-float64]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[RandomForestRegressor-mse-float32]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[RandomForestRegressor-friedman_mse-float64]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[RandomForestRegressor-friedman_mse-float32]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[RandomForestRegressor-mae-float64]\", \"sklearn/ensemble/tests/test_forest.py::test_importances[RandomForestRegressor-mae-float32]\", \"sklearn/ensemble/tests/test_forest.py::test_importances_asymptotic\", \"sklearn/ensemble/tests/test_forest.py::test_unfitted_feature_importances[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_unfitted_feature_importances[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_unfitted_feature_importances[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_unfitted_feature_importances[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_unfitted_feature_importances[RandomTreesEmbedding]\", \"sklearn/ensemble/tests/test_forest.py::test_oob_score_classifiers[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_oob_score_classifiers[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_oob_score_regressors[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_oob_score_regressors[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_oob_score_raise_error[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_oob_score_raise_error[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_oob_score_raise_error[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_oob_score_raise_error[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_oob_score_raise_error[RandomTreesEmbedding]\", \"sklearn/ensemble/tests/test_forest.py::test_gridsearch[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_gridsearch[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_parallel[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_parallel[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_parallel[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_parallel[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_pickle[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_pickle[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_pickle[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_pickle[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_multioutput[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_multioutput[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_multioutput[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_multioutput[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_classes_shape[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_classes_shape[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_random_trees_dense_type\", \"sklearn/ensemble/tests/test_forest.py::test_random_trees_dense_equal\", \"sklearn/ensemble/tests/test_forest.py::test_random_hasher\", \"sklearn/ensemble/tests/test_forest.py::test_random_hasher_sparse_data\", \"sklearn/ensemble/tests/test_forest.py::test_parallel_train\", \"sklearn/ensemble/tests/test_forest.py::test_distribution\", \"sklearn/ensemble/tests/test_forest.py::test_max_leaf_nodes_max_depth[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_max_leaf_nodes_max_depth[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_max_leaf_nodes_max_depth[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_max_leaf_nodes_max_depth[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_max_leaf_nodes_max_depth[RandomTreesEmbedding]\", \"sklearn/ensemble/tests/test_forest.py::test_min_samples_split[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_min_samples_split[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_min_samples_split[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_min_samples_split[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_min_samples_split[RandomTreesEmbedding]\", \"sklearn/ensemble/tests/test_forest.py::test_min_samples_leaf[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_min_samples_leaf[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_min_samples_leaf[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_min_samples_leaf[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_min_samples_leaf[RandomTreesEmbedding]\", \"sklearn/ensemble/tests/test_forest.py::test_min_weight_fraction_leaf[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_min_weight_fraction_leaf[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_min_weight_fraction_leaf[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_min_weight_fraction_leaf[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_min_weight_fraction_leaf[RandomTreesEmbedding]\", \"sklearn/ensemble/tests/test_forest.py::test_sparse_input[csr_matrix-ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_sparse_input[csr_matrix-RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_sparse_input[csr_matrix-ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_sparse_input[csr_matrix-RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_sparse_input[csr_matrix-RandomTreesEmbedding]\", \"sklearn/ensemble/tests/test_forest.py::test_sparse_input[csc_matrix-ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_sparse_input[csc_matrix-RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_sparse_input[csc_matrix-ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_sparse_input[csc_matrix-RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_sparse_input[csc_matrix-RandomTreesEmbedding]\", \"sklearn/ensemble/tests/test_forest.py::test_sparse_input[coo_matrix-ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_sparse_input[coo_matrix-RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_sparse_input[coo_matrix-ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_sparse_input[coo_matrix-RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_sparse_input[coo_matrix-RandomTreesEmbedding]\", \"sklearn/ensemble/tests/test_forest.py::test_memory_layout[float64-ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_memory_layout[float64-RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_memory_layout[float64-ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_memory_layout[float64-RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_memory_layout[float32-ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_memory_layout[float32-RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_memory_layout[float32-ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_memory_layout[float32-RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_1d_input[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_1d_input[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_1d_input[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_1d_input[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_1d_input[RandomTreesEmbedding]\", \"sklearn/ensemble/tests/test_forest.py::test_class_weights[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_class_weights[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_class_weight_balanced_and_bootstrap_multi_output[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_class_weight_balanced_and_bootstrap_multi_output[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_class_weight_errors[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_class_weight_errors[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start[RandomTreesEmbedding]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_clear[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_clear[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_clear[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_clear[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_clear[RandomTreesEmbedding]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_smaller_n_estimators[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_smaller_n_estimators[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_smaller_n_estimators[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_smaller_n_estimators[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_smaller_n_estimators[RandomTreesEmbedding]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_equal_n_estimators[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_equal_n_estimators[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_equal_n_estimators[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_equal_n_estimators[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_equal_n_estimators[RandomTreesEmbedding]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_oob[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_oob[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_oob[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_warm_start_oob[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_dtype_convert\", \"sklearn/ensemble/tests/test_forest.py::test_decision_path[ExtraTreesClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_decision_path[RandomForestClassifier]\", \"sklearn/ensemble/tests/test_forest.py::test_decision_path[ExtraTreesRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_decision_path[RandomForestRegressor]\", \"sklearn/ensemble/tests/test_forest.py::test_min_impurity_split\", \"sklearn/ensemble/tests/test_forest.py::test_min_impurity_decrease\", \"sklearn/ensemble/tests/test_voting_classifier.py::test_estimator_init\", \"sklearn/ensemble/tests/test_voting_classifier.py::test_predictproba_hardvoting\", \"sklearn/ensemble/tests/test_voting_classifier.py::test_notfitted\", \"sklearn/ensemble/tests/test_voting_classifier.py::test_majority_label_iris\", \"sklearn/ensemble/tests/test_voting_classifier.py::test_tie_situation\", \"sklearn/ensemble/tests/test_voting_classifier.py::test_weights_iris\", \"sklearn/ensemble/tests/test_voting_classifier.py::test_predict_on_toy_problem\", \"sklearn/ensemble/tests/test_voting_classifier.py::test_predict_proba_on_toy_problem\", \"sklearn/ensemble/tests/test_voting_classifier.py::test_multilabel\", \"sklearn/ensemble/tests/test_voting_classifier.py::test_gridsearch\", \"sklearn/ensemble/tests/test_voting_classifier.py::test_parallel_fit\", \"sklearn/ensemble/tests/test_voting_classifier.py::test_sample_weight\", \"sklearn/ensemble/tests/test_voting_classifier.py::test_sample_weight_kwargs\", \"sklearn/ensemble/tests/test_voting_classifier.py::test_set_params\", \"sklearn/ensemble/tests/test_voting_classifier.py::test_set_estimator_none\", \"sklearn/ensemble/tests/test_voting_classifier.py::test_estimator_weights_format\", \"sklearn/ensemble/tests/test_voting_classifier.py::test_transform\", \"sklearn/ensemble/tests/test_weight_boosting.py::test_samme_proba\", \"sklearn/ensemble/tests/test_weight_boosting.py::test_oneclass_adaboost_proba\", \"sklearn/ensemble/tests/test_weight_boosting.py::test_classification_toy\", \"sklearn/ensemble/tests/test_weight_boosting.py::test_regression_toy\", \"sklearn/ensemble/tests/test_weight_boosting.py::test_iris\", \"sklearn/ensemble/tests/test_weight_boosting.py::test_boston\", \"sklearn/ensemble/tests/test_weight_boosting.py::test_staged_predict\", \"sklearn/ensemble/tests/test_weight_boosting.py::test_gridsearch\", \"sklearn/ensemble/tests/test_weight_boosting.py::test_pickle\", \"sklearn/ensemble/tests/test_weight_boosting.py::test_importances\", \"sklearn/ensemble/tests/test_weight_boosting.py::test_error\", \"sklearn/ensemble/tests/test_weight_boosting.py::test_base_estimator\", \"sklearn/ensemble/tests/test_weight_boosting.py::test_sample_weight_missing\", \"sklearn/ensemble/tests/test_weight_boosting.py::test_sparse_classification\", \"sklearn/ensemble/tests/test_weight_boosting.py::test_sparse_regression\", \"sklearn/ensemble/tests/test_weight_boosting.py::test_sample_weight_adaboost_regressor\", \"sklearn/feature_selection/tests/test_from_model.py::test_invalid_input\", \"sklearn/feature_selection/tests/test_from_model.py::test_input_estimator_unchanged\", \"sklearn/feature_selection/tests/test_from_model.py::test_feature_importances\", \"sklearn/feature_selection/tests/test_from_model.py::test_sample_weight\", \"sklearn/feature_selection/tests/test_from_model.py::test_coef_default_threshold\", \"sklearn/feature_selection/tests/test_from_model.py::test_2d_coef\", \"sklearn/feature_selection/tests/test_from_model.py::test_partial_fit\", \"sklearn/feature_selection/tests/test_from_model.py::test_calling_fit_reinitializes\", \"sklearn/feature_selection/tests/test_from_model.py::test_prefit\", \"sklearn/feature_selection/tests/test_from_model.py::test_threshold_string\", \"sklearn/feature_selection/tests/test_from_model.py::test_threshold_without_refitting\", \"sklearn/feature_selection/tests/test_rfe.py::test_rfe_features_importance\", \"sklearn/feature_selection/tests/test_rfe.py::test_rfe\", \"sklearn/feature_selection/tests/test_rfe.py::test_rfe_mockclassifier\", \"sklearn/feature_selection/tests/test_rfe.py::test_rfecv\", \"sklearn/feature_selection/tests/test_rfe.py::test_rfecv_mockclassifier\", \"sklearn/feature_selection/tests/test_rfe.py::test_rfecv_verbose_output\", \"sklearn/feature_selection/tests/test_rfe.py::test_rfe_estimator_tags\", \"sklearn/feature_selection/tests/test_rfe.py::test_rfe_min_step\", \"sklearn/feature_selection/tests/test_rfe.py::test_number_of_subsets_of_features\", \"sklearn/feature_selection/tests/test_rfe.py::test_rfe_cv_n_jobs\", \"sklearn/feature_selection/tests/test_rfe.py::test_rfe_cv_groups\", \"sklearn/tests/test_calibration.py::test_calibration\", \"sklearn/tests/test_calibration.py::test_sample_weight\", \"sklearn/tests/test_calibration.py::test_calibration_multiclass\", \"sklearn/tests/test_calibration.py::test_calibration_prefit\", \"sklearn/tests/test_calibration.py::test_sigmoid_calibration\", \"sklearn/tests/test_calibration.py::test_calibration_curve\", \"sklearn/tests/test_calibration.py::test_calibration_nan_imputer\", \"sklearn/tests/test_calibration.py::test_calibration_prob_sum\", \"sklearn/tests/test_calibration.py::test_calibration_less_classes\"]",
  "environment_setup_commit": "55bf5d93e5674f13a1134d93a11fd0cd11aabcd1",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.962624",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}