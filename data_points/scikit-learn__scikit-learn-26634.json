{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-26634",
  "base_commit": "9cbcc1f205e8be4dad1f383239e98381abb28bd0",
  "patch": "diff --git a/sklearn/decomposition/_nmf.py b/sklearn/decomposition/_nmf.py\n--- a/sklearn/decomposition/_nmf.py\n+++ b/sklearn/decomposition/_nmf.py\n@@ -27,6 +27,7 @@\n from ..exceptions import ConvergenceWarning\n from ..utils import check_array, check_random_state, gen_batches, metadata_routing\n from ..utils._param_validation import (\n+    Hidden,\n     Interval,\n     StrOptions,\n     validate_params,\n@@ -69,14 +70,19 @@ def trace_dot(X, Y):\n \n def _check_init(A, shape, whom):\n     A = check_array(A)\n-    if np.shape(A) != shape:\n+    if shape[0] != \"auto\" and A.shape[0] != shape[0]:\n         raise ValueError(\n-            \"Array with wrong shape passed to %s. Expected %s, but got %s \"\n-            % (whom, shape, np.shape(A))\n+            f\"Array with wrong first dimension passed to {whom}. Expected {shape[0]}, \"\n+            f\"but got {A.shape[0]}.\"\n+        )\n+    if shape[1] != \"auto\" and A.shape[1] != shape[1]:\n+        raise ValueError(\n+            f\"Array with wrong second dimension passed to {whom}. Expected {shape[1]}, \"\n+            f\"but got {A.shape[1]}.\"\n         )\n     check_non_negative(A, whom)\n     if np.max(A) == 0:\n-        raise ValueError(\"Array passed to %s is full of zeros.\" % whom)\n+        raise ValueError(f\"Array passed to {whom} is full of zeros.\")\n \n \n def _beta_divergence(X, W, H, beta, square_root=False):\n@@ -903,7 +909,7 @@ def non_negative_factorization(\n     X,\n     W=None,\n     H=None,\n-    n_components=None,\n+    n_components=\"warn\",\n     *,\n     init=None,\n     update_H=True,\n@@ -976,9 +982,14 @@ def non_negative_factorization(\n         If `update_H=False`, it is used as a constant, to solve for W only.\n         If `None`, uses the initialisation method specified in `init`.\n \n-    n_components : int, default=None\n+    n_components : int or {'auto'} or None, default=None\n         Number of components, if n_components is not set all features\n         are kept.\n+        If `n_components='auto'`, the number of components is automatically inferred\n+        from `W` or `H` shapes.\n+\n+        .. versionchanged:: 1.4\n+            Added `'auto'` value.\n \n     init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n         Method used to initialize the procedure.\n@@ -1133,7 +1144,12 @@ class _BaseNMF(ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator,\n     __metadata_request__inverse_transform = {\"W\": metadata_routing.UNUSED}\n \n     _parameter_constraints: dict = {\n-        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\n+        \"n_components\": [\n+            Interval(Integral, 1, None, closed=\"left\"),\n+            None,\n+            StrOptions({\"auto\"}),\n+            Hidden(StrOptions({\"warn\"})),\n+        ],\n         \"init\": [\n             StrOptions({\"random\", \"nndsvd\", \"nndsvda\", \"nndsvdar\", \"custom\"}),\n             None,\n@@ -1153,7 +1169,7 @@ class _BaseNMF(ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator,\n \n     def __init__(\n         self,\n-        n_components=None,\n+        n_components=\"warn\",\n         *,\n         init=None,\n         beta_loss=\"frobenius\",\n@@ -1179,6 +1195,16 @@ def __init__(\n     def _check_params(self, X):\n         # n_components\n         self._n_components = self.n_components\n+        if self.n_components == \"warn\":\n+            warnings.warn(\n+                (\n+                    \"The default value of `n_components` will change from `None` to\"\n+                    \" `'auto'` in 1.6. Set the value of `n_components` to `None`\"\n+                    \" explicitly to supress the warning.\"\n+                ),\n+                FutureWarning,\n+            )\n+            self._n_components = None  # Keeping the old default value\n         if self._n_components is None:\n             self._n_components = X.shape[1]\n \n@@ -1188,32 +1214,61 @@ def _check_params(self, X):\n     def _check_w_h(self, X, W, H, update_H):\n         \"\"\"Check W and H, or initialize them.\"\"\"\n         n_samples, n_features = X.shape\n+\n         if self.init == \"custom\" and update_H:\n             _check_init(H, (self._n_components, n_features), \"NMF (input H)\")\n             _check_init(W, (n_samples, self._n_components), \"NMF (input W)\")\n+            if self._n_components == \"auto\":\n+                self._n_components = H.shape[0]\n+\n             if H.dtype != X.dtype or W.dtype != X.dtype:\n                 raise TypeError(\n                     \"H and W should have the same dtype as X. Got \"\n                     \"H.dtype = {} and W.dtype = {}.\".format(H.dtype, W.dtype)\n                 )\n+\n         elif not update_H:\n+            if W is not None:\n+                warnings.warn(\n+                    \"When update_H=False, the provided initial W is not used.\",\n+                    RuntimeWarning,\n+                )\n+\n             _check_init(H, (self._n_components, n_features), \"NMF (input H)\")\n+            if self._n_components == \"auto\":\n+                self._n_components = H.shape[0]\n+\n             if H.dtype != X.dtype:\n                 raise TypeError(\n                     \"H should have the same dtype as X. Got H.dtype = {}.\".format(\n                         H.dtype\n                     )\n                 )\n+\n             # 'mu' solver should not be initialized by zeros\n             if self.solver == \"mu\":\n                 avg = np.sqrt(X.mean() / self._n_components)\n                 W = np.full((n_samples, self._n_components), avg, dtype=X.dtype)\n             else:\n                 W = np.zeros((n_samples, self._n_components), dtype=X.dtype)\n+\n         else:\n+            if W is not None or H is not None:\n+                warnings.warn(\n+                    (\n+                        \"When init!='custom', provided W or H are ignored. Set \"\n+                        \" init='custom' to use them as initialization.\"\n+                    ),\n+                    RuntimeWarning,\n+                )\n+\n+            if self._n_components == \"auto\":\n+                self._n_components = X.shape[1]\n+\n             W, H = _initialize_nmf(\n                 X, self._n_components, init=self.init, random_state=self.random_state\n             )\n+\n         return W, H\n \n     def _compute_regularization(self, X):\n@@ -1352,9 +1407,14 @@ class NMF(_BaseNMF):\n \n     Parameters\n     ----------\n-    n_components : int, default=None\n+    n_components : int or {'auto'} or None, default=None\n         Number of components, if n_components is not set all features\n         are kept.\n+        If `n_components='auto'`, the number of components is automatically inferred\n+        from W or H shapes.\n+\n+        .. versionchanged:: 1.4\n+            Added `'auto'` value.\n \n     init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n         Method used to initialize the procedure.\n@@ -1517,7 +1577,7 @@ class NMF(_BaseNMF):\n \n     def __init__(\n         self,\n-        n_components=None,\n+        n_components=\"warn\",\n         *,\n         init=None,\n         solver=\"cd\",\n@@ -1786,9 +1846,14 @@ class MiniBatchNMF(_BaseNMF):\n \n     Parameters\n     ----------\n-    n_components : int, default=None\n+    n_components : int or {'auto'} or None, default=None\n         Number of components, if `n_components` is not set all features\n         are kept.\n+        If `n_components='auto'`, the number of components is automatically inferred\n+        from W or H shapes.\n+\n+        .. versionchanged:: 1.4\n+            Added `'auto'` value.\n \n     init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n         Method used to initialize the procedure.\n@@ -1953,7 +2018,7 @@ class MiniBatchNMF(_BaseNMF):\n \n     def __init__(\n         self,\n-        n_components=None,\n+        n_components=\"warn\",\n         *,\n         init=None,\n         batch_size=1024,\n",
  "test_patch": "diff --git a/sklearn/decomposition/tests/test_nmf.py b/sklearn/decomposition/tests/test_nmf.py\n--- a/sklearn/decomposition/tests/test_nmf.py\n+++ b/sklearn/decomposition/tests/test_nmf.py\n@@ -45,9 +45,11 @@ def test_initialize_nn_output():\n         assert not ((W < 0).any() or (H < 0).any())\n \n \n+# TODO(1.6): remove the warning filter for `n_components`\n @pytest.mark.filterwarnings(\n     r\"ignore:The multiplicative update \\('mu'\\) solver cannot update zeros present in\"\n-    r\" the initialization\"\n+    r\" the initialization\",\n+    \"ignore:The default value of `n_components` will change\",\n )\n def test_parameter_checking():\n     # Here we only check for invalid parameter values that are not already\n@@ -267,6 +269,8 @@ def test_nmf_inverse_transform(solver):\n     assert_array_almost_equal(A, A_new, decimal=2)\n \n \n+# TODO(1.6): remove the warning filter\n+@pytest.mark.filterwarnings(\"ignore:The default value of `n_components` will change\")\n def test_mbnmf_inverse_transform():\n     # Test that MiniBatchNMF.transform followed by MiniBatchNMF.inverse_transform\n     # is close to the identity\n@@ -344,6 +348,8 @@ def test_nmf_sparse_transform(Estimator, solver):\n     assert_allclose(A_fit_tr, A_tr, atol=1e-1)\n \n \n+# TODO(1.6): remove the warning filter\n+@pytest.mark.filterwarnings(\"ignore:The default value of `n_components` will change\")\n @pytest.mark.parametrize(\"init\", [\"random\", \"nndsvd\"])\n @pytest.mark.parametrize(\"solver\", (\"cd\", \"mu\"))\n @pytest.mark.parametrize(\"alpha_W\", (0.0, 1.0))\n@@ -610,6 +616,8 @@ def _assert_nmf_no_nan(X, beta_loss):\n         _assert_nmf_no_nan(X_csr, beta_loss)\n \n \n+# TODO(1.6): remove the warning filter\n+@pytest.mark.filterwarnings(\"ignore:The default value of `n_components` will change\")\n @pytest.mark.parametrize(\"beta_loss\", [-0.5, 0.0])\n def test_minibatch_nmf_negative_beta_loss(beta_loss):\n     \"\"\"Check that an error is raised if beta_loss < 0 and X contains zeros.\"\"\"\n@@ -766,6 +774,8 @@ def test_nmf_underflow():\n     assert_almost_equal(res, ref)\n \n \n+# TODO(1.6): remove the warning filter\n+@pytest.mark.filterwarnings(\"ignore:The default value of `n_components` will change\")\n @pytest.mark.parametrize(\n     \"dtype_in, dtype_out\",\n     [\n@@ -784,13 +794,21 @@ def test_nmf_dtype_match(Estimator, solver, dtype_in, dtype_out):\n     X = np.random.RandomState(0).randn(20, 15).astype(dtype_in, copy=False)\n     np.abs(X, out=X)\n \n-    nmf = Estimator(alpha_W=1.0, alpha_H=1.0, tol=1e-2, random_state=0, **solver)\n+    nmf = Estimator(\n+        alpha_W=1.0,\n+        alpha_H=1.0,\n+        tol=1e-2,\n+        random_state=0,\n+        **solver,\n+    )\n \n     assert nmf.fit(X).transform(X).dtype == dtype_out\n     assert nmf.fit_transform(X).dtype == dtype_out\n     assert nmf.components_.dtype == dtype_out\n \n \n+# TODO(1.6): remove the warning filter\n+@pytest.mark.filterwarnings(\"ignore:The default value of `n_components` will change\")\n @pytest.mark.parametrize(\n     [\"Estimator\", \"solver\"],\n     [[NMF, {\"solver\": \"cd\"}], [NMF, {\"solver\": \"mu\"}], [MiniBatchNMF, {}]],\n@@ -807,6 +825,8 @@ def test_nmf_float32_float64_consistency(Estimator, solver):\n     assert_allclose(W32, W64, atol=1e-5)\n \n \n+# TODO(1.6): remove the warning filter\n+@pytest.mark.filterwarnings(\"ignore:The default value of `n_components` will change\")\n @pytest.mark.parametrize(\"Estimator\", [NMF, MiniBatchNMF])\n def test_nmf_custom_init_dtype_error(Estimator):\n     # Check that an error is raise if custom H and/or W don't have the same\n@@ -896,6 +916,8 @@ def test_feature_names_out():\n     assert_array_equal([f\"nmf{i}\" for i in range(3)], names)\n \n \n+# TODO(1.6): remove the warning filter\n+@pytest.mark.filterwarnings(\"ignore:The default value of `n_components` will change\")\n def test_minibatch_nmf_verbose():\n     # Check verbose mode of MiniBatchNMF for better coverage.\n     A = np.random.RandomState(0).random_sample((100, 10))\n@@ -932,3 +954,106 @@ def test_NMF_inverse_transform_W_deprecation():\n \n     with pytest.warns(FutureWarning, match=\"Input argument `W` was renamed to `Xt`\"):\n         est.inverse_transform(W=Xt)\n+\n+\n+@pytest.mark.parametrize(\"Estimator\", [NMF, MiniBatchNMF])\n+def test_nmf_n_components_auto(Estimator):\n+    # Check that n_components is correctly inferred\n+    # from the provided custom initialization.\n+    rng = np.random.RandomState(0)\n+    X = rng.random_sample((6, 5))\n+    W = rng.random_sample((6, 2))\n+    H = rng.random_sample((2, 5))\n+    est = Estimator(\n+        n_components=\"auto\",\n+        init=\"custom\",\n+        random_state=0,\n+        tol=1e-6,\n+    )\n+    est.fit_transform(X, W=W, H=H)\n+    assert est._n_components == H.shape[0]\n+\n+\n+def test_nmf_non_negative_factorization_n_components_auto():\n+    # Check that n_components is correctly inferred from the provided\n+    # custom initialization.\n+    rng = np.random.RandomState(0)\n+    X = rng.random_sample((6, 5))\n+    W_init = rng.random_sample((6, 2))\n+    H_init = rng.random_sample((2, 5))\n+    W, H, _ = non_negative_factorization(\n+        X, W=W_init, H=H_init, init=\"custom\", n_components=\"auto\"\n+    )\n+    assert H.shape == H_init.shape\n+    assert W.shape == W_init.shape\n+\n+\n+# TODO(1.6): remove\n+def test_nmf_n_components_default_value_warning():\n+    rng = np.random.RandomState(0)\n+    X = rng.random_sample((6, 5))\n+    H = rng.random_sample((2, 5))\n+    with pytest.warns(\n+        FutureWarning, match=\"The default value of `n_components` will change from\"\n+    ):\n+        non_negative_factorization(X, H=H)\n+\n+\n+def test_nmf_n_components_auto_no_h_update():\n+    # Tests that non_negative_factorization does not fail when setting\n+    # n_components=\"auto\" also tests that the inferred n_component\n+    # value is the right one.\n+    rng = np.random.RandomState(0)\n+    X = rng.random_sample((6, 5))\n+    H_true = rng.random_sample((2, 5))\n+    W, H, _ = non_negative_factorization(\n+        X, H=H_true, n_components=\"auto\", update_H=False\n+    )  # should not fail\n+    assert_allclose(H, H_true)\n+    assert W.shape == (X.shape[0], H_true.shape[0])\n+\n+\n+def test_nmf_w_h_not_used_warning():\n+    # Check that warnings are raised if user provided W and H are not used\n+    # and initialization overrides value of W or H\n+    rng = np.random.RandomState(0)\n+    X = rng.random_sample((6, 5))\n+    W_init = rng.random_sample((6, 2))\n+    H_init = rng.random_sample((2, 5))\n+    with pytest.warns(\n+        RuntimeWarning,\n+        match=\"When init!='custom', provided W or H are ignored\",\n+    ):\n+        non_negative_factorization(X, H=H_init, update_H=True, n_components=\"auto\")\n+\n+    with pytest.warns(\n+        RuntimeWarning,\n+        match=\"When init!='custom', provided W or H are ignored\",\n+    ):\n+        non_negative_factorization(\n+            X, W=W_init, H=H_init, update_H=True, n_components=\"auto\"\n+        )\n+\n+    with pytest.warns(\n+        RuntimeWarning, match=\"When update_H=False, the provided initial W is not used.\"\n+    ):\n+        # When update_H is False, W is ignored regardless of init\n+        # TODO: use the provided W when init=\"custom\".\n+        non_negative_factorization(\n+            X, W=W_init, H=H_init, update_H=False, n_components=\"auto\"\n+        )\n+\n+\n+def test_nmf_custom_init_shape_error():\n+    # Check that an informative error is raised when custom initialization does not\n+    # have the right shape\n+    rng = np.random.RandomState(0)\n+    X = rng.random_sample((6, 5))\n+    H = rng.random_sample((2, 5))\n+    nmf = NMF(n_components=2, init=\"custom\", random_state=0)\n+\n+    with pytest.raises(ValueError, match=\"Array with wrong first dimension passed\"):\n+        nmf.fit(X, H=H, W=rng.random_sample((5, 2)))\n+\n+    with pytest.raises(ValueError, match=\"Array with wrong second dimension passed\"):\n+        nmf.fit(X, H=H, W=rng.random_sample((6, 3)))\ndiff --git a/sklearn/tests/test_docstring_parameters.py b/sklearn/tests/test_docstring_parameters.py\n--- a/sklearn/tests/test_docstring_parameters.py\n+++ b/sklearn/tests/test_docstring_parameters.py\n@@ -260,6 +260,10 @@ def test_fit_docstring_attributes(name, Estimator):\n     ):\n         est.set_params(force_alpha=True)\n \n+    # TODO(1.6): remove (avoid FutureWarning)\n+    if Estimator.__name__ in (\"NMF\", \"MiniBatchNMF\"):\n+        est.set_params(n_components=\"auto\")\n+\n     if Estimator.__name__ == \"QuantileRegressor\":\n         solver = \"highs\" if sp_version >= parse_version(\"1.6.0\") else \"interior-point\"\n         est.set_params(solver=solver)\n",
  "problem_statement": "NMF fit transform without updating H should not require the user to input \"n_components\"\nThe `_fit_transform` function of the `_nmf` module has the option to set `update_H=False`, where the H matrix is left constant. the private method `_fit_transform` is called by the exposed `non_negative_factorization` function.\r\nIn a scenario I've encountered, the user provides the H matrix, meaning the number of components is known a-prior, and there is no reason for the algorithm to run the lines\r\n```\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n``` \r\nand raise an error later in the `_check_w_h`\r\n\r\n\r\nhttps://github.com/scikit-learn/scikit-learn/blob/f5ec34e0f76277ba6d0a77d3033db0af83899b64/sklearn/decomposition/_nmf.py#LL1188C19-L1188C19\n",
  "hints_text": "Hi @yotamcons, the ``fit_transform`` method of NMF does not expose the option ``update_H``. It's the private method ``_fit_transform`` that does expose it, but it's there for internal purpose, so it's advised not to call it directly. I f you really want to use it, you need to set n_components appropriately.\nSorry for the misleading writing, the problem is that `non_negative_factorization` internally calls `_fit_transform` and causes the said issue. I've edited the issue to make it clearer.\nThanks for the clarification. Indeed, we check that the shape are consistent. `n_components` is only determined by the `n_components` parameter so it must be set. The same behavior occurs when you set `init=\"custom\"` and provide H and W with wrong shapes.\r\n\r\nI wouldn't change this behavior. Instead we could probably improve the description of the W and H parameters to mention that their shapes must be in line with `n_components`. Would you like to submit a PR ?\nAs the given `H` already holds the information regarding the `n_components`, wouldn't it be preferred to set `n_components = H.shape[0]`?\r\nthis is especially true if the users haven't provided `n_components` themselves\nSuppose you set `init=\"custom\"` and provide W and H with shapes that don't match. Which one would you take ? I think the best solution is to raise an error in that situation. Another example: if you set `update_H=False` and set `n_components` but provide H with an non matching shape. I would also raise an error here.\r\n\r\n`n_components=None` doesn't mean ignored, it just means `n_components=n_features`. I don't think it should generate a different behavior than setting any other value regarding matching shapes. What prevents you to set `n_components=H.shape[0]` ?\nIt seems I'm unable to convey the scenario to you:\r\nIf you enter `update_H=False` then you do not initiate neither H nor W (which is just taken as the average entry of `X`\r\nOnly in that scenario the n_components should be ignored, as the decomposition rank is decided by the dimensions of the given H\nPlease provide a minimal reproducible, causing the error, and what you expect to happen. Would make it easier to investigate.\nreproducible code:\r\n```\r\nimport numpy as np\r\nfrom sklearn.decomposition import non_negative_factorization\r\n\r\nW_true = np.random.rand(6, 2)\r\nH_true = np.random.rand(2, 5)\r\nX = np.dot(W_true, H_true)\r\n\r\nW, H, n_iter = non_negative_factorization(X, H=H_true, update_H=False)\r\n```\r\n\r\nI get the error: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/Applications/miniconda3/envs/gep-dynamics/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-14-a8ac745879a9>\", line 1, in <module>\r\n    W, H, n_iter = non_negative_factorization(X, H=H_true, update_H=False)\r\n  File \"/Applications/miniconda3/envs/gep-dynamics/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 192, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/Applications/miniconda3/envs/gep-dynamics/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py\", line 1111, in non_negative_factorization\r\n    W, H, n_iter = est._fit_transform(X, W=W, H=H, update_H=update_H)\r\n  File \"/Applications/miniconda3/envs/gep-dynamics/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py\", line 1625, in _fit_transform\r\n    W, H = self._check_w_h(X, W, H, update_H)\r\n  File \"/Applications/miniconda3/envs/gep-dynamics/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py\", line 1184, in _check_w_h\r\n    _check_init(H, (self._n_components, n_features), \"NMF (input H)\")\r\n  File \"/Applications/miniconda3/envs/gep-dynamics/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py\", line 68, in _check_init\r\n    raise ValueError(\r\nValueError: Array with wrong shape passed to NMF (input H). Expected (5, 5), but got (2, 5) \r\n```\r\n\r\nThe error is caused due to the wrong dimensions of `self._n_components`. The source of the wrong dimension is that `_fit_transform` calls `self._check_params(X)`, which doesn't see a value assinged to `self.n_components` sets `self._n_components = X.shape[1]`. The error can be avoided by providing the `n_components` argument.\r\n\r\nThe key point of my issue is that when `H` is provided by the user, then **clearly** the user means to have `H.shape[0]` components in the decomposition, and thus the `n_components` argument is redundant.\nAs I said, the default ``n_components=None`` doesn't mean unspecified n_components, but automatically set ``n_components=n_features``. When H is user provided, there's a check for constistency between ``n_components`` and ``H.shape[0]``. I think this is a desirable behavior, rather than having ``n_components=None`` to mean a different thing based on ``update_H`` being True or False. What prevents you from setting ``n_components=H.shape[0]`` ?\nDiving into the code, i now see the issue has nothing to do with `update_H`. If a user provides either `W` or `H`, then `n_components` should be set accordingly. This is a completely different scenario then when neither of the both is provided, and users shouldn't have the need to specify n_components\nWhat prevents you from setting ``n_components=H.shape[0]`` ?\nI personally don't believe in giving a function the same information twice, and errors that don't make sense until you dive into the classes where the functions are defined.\r\nIf a user gives the data of the rank (implicitly in the dimensions of W/H), why make them give the same information again explicitly?\n> errors that don't make sense until you dive into the classes where the functions are defined.\r\n\r\nThe error makes sense because `n_components=None` is documented as equivalent to `n_components=n_features`. Then it is expected that an error is raised if `W` or `H` doesn't have the appropriate shape, since it does not correspond to the requested `n_components`.\r\n\r\nI'm not against changing the default to `n_components=\"auto\"` (with a deprecation cycle) such that:\r\n- if neither W or H are provided, it defaults to `n_features`\r\n- if `H` is provided, it's inferred from `H`\r\n- if `W` and `H` are provided, it's inferred from both and if their shape don't match, an error is raised\r\n- in any case, if n_components != \"auto\" and `W` or `H` is provided, an error is raised if they don't match.",
  "created_at": "2023-06-20T14:01:24Z",
  "version": "1.4",
  "FAIL_TO_PASS": "[\"sklearn/decomposition/tests/test_nmf.py::test_nmf_n_components_auto[NMF]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_n_components_auto[MiniBatchNMF]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_non_negative_factorization_n_components_auto\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_n_components_default_value_warning\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_n_components_auto_no_h_update\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_w_h_not_used_warning\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_custom_init_shape_error\"]",
  "PASS_TO_PASS": "[\"sklearn/decomposition/tests/test_nmf.py::test_convergence_warning[NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_convergence_warning[NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_convergence_warning[MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_initialize_nn_output\", \"sklearn/decomposition/tests/test_nmf.py::test_parameter_checking\", \"sklearn/decomposition/tests/test_nmf.py::test_initialize_close\", \"sklearn/decomposition/tests/test_nmf.py::test_initialize_variants\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-0.0-None-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-0.0-None-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-0.0-None-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-0.0-nndsvd-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-0.0-nndsvd-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-0.0-nndsvd-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-0.0-nndsvda-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-0.0-nndsvda-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-0.0-nndsvda-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-0.0-nndsvdar-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-0.0-nndsvdar-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-0.0-nndsvdar-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-0.0-random-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-0.0-random-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-0.0-random-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-1.0-None-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-1.0-None-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-1.0-None-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-1.0-nndsvd-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-1.0-nndsvd-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-1.0-nndsvd-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-1.0-nndsvda-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-1.0-nndsvda-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-1.0-nndsvda-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-1.0-nndsvdar-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-1.0-nndsvdar-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-1.0-nndsvdar-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-1.0-random-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-1.0-random-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[0.0-1.0-random-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-0.0-None-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-0.0-None-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-0.0-None-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-0.0-nndsvd-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-0.0-nndsvd-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-0.0-nndsvd-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-0.0-nndsvda-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-0.0-nndsvda-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-0.0-nndsvda-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-0.0-nndsvdar-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-0.0-nndsvdar-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-0.0-nndsvdar-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-0.0-random-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-0.0-random-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-0.0-random-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-1.0-None-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-1.0-None-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-1.0-None-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-1.0-nndsvd-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-1.0-nndsvd-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-1.0-nndsvd-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-1.0-nndsvda-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-1.0-nndsvda-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-1.0-nndsvda-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-1.0-nndsvdar-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-1.0-nndsvdar-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-1.0-nndsvdar-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-1.0-random-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-1.0-random-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[1.0-1.0-random-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-0.0-None-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-0.0-None-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-0.0-None-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-0.0-nndsvd-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-0.0-nndsvd-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-0.0-nndsvd-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-0.0-nndsvda-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-0.0-nndsvda-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-0.0-nndsvda-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-0.0-nndsvdar-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-0.0-nndsvdar-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-0.0-nndsvdar-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-0.0-random-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-0.0-random-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-0.0-random-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-1.0-None-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-1.0-None-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-1.0-None-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-1.0-nndsvd-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-1.0-nndsvd-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-1.0-nndsvd-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-1.0-nndsvda-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-1.0-nndsvda-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-1.0-nndsvda-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-1.0-nndsvdar-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-1.0-nndsvdar-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-1.0-nndsvdar-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-1.0-random-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-1.0-random-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_nn_output[same-1.0-random-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_close[NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_close[NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_fit_close[MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_true_reconstruction\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_transform[cd]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_transform[mu]\", \"sklearn/decomposition/tests/test_nmf.py::test_minibatch_nmf_transform\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_transform_custom_init[NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_transform_custom_init[NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_transform_custom_init[MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_inverse_transform[cd]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_inverse_transform[mu]\", \"sklearn/decomposition/tests/test_nmf.py::test_mbnmf_inverse_transform\", \"sklearn/decomposition/tests/test_nmf.py::test_n_components_greater_n_features[NMF]\", \"sklearn/decomposition/tests/test_nmf.py::test_n_components_greater_n_features[MiniBatchNMF]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_input[0.0-0.0-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_input[0.0-0.0-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_input[0.0-0.0-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_input[0.0-1.0-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_input[0.0-1.0-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_input[0.0-1.0-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_input[1.0-0.0-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_input[1.0-0.0-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_input[1.0-0.0-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_input[1.0-1.0-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_input[1.0-1.0-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_input[1.0-1.0-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_input[same-0.0-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_input[same-0.0-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_input[same-0.0-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_input[same-1.0-NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_input[same-1.0-NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_input[same-1.0-MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_transform[NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_transform[NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_sparse_transform[MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[0.0-0.0-cd-random]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[0.0-0.0-cd-nndsvd]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[0.0-0.0-mu-random]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[0.0-0.0-mu-nndsvd]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[0.0-1.0-cd-random]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[0.0-1.0-cd-nndsvd]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[0.0-1.0-mu-random]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[0.0-1.0-mu-nndsvd]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[1.0-0.0-cd-random]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[1.0-0.0-cd-nndsvd]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[1.0-0.0-mu-random]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[1.0-0.0-mu-nndsvd]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[1.0-1.0-cd-random]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[1.0-1.0-cd-nndsvd]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[1.0-1.0-mu-random]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[1.0-1.0-mu-nndsvd]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[same-0.0-cd-random]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[same-0.0-cd-nndsvd]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[same-0.0-mu-random]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[same-0.0-mu-nndsvd]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[same-1.0-cd-random]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[same-1.0-cd-nndsvd]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[same-1.0-mu-random]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_consistency[same-1.0-mu-nndsvd]\", \"sklearn/decomposition/tests/test_nmf.py::test_non_negative_factorization_checking\", \"sklearn/decomposition/tests/test_nmf.py::test_beta_divergence\", \"sklearn/decomposition/tests/test_nmf.py::test_special_sparse_dot\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_multiplicative_update_sparse\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_negative_beta_loss\", \"sklearn/decomposition/tests/test_nmf.py::test_minibatch_nmf_negative_beta_loss[-0.5]\", \"sklearn/decomposition/tests/test_nmf.py::test_minibatch_nmf_negative_beta_loss[0.0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_regularization[NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_regularization[NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_regularization[MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_decreasing[cd]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_decreasing[mu]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_underflow\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_dtype_match[NMF-solver0-float32-float32]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_dtype_match[NMF-solver0-float64-float64]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_dtype_match[NMF-solver0-int32-float64]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_dtype_match[NMF-solver0-int64-float64]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_dtype_match[NMF-solver1-float32-float32]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_dtype_match[NMF-solver1-float64-float64]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_dtype_match[NMF-solver1-int32-float64]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_dtype_match[NMF-solver1-int64-float64]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_dtype_match[MiniBatchNMF-solver2-float32-float32]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_dtype_match[MiniBatchNMF-solver2-float64-float64]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_dtype_match[MiniBatchNMF-solver2-int32-float64]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_dtype_match[MiniBatchNMF-solver2-int64-float64]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_float32_float64_consistency[NMF-solver0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_float32_float64_consistency[NMF-solver1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_float32_float64_consistency[MiniBatchNMF-solver2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_custom_init_dtype_error[NMF]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_custom_init_dtype_error[MiniBatchNMF]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_minibatchnmf_equivalence[-0.5]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_minibatchnmf_equivalence[0]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_minibatchnmf_equivalence[0.5]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_minibatchnmf_equivalence[1]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_minibatchnmf_equivalence[1.5]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_minibatchnmf_equivalence[2]\", \"sklearn/decomposition/tests/test_nmf.py::test_nmf_minibatchnmf_equivalence[2.5]\", \"sklearn/decomposition/tests/test_nmf.py::test_minibatch_nmf_partial_fit\", \"sklearn/decomposition/tests/test_nmf.py::test_feature_names_out\", \"sklearn/decomposition/tests/test_nmf.py::test_minibatch_nmf_verbose\", \"sklearn/decomposition/tests/test_nmf.py::test_NMF_inverse_transform_W_deprecation\"]",
  "environment_setup_commit": "33a1f1690e7a7007633f59b6bee32017f4229864",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:31.026208",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}