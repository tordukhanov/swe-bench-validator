{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-25805",
  "base_commit": "67ea7206bc052eb752f7881eb6043a00fe27c800",
  "patch": "diff --git a/sklearn/calibration.py b/sklearn/calibration.py\n--- a/sklearn/calibration.py\n+++ b/sklearn/calibration.py\n@@ -308,9 +308,6 @@ def fit(self, X, y, sample_weight=None, **fit_params):\n         if sample_weight is not None:\n             sample_weight = _check_sample_weight(sample_weight, X)\n \n-        for sample_aligned_params in fit_params.values():\n-            check_consistent_length(y, sample_aligned_params)\n-\n         # TODO(1.4): Remove when base_estimator is removed\n         if self.base_estimator != \"deprecated\":\n             if self.estimator is not None:\n",
  "test_patch": "diff --git a/sklearn/tests/test_calibration.py b/sklearn/tests/test_calibration.py\n--- a/sklearn/tests/test_calibration.py\n+++ b/sklearn/tests/test_calibration.py\n@@ -974,23 +974,6 @@ def fit(self, X, y, **fit_params):\n         pc_clf.fit(X, y, sample_weight=sample_weight)\n \n \n-def test_calibration_with_fit_params_inconsistent_length(data):\n-    \"\"\"fit_params having different length than data should raise the\n-    correct error message.\n-    \"\"\"\n-    X, y = data\n-    fit_params = {\"a\": y[:5]}\n-    clf = CheckingClassifier(expected_fit_params=fit_params)\n-    pc_clf = CalibratedClassifierCV(clf)\n-\n-    msg = (\n-        r\"Found input variables with inconsistent numbers of \"\n-        r\"samples: \\[\" + str(N_SAMPLES) + r\", 5\\]\"\n-    )\n-    with pytest.raises(ValueError, match=msg):\n-        pc_clf.fit(X, y, **fit_params)\n-\n-\n @pytest.mark.parametrize(\"method\", [\"sigmoid\", \"isotonic\"])\n @pytest.mark.parametrize(\"ensemble\", [True, False])\n def test_calibrated_classifier_cv_zeros_sample_weights_equivalence(method, ensemble):\n@@ -1054,3 +1037,17 @@ def test_calibrated_classifier_deprecation_base_estimator(data):\n     warn_msg = \"`base_estimator` was renamed to `estimator`\"\n     with pytest.warns(FutureWarning, match=warn_msg):\n         calibrated_classifier.fit(*data)\n+\n+\n+def test_calibration_with_non_sample_aligned_fit_param(data):\n+    \"\"\"Check that CalibratedClassifierCV does not enforce sample alignment\n+    for fit parameters.\"\"\"\n+\n+    class TestClassifier(LogisticRegression):\n+        def fit(self, X, y, sample_weight=None, fit_param=None):\n+            assert fit_param is not None\n+            return super().fit(X, y, sample_weight=sample_weight)\n+\n+    CalibratedClassifierCV(estimator=TestClassifier()).fit(\n+        *data, fit_param=np.ones(len(data[1]) + 1)\n+    )\n",
  "problem_statement": "CalibratedClassifierCV fails on lgbm fit_params\nHi,\r\n\r\nI'm trying to use CalibratedClassifierCV to calibrate the probabilities from a LGBM model. The issue is that when I try CalibratedClassifierCV with eval_set, I get an error ValueError: Found input variables with inconsistent numbers of samples: [43364, 1] which is caused by check_consistent_length function in validation.py The input to eval set is [X_valid,Y_valid] where both X_valid,Y_valid are arrays with different shape. Since this format is a requirement for LGBM eval set https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html, I am not sure how will I make the check_consistent_length pass in my scenario. Full code is given below:\r\n\r\n```python\r\nimport lightgbm as lgbm\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\n\r\ndef _train_lgbm_model():\r\n    model = lgbm.LGBMClassifier(**parameters.lgbm_params)\r\n    fit_params = {\r\n        \"eval_set\": [(X_valid, Y_valid)],\r\n        \"eval_names\": [\"train\", \"valid\"],\r\n        \"verbose\": 0,\r\n    }\r\n    return model, fit_params\r\n\r\nmodel = CalibratedClassifierCV(model, method='isotonic')\r\ntrained_model = model.fit(X_train, Y_train, **fit_param)\r\n\r\nError: ValueError: Found input variables with inconsistent numbers of samples: [43364, 1]\r\n\r\n``` \r\nX_train.shape = (43364, 152)\r\nX_valid.shape = (43364,)\r\nY_train.shape = (43364, 152)\r\nY_valid.shape = (43364,)\r\n\n",
  "hints_text": "Once we have metadata routing with would have an explicit way to tell how to broadcast such parameters to the base estimators. However as far as I know, the current state of SLEP6 does not have a way to tell whether or not we want to apply cross-validation indexing and therefore disable the length consistency check for the things that are not meant to be the sample-aligned fit params.\r\n\r\nMaybe @adrinjalali knows if this topic was already addressed in SLEP6 or not.\nI think this is orthogonal to SLEP006, and a bug in `CalibratedClassifierCV`, which has these lines:\r\n\r\nhttps://github.com/scikit-learn/scikit-learn/blob/c991e30c96ace1565604b429de22e36ed6b1e7bd/sklearn/calibration.py#L311-L312\r\n\r\nI'm not sure why this check is there. IMO the logic should be like `GridSearchCV`, where we split the data where the length is the same as `y`, otherwise pass unchanged.\r\n\r\ncc @glemaitre \n@adrinjalali - Thanks for your response Adrin, this was my conclusion as well. What would be the path forward to resolve this bug?\nI'll open a PR to propose a fix @Sidshroff \nI looked at other estimators and indeed we never check that `fit_params` are sample-aligned but only propagate. I could not find the reasoning in the original PR, we were a bit sloppy regarding this aspect. I think a PR removing this check is fine.",
  "created_at": "2023-03-10T12:37:03Z",
  "version": "1.3",
  "FAIL_TO_PASS": "[\"sklearn/tests/test_calibration.py::test_calibration_with_non_sample_aligned_fit_param\"]",
  "PASS_TO_PASS": "[\"sklearn/tests/test_calibration.py::test_calibration[True-sigmoid]\", \"sklearn/tests/test_calibration.py::test_calibration[True-isotonic]\", \"sklearn/tests/test_calibration.py::test_calibration[False-sigmoid]\", \"sklearn/tests/test_calibration.py::test_calibration[False-isotonic]\", \"sklearn/tests/test_calibration.py::test_calibration_default_estimator\", \"sklearn/tests/test_calibration.py::test_calibration_cv_splitter[True]\", \"sklearn/tests/test_calibration.py::test_calibration_cv_splitter[False]\", \"sklearn/tests/test_calibration.py::test_sample_weight[True-sigmoid]\", \"sklearn/tests/test_calibration.py::test_sample_weight[True-isotonic]\", \"sklearn/tests/test_calibration.py::test_sample_weight[False-sigmoid]\", \"sklearn/tests/test_calibration.py::test_sample_weight[False-isotonic]\", \"sklearn/tests/test_calibration.py::test_parallel_execution[True-sigmoid]\", \"sklearn/tests/test_calibration.py::test_parallel_execution[True-isotonic]\", \"sklearn/tests/test_calibration.py::test_parallel_execution[False-sigmoid]\", \"sklearn/tests/test_calibration.py::test_parallel_execution[False-isotonic]\", \"sklearn/tests/test_calibration.py::test_calibration_multiclass[0-True-sigmoid]\", \"sklearn/tests/test_calibration.py::test_calibration_multiclass[0-True-isotonic]\", \"sklearn/tests/test_calibration.py::test_calibration_multiclass[0-False-sigmoid]\", \"sklearn/tests/test_calibration.py::test_calibration_multiclass[0-False-isotonic]\", \"sklearn/tests/test_calibration.py::test_calibration_multiclass[1-True-sigmoid]\", \"sklearn/tests/test_calibration.py::test_calibration_multiclass[1-True-isotonic]\", \"sklearn/tests/test_calibration.py::test_calibration_multiclass[1-False-sigmoid]\", \"sklearn/tests/test_calibration.py::test_calibration_multiclass[1-False-isotonic]\", \"sklearn/tests/test_calibration.py::test_calibration_zero_probability\", \"sklearn/tests/test_calibration.py::test_calibration_prefit\", \"sklearn/tests/test_calibration.py::test_calibration_ensemble_false[sigmoid]\", \"sklearn/tests/test_calibration.py::test_calibration_ensemble_false[isotonic]\", \"sklearn/tests/test_calibration.py::test_sigmoid_calibration\", \"sklearn/tests/test_calibration.py::test_calibration_curve\", \"sklearn/tests/test_calibration.py::test_calibration_curve_with_unnormalized_proba\", \"sklearn/tests/test_calibration.py::test_calibration_nan_imputer[True]\", \"sklearn/tests/test_calibration.py::test_calibration_nan_imputer[False]\", \"sklearn/tests/test_calibration.py::test_calibration_prob_sum[True]\", \"sklearn/tests/test_calibration.py::test_calibration_prob_sum[False]\", \"sklearn/tests/test_calibration.py::test_calibration_less_classes[True]\", \"sklearn/tests/test_calibration.py::test_calibration_less_classes[False]\", \"sklearn/tests/test_calibration.py::test_calibration_accepts_ndarray[X0]\", \"sklearn/tests/test_calibration.py::test_calibration_accepts_ndarray[X1]\", \"sklearn/tests/test_calibration.py::test_calibration_dict_pipeline\", \"sklearn/tests/test_calibration.py::test_calibration_attributes[clf0-2]\", \"sklearn/tests/test_calibration.py::test_calibration_attributes[clf1-prefit]\", \"sklearn/tests/test_calibration.py::test_calibration_inconsistent_prefit_n_features_in\", \"sklearn/tests/test_calibration.py::test_calibration_votingclassifier\", \"sklearn/tests/test_calibration.py::test_calibration_display_validation\", \"sklearn/tests/test_calibration.py::test_calibration_display_non_binary[from_estimator]\", \"sklearn/tests/test_calibration.py::test_calibration_display_non_binary[from_predictions]\", \"sklearn/tests/test_calibration.py::test_calibration_display_compute[uniform-5]\", \"sklearn/tests/test_calibration.py::test_calibration_display_compute[uniform-10]\", \"sklearn/tests/test_calibration.py::test_calibration_display_compute[quantile-5]\", \"sklearn/tests/test_calibration.py::test_calibration_display_compute[quantile-10]\", \"sklearn/tests/test_calibration.py::test_plot_calibration_curve_pipeline\", \"sklearn/tests/test_calibration.py::test_calibration_display_default_labels[None-_line1]\", \"sklearn/tests/test_calibration.py::test_calibration_display_default_labels[my_est-my_est]\", \"sklearn/tests/test_calibration.py::test_calibration_display_label_class_plot\", \"sklearn/tests/test_calibration.py::test_calibration_display_name_multiple_calls[from_estimator]\", \"sklearn/tests/test_calibration.py::test_calibration_display_name_multiple_calls[from_predictions]\", \"sklearn/tests/test_calibration.py::test_calibration_display_ref_line\", \"sklearn/tests/test_calibration.py::test_calibration_curve_pos_label_error_str[str]\", \"sklearn/tests/test_calibration.py::test_calibration_curve_pos_label_error_str[object]\", \"sklearn/tests/test_calibration.py::test_calibration_curve_pos_label[str]\", \"sklearn/tests/test_calibration.py::test_calibration_curve_pos_label[object]\", \"sklearn/tests/test_calibration.py::test_calibration_display_pos_label[None-1]\", \"sklearn/tests/test_calibration.py::test_calibration_display_pos_label[0-0]\", \"sklearn/tests/test_calibration.py::test_calibration_display_pos_label[1-1]\", \"sklearn/tests/test_calibration.py::test_calibrated_classifier_cv_double_sample_weights_equivalence[True-sigmoid]\", \"sklearn/tests/test_calibration.py::test_calibrated_classifier_cv_double_sample_weights_equivalence[True-isotonic]\", \"sklearn/tests/test_calibration.py::test_calibrated_classifier_cv_double_sample_weights_equivalence[False-sigmoid]\", \"sklearn/tests/test_calibration.py::test_calibrated_classifier_cv_double_sample_weights_equivalence[False-isotonic]\", \"sklearn/tests/test_calibration.py::test_calibration_with_fit_params[list]\", \"sklearn/tests/test_calibration.py::test_calibration_with_fit_params[array]\", \"sklearn/tests/test_calibration.py::test_calibration_with_sample_weight_base_estimator[sample_weight0]\", \"sklearn/tests/test_calibration.py::test_calibration_with_sample_weight_base_estimator[sample_weight1]\", \"sklearn/tests/test_calibration.py::test_calibration_without_sample_weight_base_estimator\", \"sklearn/tests/test_calibration.py::test_calibrated_classifier_cv_zeros_sample_weights_equivalence[True-sigmoid]\", \"sklearn/tests/test_calibration.py::test_calibrated_classifier_cv_zeros_sample_weights_equivalence[True-isotonic]\", \"sklearn/tests/test_calibration.py::test_calibrated_classifier_cv_zeros_sample_weights_equivalence[False-sigmoid]\", \"sklearn/tests/test_calibration.py::test_calibrated_classifier_cv_zeros_sample_weights_equivalence[False-isotonic]\", \"sklearn/tests/test_calibration.py::test_calibrated_classifier_error_base_estimator\", \"sklearn/tests/test_calibration.py::test_calibrated_classifier_deprecation_base_estimator\"]",
  "environment_setup_commit": "1e8a5b833d1b58f3ab84099c4582239af854b23a",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:31.023228",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}