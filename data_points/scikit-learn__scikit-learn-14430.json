{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-14430",
  "base_commit": "7e022a7e8ba0f95b65d9acade559da95115ad9e5",
  "patch": "diff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py\n--- a/sklearn/feature_extraction/text.py\n+++ b/sklearn/feature_extraction/text.py\n@@ -15,6 +15,7 @@\n import array\n from collections import defaultdict\n from collections.abc import Mapping\n+from functools import partial\n import numbers\n from operator import itemgetter\n import re\n@@ -44,6 +45,72 @@\n            'strip_tags']\n \n \n+def _preprocess(doc, accent_function=None, lower=False):\n+    \"\"\"Chain together an optional series of text preprocessing steps to\n+    apply to a document.\n+\n+    Parameters\n+    ----------\n+    doc: str\n+        The string to preprocess\n+    accent_function: callable\n+        Function for handling accented characters. Common strategies include\n+        normalizing and removing.\n+    lower: bool\n+        Whether to use str.lower to lowercase all fo the text\n+\n+    Returns\n+    -------\n+    doc: str\n+        preprocessed string\n+    \"\"\"\n+    if lower:\n+        doc = doc.lower()\n+    if accent_function is not None:\n+        doc = accent_function(doc)\n+    return doc\n+\n+\n+def _analyze(doc, analyzer=None, tokenizer=None, ngrams=None,\n+             preprocessor=None, decoder=None, stop_words=None):\n+    \"\"\"Chain together an optional series of text processing steps to go from\n+    a single document to ngrams, with or without tokenizing or preprocessing.\n+\n+    If analyzer is used, only the decoder argument is used, as the analyzer is\n+    intended to replace the preprocessor, tokenizer, and ngrams steps.\n+\n+    Parameters\n+    ----------\n+    analyzer: callable\n+    tokenizer: callable\n+    ngrams: callable\n+    preprocessor: callable\n+    decoder: callable\n+    stop_words: list\n+\n+    Returns\n+    -------\n+    ngrams: list\n+        A sequence of tokens, possibly with pairs, triples, etc.\n+    \"\"\"\n+\n+    if decoder is not None:\n+        doc = decoder(doc)\n+    if analyzer is not None:\n+        doc = analyzer(doc)\n+    else:\n+        if preprocessor is not None:\n+            doc = preprocessor(doc)\n+        if tokenizer is not None:\n+            doc = tokenizer(doc)\n+        if ngrams is not None:\n+            if stop_words is not None:\n+                doc = ngrams(doc, stop_words)\n+            else:\n+                doc = ngrams(doc)\n+    return doc\n+\n+\n def strip_accents_unicode(s):\n     \"\"\"Transform accentuated unicode symbols into their simple counterpart\n \n@@ -232,16 +299,9 @@ def build_preprocessor(self):\n         if self.preprocessor is not None:\n             return self.preprocessor\n \n-        # unfortunately python functools package does not have an efficient\n-        # `compose` function that would have allowed us to chain a dynamic\n-        # number of functions. However the cost of a lambda call is a few\n-        # hundreds of nanoseconds which is negligible when compared to the\n-        # cost of tokenizing a string of 1000 chars for instance.\n-        noop = lambda x: x\n-\n         # accent stripping\n         if not self.strip_accents:\n-            strip_accents = noop\n+            strip_accents = None\n         elif callable(self.strip_accents):\n             strip_accents = self.strip_accents\n         elif self.strip_accents == 'ascii':\n@@ -252,17 +312,16 @@ def build_preprocessor(self):\n             raise ValueError('Invalid value for \"strip_accents\": %s' %\n                              self.strip_accents)\n \n-        if self.lowercase:\n-            return lambda x: strip_accents(x.lower())\n-        else:\n-            return strip_accents\n+        return partial(\n+            _preprocess, accent_function=strip_accents, lower=self.lowercase\n+        )\n \n     def build_tokenizer(self):\n         \"\"\"Return a function that splits a string into a sequence of tokens\"\"\"\n         if self.tokenizer is not None:\n             return self.tokenizer\n         token_pattern = re.compile(self.token_pattern)\n-        return lambda doc: token_pattern.findall(doc)\n+        return token_pattern.findall\n \n     def get_stop_words(self):\n         \"\"\"Build or fetch the effective stop words list\"\"\"\n@@ -335,24 +394,28 @@ def build_analyzer(self):\n         if callable(self.analyzer):\n             if self.input in ['file', 'filename']:\n                 self._validate_custom_analyzer()\n-            return lambda doc: self.analyzer(self.decode(doc))\n+            return partial(\n+                _analyze, analyzer=self.analyzer, decoder=self.decode\n+            )\n \n         preprocess = self.build_preprocessor()\n \n         if self.analyzer == 'char':\n-            return lambda doc: self._char_ngrams(preprocess(self.decode(doc)))\n+            return partial(_analyze, ngrams=self._char_ngrams,\n+                           preprocessor=preprocess, decoder=self.decode)\n \n         elif self.analyzer == 'char_wb':\n-            return lambda doc: self._char_wb_ngrams(\n-                preprocess(self.decode(doc)))\n+            return partial(_analyze, ngrams=self._char_wb_ngrams,\n+                           preprocessor=preprocess, decoder=self.decode)\n \n         elif self.analyzer == 'word':\n             stop_words = self.get_stop_words()\n             tokenize = self.build_tokenizer()\n             self._check_stop_words_consistency(stop_words, preprocess,\n                                                tokenize)\n-            return lambda doc: self._word_ngrams(\n-                tokenize(preprocess(self.decode(doc))), stop_words)\n+            return partial(_analyze, ngrams=self._word_ngrams,\n+                           tokenizer=tokenize, preprocessor=preprocess,\n+                           decoder=self.decode, stop_words=stop_words)\n \n         else:\n             raise ValueError('%s is not a valid tokenization scheme/analyzer' %\n",
  "test_patch": "diff --git a/sklearn/feature_extraction/tests/test_text.py b/sklearn/feature_extraction/tests/test_text.py\n--- a/sklearn/feature_extraction/tests/test_text.py\n+++ b/sklearn/feature_extraction/tests/test_text.py\n@@ -480,7 +480,12 @@ def test_vectorizer():\n \n     # ascii preprocessor?\n     v3.set_params(strip_accents='ascii', lowercase=False)\n-    assert v3.build_preprocessor() == strip_accents_ascii\n+    processor = v3.build_preprocessor()\n+    text = (\"J'ai mangé du kangourou  ce midi, \"\n+            \"c'était pas très bon.\")\n+    expected = strip_accents_ascii(text)\n+    result = processor(text)\n+    assert expected == result\n \n     # error on bad strip_accents param\n     v3.set_params(strip_accents='_gabbledegook_', preprocessor=None)\n@@ -884,6 +889,25 @@ def test_pickling_vectorizer():\n                 orig.fit_transform(JUNK_FOOD_DOCS).toarray())\n \n \n+@pytest.mark.parametrize('factory', [\n+    CountVectorizer.build_analyzer,\n+    CountVectorizer.build_preprocessor,\n+    CountVectorizer.build_tokenizer,\n+])\n+def test_pickling_built_processors(factory):\n+    \"\"\"Tokenizers cannot be pickled\n+    https://github.com/scikit-learn/scikit-learn/issues/12833\n+    \"\"\"\n+    vec = CountVectorizer()\n+    function = factory(vec)\n+    text = (\"J'ai mangé du kangourou  ce midi, \"\n+            \"c'était pas très bon.\")\n+    roundtripped_function = pickle.loads(pickle.dumps(function))\n+    expected = function(text)\n+    result = roundtripped_function(text)\n+    assert result == expected\n+\n+\n def test_countvectorizer_vocab_sets_when_pickling():\n     # ensure that vocabulary of type set is coerced to a list to\n     # preserve iteration ordering after deserialization\n",
  "problem_statement": "Pickling Tokenizers fails due to use of lambdas\n#### Description\r\nCannot pickle a `CountVectorizer` using the builtin python `pickle` module, likely due to the use of lambdas in https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py \r\n\r\n#### Steps/Code to Reproduce\r\n\r\nExample:\r\n```python\r\nimport pickle\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nraw_texts = [\"this is a text\", \"oh look, here's another\", \"including my full model vocab is...well, a lot\"]\r\nvectorizer = CountVectorizer(max_features=20000, token_pattern=r\"\\b\\w+\\b\")\r\nvectorizer.fit(raw_texts)\r\ntokenizer = vectorizer.build_tokenizer()\r\noutput_file = 'foo.pkl'\r\nwith open(output_file, 'wb') as out:\r\n    pickle.dump(tokenizer, out)\r\nwith open(output_file, 'rb') as infile:\r\n    pickle.load(infile)\r\n```\r\n\r\n#### Expected Results\r\n\r\nProgram runs without error\r\n\r\n#### Actual Results\r\n\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"tst.py\", line 14, in <module>\r\n    pickle.dump(tokenizer, out)\r\nAttributeError: Can't pickle local object 'VectorizerMixin.build_tokenizer.<locals>.<lambda>'\r\n```\r\n\r\n#### Workaround:\r\n\r\nInstead of the builtin `pickle`, use `cloudpickle`, which can capture the `lambda` expression.\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nFor scikit-learn >= 0.20:\r\nimport sklearn; sklearn.show_versions()\r\nFor scikit-learn < 0.20:\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport numpy; print(\"NumPy\", numpy.__version__)\r\nimport scipy; print(\"SciPy\", scipy.__version__)\r\nimport sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\n-->\r\n\r\nVersion information:\r\n\r\n```python\r\n>>> import sklearn\r\n>>> print(sklearn.show_versions())\r\n/home/jay/Documents/projects/evidence-inference/venv/lib/python3.6/site-packages/numpy/distutils/system_info.py:625: UserWarning:\r\n    Atlas (http://math-atlas.sourceforge.net/) libraries not found.\r\n    Directories to search for the libraries can be specified in the\r\n    numpy/distutils/site.cfg file (section [atlas]) or by setting\r\n    the ATLAS environment variable.\r\n  self.calc_info()\r\n/usr/bin/ld: cannot find -lcblas\r\ncollect2: error: ld returned 1 exit status\r\n/usr/bin/ld: cannot find -lcblas\r\ncollect2: error: ld returned 1 exit status\r\n\r\nSystem:\r\n    python: 3.6.5 (default, Apr  1 2018, 05:46:30)  [GCC 7.3.0]\r\nexecutable: /home/jay/Documents/projects/evidence-inference/venv/bin/python\r\n   machine: Linux-4.15.0-39-generic-x86_64-with-Ubuntu-18.04-bionic\r\n\r\nBLAS:\r\n    macros: NO_ATLAS_INFO=1, HAVE_CBLAS=None\r\n  lib_dirs: /usr/lib/x86_64-linux-gnu\r\ncblas_libs: cblas\r\n\r\nPython deps:\r\n       pip: 18.1\r\nsetuptools: 39.1.0\r\n   sklearn: 0.20.2\r\n     numpy: 1.15.1\r\n     scipy: 1.1.0\r\n    Cython: None\r\n    pandas: 0.23.4\r\nNone\r\n```\r\n\r\n#### Similar Issues\r\n\r\nI think this is similar to issues:\r\n* https://github.com/scikit-learn/scikit-learn/issues/10807 \r\n* https://github.com/scikit-learn/scikit-learn/issues/9467 (looking at the stackoverflow thread at https://stackoverflow.com/questions/25348532/can-python-pickle-lambda-functions/25353243#25353243 , it suggests using `dill` which also seems to work for the toy example)\r\n\r\n#### Proposed fix\r\n \r\nNaively, I would make one of the two changes below, but I am not familiar with the scikit-learn codebase, so they might not be appropriate:\r\n1. Update the FAQ to direct people to other serialization libraries (perhaps I missed this recommendation?), e.g. `cloudpickle` at https://github.com/cloudpipe/cloudpickle or `dill`\r\n2. Remove the use of the lambdas in the vectorizer and replace them with locally def'd functions. I suspect that this solution is flawed because it doesn't account for other uses of lambdas elsewhere in the codebase, and the only complete solution would be to stop using lambdas, but these are a useful language feature. \r\n\n",
  "hints_text": "You're saying we can't pickle the tokenizer, pickling the vectorizer is fine, right? The title says vectorizer.\r\nWe could rewrite it to allow pickling the tokenizer if we want to support that. There doesn't really seem a reason not to do that, but it's not a very common use-case, right?\r\n\r\nAnd I would prefer the fix 2.\r\nAll estimators pickle and we test that. Even though lambdas are used in some places. The reason there is an issue here is because you're trying to pickle something that's more of an internal data structure (though it's a public interface).\nI edited the title - you're right. I do not know how common a use-case it is - I happen to be saving the tokenizer and vectorizer in a pytorch model and came across this error, so I thought it was worth reporting and maybe solving (maybe I'm wrong). \r\n\r\nSo far as I can tell, there are six lambdas in what I believe to be the offending file at https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py , which isn't many.\nI think it's fine to fix this. I guess I just wanted to say this is not really a bigger issue and basically everything in sklearn pickles but you found an object that doesn't and we should just fix that object ;)\nI made a PR that fixes the issue but I did not add a test case - where would be appropriate?\n> Remove the use of the lambdas in the vectorizer and replace them with locally def'd functions. \r\n\r\n+1 particularly that some of those are assigned to a named variable, which is not PEP8 compatible.",
  "created_at": "2019-07-21T02:47:05Z",
  "version": "0.22",
  "FAIL_TO_PASS": "[\"sklearn/feature_extraction/tests/test_text.py::test_pickling_built_processors[build_analyzer]\", \"sklearn/feature_extraction/tests/test_text.py::test_pickling_built_processors[build_preprocessor]\", \"sklearn/feature_extraction/tests/test_text.py::test_pickling_built_processors[build_tokenizer]\"]",
  "PASS_TO_PASS": "[\"sklearn/feature_extraction/tests/test_text.py::test_strip_accents\", \"sklearn/feature_extraction/tests/test_text.py::test_to_ascii\", \"sklearn/feature_extraction/tests/test_text.py::test_word_analyzer_unigrams[CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_word_analyzer_unigrams[HashingVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_word_analyzer_unigrams_and_bigrams\", \"sklearn/feature_extraction/tests/test_text.py::test_unicode_decode_error\", \"sklearn/feature_extraction/tests/test_text.py::test_char_ngram_analyzer\", \"sklearn/feature_extraction/tests/test_text.py::test_char_wb_ngram_analyzer\", \"sklearn/feature_extraction/tests/test_text.py::test_word_ngram_analyzer\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_custom_vocabulary\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_custom_vocabulary_pipeline\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_custom_vocabulary_repeated_indices\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_custom_vocabulary_gap_index\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_stop_words\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_empty_vocabulary\", \"sklearn/feature_extraction/tests/test_text.py::test_fit_countvectorizer_twice\", \"sklearn/feature_extraction/tests/test_text.py::test_tf_idf_smoothing\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_no_smoothing\", \"sklearn/feature_extraction/tests/test_text.py::test_sublinear_tf\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_setters\", \"sklearn/feature_extraction/tests/test_text.py::test_hashing_vectorizer\", \"sklearn/feature_extraction/tests/test_text.py::test_feature_names\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_max_features[CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_max_features[TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_count_vectorizer_max_features\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_max_df\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_min_df\", \"sklearn/feature_extraction/tests/test_text.py::test_count_binary_occurrences\", \"sklearn/feature_extraction/tests/test_text.py::test_hashed_binary_occurrences\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_inverse_transform[CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_inverse_transform[TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_count_vectorizer_pipeline_grid_selection\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_pipeline_grid_selection\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_pipeline_cross_validation\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_unicode\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_with_fixed_vocabulary\", \"sklearn/feature_extraction/tests/test_text.py::test_pickling_vectorizer\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_vocab_sets_when_pickling\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_vocab_dicts_when_pickling\", \"sklearn/feature_extraction/tests/test_text.py::test_stop_words_removal\", \"sklearn/feature_extraction/tests/test_text.py::test_pickling_transformer\", \"sklearn/feature_extraction/tests/test_text.py::test_transformer_idf_setter\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_setter\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidfvectorizer_invalid_idf_attr\", \"sklearn/feature_extraction/tests/test_text.py::test_non_unique_vocab\", \"sklearn/feature_extraction/tests/test_text.py::test_hashingvectorizer_nan_in_docs\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidfvectorizer_binary\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidfvectorizer_export_idf\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_vocab_clone\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_string_object_as_input[CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_string_object_as_input[TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_string_object_as_input[HashingVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_transformer_type[float32]\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_transformer_type[float64]\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_transformer_sparse\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_type[int32-float64-True]\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_type[int64-float64-True]\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_type[float32-float32-False]\", \"sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_type[float64-float64-False]\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizers_invalid_ngram_range[vec1]\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizers_invalid_ngram_range[vec2]\", \"sklearn/feature_extraction/tests/test_text.py::test_vectorizer_stop_words_inconsistent\", \"sklearn/feature_extraction/tests/test_text.py::test_countvectorizer_sort_features_64bit_sparse_indices\", \"sklearn/feature_extraction/tests/test_text.py::test_stop_word_validation_custom_preprocessor[CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_stop_word_validation_custom_preprocessor[TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_stop_word_validation_custom_preprocessor[HashingVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_error[filename-FileNotFoundError--CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_error[filename-FileNotFoundError--TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_error[file-AttributeError-'str'\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[file-<lambda>0-CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[file-<lambda>0-TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[file-<lambda>0-HashingVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[file-<lambda>1-CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[file-<lambda>1-TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[file-<lambda>1-HashingVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[filename-<lambda>0-CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[filename-<lambda>0-TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[filename-<lambda>0-HashingVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[filename-<lambda>1-CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[filename-<lambda>1-TfidfVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_change_behavior[filename-<lambda>1-HashingVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_reraise_error[CountVectorizer]\", \"sklearn/feature_extraction/tests/test_text.py::test_callable_analyzer_reraise_error[TfidfVectorizer]\"]",
  "environment_setup_commit": "7e85a6d1f038bbb932b36f18d75df6be937ed00d",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:31.004223",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}