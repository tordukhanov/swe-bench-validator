{
  "repo": "pytest-dev/pytest",
  "instance_id": "pytest-dev__pytest-5559",
  "base_commit": "60a358fa2dc82a571c68d1be2d25703b51351538",
  "patch": "diff --git a/doc/en/example/nonpython/conftest.py b/doc/en/example/nonpython/conftest.py\n--- a/doc/en/example/nonpython/conftest.py\n+++ b/doc/en/example/nonpython/conftest.py\n@@ -3,7 +3,7 @@\n \n \n def pytest_collect_file(parent, path):\n-    if path.ext == \".yml\" and path.basename.startswith(\"test\"):\n+    if path.ext == \".yaml\" and path.basename.startswith(\"test\"):\n         return YamlFile(path, parent)\n \n \ndiff --git a/src/_pytest/_code/code.py b/src/_pytest/_code/code.py\n--- a/src/_pytest/_code/code.py\n+++ b/src/_pytest/_code/code.py\n@@ -544,7 +544,7 @@ def match(self, regexp):\n         \"\"\"\n         __tracebackhide__ = True\n         if not re.search(regexp, str(self.value)):\n-            assert 0, \"Pattern '{!s}' not found in '{!s}'\".format(regexp, self.value)\n+            assert 0, \"Pattern {!r} not found in {!r}\".format(regexp, str(self.value))\n         return True\n \n \ndiff --git a/src/_pytest/stepwise.py b/src/_pytest/stepwise.py\n--- a/src/_pytest/stepwise.py\n+++ b/src/_pytest/stepwise.py\n@@ -72,7 +72,7 @@ def pytest_collection_modifyitems(self, session, config, items):\n \n     def pytest_runtest_logreport(self, report):\n         # Skip this hook if plugin is not active or the test is xfailed.\n-        if not self.active or \"xfail\" in report.keywords:\n+        if not self.active:\n             return\n \n         if report.failed:\n",
  "test_patch": "diff --git a/doc/en/example/nonpython/test_simple.yml b/doc/en/example/nonpython/test_simple.yaml\nsimilarity index 75%\nrename from doc/en/example/nonpython/test_simple.yml\nrename to doc/en/example/nonpython/test_simple.yaml\n--- a/doc/en/example/nonpython/test_simple.yml\n+++ b/doc/en/example/nonpython/test_simple.yaml\n@@ -1,4 +1,4 @@\n-# test_simple.yml\n+# test_simple.yaml\n ok:\n     sub1: sub1\n \ndiff --git a/testing/python/raises.py b/testing/python/raises.py\n--- a/testing/python/raises.py\n+++ b/testing/python/raises.py\n@@ -220,13 +220,20 @@ def test_raises_match(self):\n             int(\"asdf\")\n \n         msg = \"with base 16\"\n-        expr = r\"Pattern '{}' not found in 'invalid literal for int\\(\\) with base 10: 'asdf''\".format(\n+        expr = r\"Pattern '{}' not found in \\\"invalid literal for int\\(\\) with base 10: 'asdf'\\\"\".format(\n             msg\n         )\n         with pytest.raises(AssertionError, match=expr):\n             with pytest.raises(ValueError, match=msg):\n                 int(\"asdf\", base=10)\n \n+    def test_match_failure_string_quoting(self):\n+        with pytest.raises(AssertionError) as excinfo:\n+            with pytest.raises(AssertionError, match=\"'foo\"):\n+                raise AssertionError(\"'bar\")\n+        msg, = excinfo.value.args\n+        assert msg == 'Pattern \"\\'foo\" not found in \"\\'bar\"'\n+\n     def test_raises_match_wrong_type(self):\n         \"\"\"Raising an exception with the wrong type and match= given.\n \ndiff --git a/testing/test_stepwise.py b/testing/test_stepwise.py\n--- a/testing/test_stepwise.py\n+++ b/testing/test_stepwise.py\n@@ -165,3 +165,56 @@ def test_stop_on_collection_errors(broken_testdir, broken_first):\n         files.reverse()\n     result = broken_testdir.runpytest(\"-v\", \"--strict-markers\", \"--stepwise\", *files)\n     result.stdout.fnmatch_lines(\"*errors during collection*\")\n+\n+\n+def test_xfail_handling(testdir):\n+    \"\"\"Ensure normal xfail is ignored, and strict xfail interrupts the session in sw mode\n+\n+    (#5547)\n+    \"\"\"\n+    contents = \"\"\"\n+        import pytest\n+        def test_a(): pass\n+\n+        @pytest.mark.xfail(strict={strict})\n+        def test_b(): assert {assert_value}\n+\n+        def test_c(): pass\n+        def test_d(): pass\n+    \"\"\"\n+    testdir.makepyfile(contents.format(assert_value=\"0\", strict=\"False\"))\n+    result = testdir.runpytest(\"--sw\", \"-v\")\n+    result.stdout.fnmatch_lines(\n+        [\n+            \"*::test_a PASSED *\",\n+            \"*::test_b XFAIL *\",\n+            \"*::test_c PASSED *\",\n+            \"*::test_d PASSED *\",\n+            \"* 3 passed, 1 xfailed in *\",\n+        ]\n+    )\n+\n+    testdir.makepyfile(contents.format(assert_value=\"1\", strict=\"True\"))\n+    result = testdir.runpytest(\"--sw\", \"-v\")\n+    result.stdout.fnmatch_lines(\n+        [\n+            \"*::test_a PASSED *\",\n+            \"*::test_b FAILED *\",\n+            \"* Interrupted*\",\n+            \"* 1 failed, 1 passed in *\",\n+        ]\n+    )\n+\n+    # because we are writing to the same file, mtime might not be affected enough to\n+    # invalidate the cache, making this next run flaky\n+    testdir.tmpdir.join(\"__pycache__\").remove()\n+    testdir.makepyfile(contents.format(assert_value=\"0\", strict=\"True\"))\n+    result = testdir.runpytest(\"--sw\", \"-v\")\n+    result.stdout.fnmatch_lines(\n+        [\n+            \"*::test_b XFAIL *\",\n+            \"*::test_c PASSED *\",\n+            \"*::test_d PASSED *\",\n+            \"* 2 passed, 1 deselected, 1 xfailed in *\",\n+        ]\n+    )\n",
  "problem_statement": "pytest stepwise doesn't work with xfail strict failures\n```\r\ngraingert@onomastic:~/projects/foo$ cat tests/test_foo.py \r\nimport pytest\r\n\r\n\r\n@pytest.mark.xfail(reason=\"pass\")\r\ndef test_a():\r\n    pass\r\n\r\n\r\n@pytest.mark.xfail(reason=\"pass\")\r\ndef test_b():\r\n    pass\r\ngraingert@onomastic:~/projects/foo$ cat tests/pytest.ini \r\n[pytest]\r\naddopts = --strict\r\nxfail_strict=true\r\ngraingert@onomastic:~/projects/foo$ pytest --sw tests/\r\n================================ test session starts ================================\r\nplatform linux -- Python 3.7.3, pytest-5.0.0, py-1.8.0, pluggy-0.12.0\r\nrootdir: /home/graingert/projects/foo/tests, inifile: pytest.ini\r\ncollected 2 items                                                                   \r\nstepwise: no previously failed tests, not skipping.\r\n\r\ntests/test_foo.py FF                                                          [100%]\r\n\r\n===================================== FAILURES ======================================\r\n______________________________________ test_a _______________________________________\r\n[XPASS(strict)] pass\r\n______________________________________ test_b _______________________________________\r\n[XPASS(strict)] pass\r\n============================= 2 failed in 0.01 seconds ==============================\r\n```\nrecommended pytest-runner in setup_requires means packages fail to install often\nThe recommendation to add `pytest-runner` to `setup_requires` means that all users of that package end up with an unnecessary pytest-runner package installed. This is bad because it bypasses pip hashes and [`--trusted-host`](https://github.com/pypa/pip/issues/4156)\r\n\r\nhttps://docs.pytest.org/en/latest/goodpractices.html#integrating-with-setuptools-python-setup-py-test-pytest-runner\r\n\r\nhttps://github.com/MechanicalSoup/MechanicalSoup/pull/224\r\nhttps://github.com/rxcomm/pyaxo/issues/26\r\nhttps://github.com/jpadilla/pyjwt/issues/179\n",
  "hints_text": "\nGoing foward it's probably better to recommend only the conditional requirement https://github.com/pytest-dev/pytest-runner#conditional-requirement\nI agree - that section should at least discuss the downsides of using `pytest-runner`.  \r\n\r\nIMO we should present the manual option first, then discuss how this bypasses standard packaging tools, advise against it for libraries, and finally point to `pytest-runner` as a quick way to set this up for applications that can afford the extra dependency and may want to use `setup.py`-based testing.",
  "created_at": "2019-07-05T11:15:03Z",
  "version": "5.0",
  "FAIL_TO_PASS": "[\"testing/python/raises.py::TestRaises::test_raises_match\", \"testing/python/raises.py::TestRaises::test_match_failure_string_quoting\", \"testing/test_stepwise.py::test_xfail_handling\"]",
  "PASS_TO_PASS": "[\"testing/python/raises.py::TestRaises::test_raises\", \"testing/python/raises.py::TestRaises::test_raises_exec\", \"testing/python/raises.py::TestRaises::test_raises_exec_correct_filename\", \"testing/python/raises.py::TestRaises::test_raises_syntax_error\", \"testing/python/raises.py::TestRaises::test_raises_function\", \"testing/python/raises.py::TestRaises::test_raises_callable_no_exception\", \"testing/python/raises.py::TestRaises::test_raises_falsey_type_error\", \"testing/python/raises.py::TestRaises::test_raises_repr_inflight\", \"testing/python/raises.py::TestRaises::test_noclass\", \"testing/python/raises.py::TestRaises::test_invalid_arguments_to_raises\", \"testing/python/raises.py::TestRaises::test_tuple\", \"testing/python/raises.py::TestRaises::test_no_raise_message\", \"testing/python/raises.py::TestRaises::test_custom_raise_message\", \"testing/python/raises.py::TestRaises::test_raises_cyclic_reference[function]\", \"testing/python/raises.py::TestRaises::test_raises_cyclic_reference[with]\", \"testing/python/raises.py::TestRaises::test_raises_match_wrong_type\", \"testing/python/raises.py::TestRaises::test_raises_exception_looks_iterable\", \"testing/python/raises.py::TestRaises::test_raises_with_raising_dunder_class\", \"testing/python/raises.py::TestRaises::test_raises_as_contextmanager\", \"testing/python/raises.py::TestRaises::test_does_not_raise\", \"testing/python/raises.py::TestRaises::test_does_not_raise_does_raise\", \"testing/test_stepwise.py::test_run_without_stepwise\", \"testing/test_stepwise.py::test_fail_and_continue_with_stepwise\", \"testing/test_stepwise.py::test_run_with_skip_option\", \"testing/test_stepwise.py::test_fail_on_errors\", \"testing/test_stepwise.py::test_change_testfile\", \"testing/test_stepwise.py::test_stop_on_collection_errors[True]\", \"testing/test_stepwise.py::test_stop_on_collection_errors[False]\"]",
  "environment_setup_commit": "c2f762460f4c42547de906d53ea498dd499ea837",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.929326",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}