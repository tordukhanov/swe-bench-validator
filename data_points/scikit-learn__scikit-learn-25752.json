{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-25752",
  "base_commit": "b397b8f2d952a26344cc062ff912c663f4afa6d5",
  "patch": "diff --git a/sklearn/cluster/_bicluster.py b/sklearn/cluster/_bicluster.py\n--- a/sklearn/cluster/_bicluster.py\n+++ b/sklearn/cluster/_bicluster.py\n@@ -487,7 +487,7 @@ class SpectralBiclustering(BaseSpectral):\n     >>> clustering.row_labels_\n     array([1, 1, 1, 0, 0, 0], dtype=int32)\n     >>> clustering.column_labels_\n-    array([0, 1], dtype=int32)\n+    array([1, 0], dtype=int32)\n     >>> clustering\n     SpectralBiclustering(n_clusters=2, random_state=0)\n     \"\"\"\ndiff --git a/sklearn/cluster/_bisect_k_means.py b/sklearn/cluster/_bisect_k_means.py\n--- a/sklearn/cluster/_bisect_k_means.py\n+++ b/sklearn/cluster/_bisect_k_means.py\n@@ -190,18 +190,18 @@ class BisectingKMeans(_BaseKMeans):\n     --------\n     >>> from sklearn.cluster import BisectingKMeans\n     >>> import numpy as np\n-    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n-    ...               [10, 2], [10, 4], [10, 0],\n-    ...               [10, 6], [10, 8], [10, 10]])\n+    >>> X = np.array([[1, 1], [10, 1], [3, 1],\n+    ...               [10, 0], [2, 1], [10, 2],\n+    ...               [10, 8], [10, 9], [10, 10]])\n     >>> bisect_means = BisectingKMeans(n_clusters=3, random_state=0).fit(X)\n     >>> bisect_means.labels_\n-    array([2, 2, 2, 0, 0, 0, 1, 1, 1], dtype=int32)\n+    array([0, 2, 0, 2, 0, 2, 1, 1, 1], dtype=int32)\n     >>> bisect_means.predict([[0, 0], [12, 3]])\n-    array([2, 0], dtype=int32)\n+    array([0, 2], dtype=int32)\n     >>> bisect_means.cluster_centers_\n-    array([[10.,  2.],\n-           [10.,  8.],\n-           [ 1., 2.]])\n+    array([[ 2., 1.],\n+           [10., 9.],\n+           [10., 1.]])\n     \"\"\"\n \n     _parameter_constraints: dict = {\n@@ -309,7 +309,12 @@ def _bisect(self, X, x_squared_norms, sample_weight, cluster_to_bisect):\n         # Repeating `n_init` times to obtain best clusters\n         for _ in range(self.n_init):\n             centers_init = self._init_centroids(\n-                X, x_squared_norms, self.init, self._random_state, n_centroids=2\n+                X,\n+                x_squared_norms=x_squared_norms,\n+                init=self.init,\n+                random_state=self._random_state,\n+                n_centroids=2,\n+                sample_weight=sample_weight,\n             )\n \n             labels, inertia, centers, _ = self._kmeans_single(\n@@ -361,7 +366,8 @@ def fit(self, X, y=None, sample_weight=None):\n \n         sample_weight : array-like of shape (n_samples,), default=None\n             The weights for each observation in X. If None, all observations\n-            are assigned equal weight.\n+            are assigned equal weight. `sample_weight` is not used during\n+            initialization if `init` is a callable.\n \n         Returns\n         -------\ndiff --git a/sklearn/cluster/_kmeans.py b/sklearn/cluster/_kmeans.py\n--- a/sklearn/cluster/_kmeans.py\n+++ b/sklearn/cluster/_kmeans.py\n@@ -63,13 +63,20 @@\n     {\n         \"X\": [\"array-like\", \"sparse matrix\"],\n         \"n_clusters\": [Interval(Integral, 1, None, closed=\"left\")],\n+        \"sample_weight\": [\"array-like\", None],\n         \"x_squared_norms\": [\"array-like\", None],\n         \"random_state\": [\"random_state\"],\n         \"n_local_trials\": [Interval(Integral, 1, None, closed=\"left\"), None],\n     }\n )\n def kmeans_plusplus(\n-    X, n_clusters, *, x_squared_norms=None, random_state=None, n_local_trials=None\n+    X,\n+    n_clusters,\n+    *,\n+    sample_weight=None,\n+    x_squared_norms=None,\n+    random_state=None,\n+    n_local_trials=None,\n ):\n     \"\"\"Init n_clusters seeds according to k-means++.\n \n@@ -83,6 +90,13 @@ def kmeans_plusplus(\n     n_clusters : int\n         The number of centroids to initialize.\n \n+    sample_weight : array-like of shape (n_samples,), default=None\n+        The weights for each observation in `X`. If `None`, all observations\n+        are assigned equal weight. `sample_weight` is ignored if `init`\n+        is a callable or a user provided array.\n+\n+        .. versionadded:: 1.3\n+\n     x_squared_norms : array-like of shape (n_samples,), default=None\n         Squared Euclidean norm of each data point.\n \n@@ -125,13 +139,14 @@ def kmeans_plusplus(\n     ...               [10, 2], [10, 4], [10, 0]])\n     >>> centers, indices = kmeans_plusplus(X, n_clusters=2, random_state=0)\n     >>> centers\n-    array([[10,  4],\n+    array([[10,  2],\n            [ 1,  0]])\n     >>> indices\n-    array([4, 2])\n+    array([3, 2])\n     \"\"\"\n     # Check data\n     check_array(X, accept_sparse=\"csr\", dtype=[np.float64, np.float32])\n+    sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n \n     if X.shape[0] < n_clusters:\n         raise ValueError(\n@@ -154,13 +169,15 @@ def kmeans_plusplus(\n \n     # Call private k-means++\n     centers, indices = _kmeans_plusplus(\n-        X, n_clusters, x_squared_norms, random_state, n_local_trials\n+        X, n_clusters, x_squared_norms, sample_weight, random_state, n_local_trials\n     )\n \n     return centers, indices\n \n \n-def _kmeans_plusplus(X, n_clusters, x_squared_norms, random_state, n_local_trials=None):\n+def _kmeans_plusplus(\n+    X, n_clusters, x_squared_norms, sample_weight, random_state, n_local_trials=None\n+):\n     \"\"\"Computational component for initialization of n_clusters by\n     k-means++. Prior validation of data is assumed.\n \n@@ -172,6 +189,9 @@ def _kmeans_plusplus(X, n_clusters, x_squared_norms, random_state, n_local_trial\n     n_clusters : int\n         The number of seeds to choose.\n \n+    sample_weight : ndarray of shape (n_samples,)\n+        The weights for each observation in `X`.\n+\n     x_squared_norms : ndarray of shape (n_samples,)\n         Squared Euclidean norm of each data point.\n \n@@ -206,7 +226,7 @@ def _kmeans_plusplus(X, n_clusters, x_squared_norms, random_state, n_local_trial\n         n_local_trials = 2 + int(np.log(n_clusters))\n \n     # Pick first center randomly and track index of point\n-    center_id = random_state.randint(n_samples)\n+    center_id = random_state.choice(n_samples, p=sample_weight / sample_weight.sum())\n     indices = np.full(n_clusters, -1, dtype=int)\n     if sp.issparse(X):\n         centers[0] = X[center_id].toarray()\n@@ -218,14 +238,16 @@ def _kmeans_plusplus(X, n_clusters, x_squared_norms, random_state, n_local_trial\n     closest_dist_sq = _euclidean_distances(\n         centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms, squared=True\n     )\n-    current_pot = closest_dist_sq.sum()\n+    current_pot = closest_dist_sq @ sample_weight\n \n     # Pick the remaining n_clusters-1 points\n     for c in range(1, n_clusters):\n         # Choose center candidates by sampling with probability proportional\n         # to the squared distance to the closest existing center\n         rand_vals = random_state.uniform(size=n_local_trials) * current_pot\n-        candidate_ids = np.searchsorted(stable_cumsum(closest_dist_sq), rand_vals)\n+        candidate_ids = np.searchsorted(\n+            stable_cumsum(sample_weight * closest_dist_sq), rand_vals\n+        )\n         # XXX: numerical imprecision can result in a candidate_id out of range\n         np.clip(candidate_ids, None, closest_dist_sq.size - 1, out=candidate_ids)\n \n@@ -236,7 +258,7 @@ def _kmeans_plusplus(X, n_clusters, x_squared_norms, random_state, n_local_trial\n \n         # update closest distances squared and potential for each candidate\n         np.minimum(closest_dist_sq, distance_to_candidates, out=distance_to_candidates)\n-        candidates_pot = distance_to_candidates.sum(axis=1)\n+        candidates_pot = distance_to_candidates @ sample_weight.reshape(-1, 1)\n \n         # Decide which candidate is the best\n         best_candidate = np.argmin(candidates_pot)\n@@ -323,7 +345,8 @@ def k_means(\n \n     sample_weight : array-like of shape (n_samples,), default=None\n         The weights for each observation in `X`. If `None`, all observations\n-        are assigned equal weight.\n+        are assigned equal weight. `sample_weight` is not used during\n+        initialization if `init` is a callable or a user provided array.\n \n     init : {'k-means++', 'random'}, callable or array-like of shape \\\n             (n_clusters, n_features), default='k-means++'\n@@ -939,7 +962,14 @@ def _check_test_data(self, X):\n         return X\n \n     def _init_centroids(\n-        self, X, x_squared_norms, init, random_state, init_size=None, n_centroids=None\n+        self,\n+        X,\n+        x_squared_norms,\n+        init,\n+        random_state,\n+        init_size=None,\n+        n_centroids=None,\n+        sample_weight=None,\n     ):\n         \"\"\"Compute the initial centroids.\n \n@@ -969,6 +999,11 @@ def _init_centroids(\n             If left to 'None' the number of centroids will be equal to\n             number of clusters to form (self.n_clusters)\n \n+        sample_weight : ndarray of shape (n_samples,), default=None\n+            The weights for each observation in X. If None, all observations\n+            are assigned equal weight. `sample_weight` is not used during\n+            initialization if `init` is a callable or a user provided array.\n+\n         Returns\n         -------\n         centers : ndarray of shape (n_clusters, n_features)\n@@ -981,6 +1016,7 @@ def _init_centroids(\n             X = X[init_indices]\n             x_squared_norms = x_squared_norms[init_indices]\n             n_samples = X.shape[0]\n+            sample_weight = sample_weight[init_indices]\n \n         if isinstance(init, str) and init == \"k-means++\":\n             centers, _ = _kmeans_plusplus(\n@@ -988,9 +1024,15 @@ def _init_centroids(\n                 n_clusters,\n                 random_state=random_state,\n                 x_squared_norms=x_squared_norms,\n+                sample_weight=sample_weight,\n             )\n         elif isinstance(init, str) and init == \"random\":\n-            seeds = random_state.permutation(n_samples)[:n_clusters]\n+            seeds = random_state.choice(\n+                n_samples,\n+                size=n_clusters,\n+                replace=False,\n+                p=sample_weight / sample_weight.sum(),\n+            )\n             centers = X[seeds]\n         elif _is_arraylike_not_scalar(self.init):\n             centers = init\n@@ -1412,7 +1454,8 @@ def fit(self, X, y=None, sample_weight=None):\n \n         sample_weight : array-like of shape (n_samples,), default=None\n             The weights for each observation in X. If None, all observations\n-            are assigned equal weight.\n+            are assigned equal weight. `sample_weight` is not used during\n+            initialization if `init` is a callable or a user provided array.\n \n             .. versionadded:: 0.20\n \n@@ -1468,7 +1511,11 @@ def fit(self, X, y=None, sample_weight=None):\n         for i in range(self._n_init):\n             # Initialize centers\n             centers_init = self._init_centroids(\n-                X, x_squared_norms=x_squared_norms, init=init, random_state=random_state\n+                X,\n+                x_squared_norms=x_squared_norms,\n+                init=init,\n+                random_state=random_state,\n+                sample_weight=sample_weight,\n             )\n             if self.verbose:\n                 print(\"Initialization complete\")\n@@ -1545,7 +1592,7 @@ def _mini_batch_step(\n         Squared euclidean norm of each data point.\n \n     sample_weight : ndarray of shape (n_samples,)\n-        The weights for each observation in X.\n+        The weights for each observation in `X`.\n \n     centers : ndarray of shape (n_clusters, n_features)\n         The cluster centers before the current iteration\n@@ -1818,10 +1865,10 @@ class MiniBatchKMeans(_BaseKMeans):\n     >>> kmeans = kmeans.partial_fit(X[0:6,:])\n     >>> kmeans = kmeans.partial_fit(X[6:12,:])\n     >>> kmeans.cluster_centers_\n-    array([[2. , 1. ],\n-           [3.5, 4.5]])\n+    array([[3.375, 3.  ],\n+           [0.75 , 0.5 ]])\n     >>> kmeans.predict([[0, 0], [4, 4]])\n-    array([0, 1], dtype=int32)\n+    array([1, 0], dtype=int32)\n     >>> # fit on the whole data\n     >>> kmeans = MiniBatchKMeans(n_clusters=2,\n     ...                          random_state=0,\n@@ -1829,8 +1876,8 @@ class MiniBatchKMeans(_BaseKMeans):\n     ...                          max_iter=10,\n     ...                          n_init=\"auto\").fit(X)\n     >>> kmeans.cluster_centers_\n-    array([[3.97727273, 2.43181818],\n-           [1.125     , 1.6       ]])\n+    array([[3.55102041, 2.48979592],\n+           [1.06896552, 1.        ]])\n     >>> kmeans.predict([[0, 0], [4, 4]])\n     array([1, 0], dtype=int32)\n     \"\"\"\n@@ -2015,7 +2062,8 @@ def fit(self, X, y=None, sample_weight=None):\n \n         sample_weight : array-like of shape (n_samples,), default=None\n             The weights for each observation in X. If None, all observations\n-            are assigned equal weight.\n+            are assigned equal weight. `sample_weight` is not used during\n+            initialization if `init` is a callable or a user provided array.\n \n             .. versionadded:: 0.20\n \n@@ -2070,6 +2118,7 @@ def fit(self, X, y=None, sample_weight=None):\n                 init=init,\n                 random_state=random_state,\n                 init_size=self._init_size,\n+                sample_weight=sample_weight,\n             )\n \n             # Compute inertia on a validation set.\n@@ -2170,7 +2219,8 @@ def partial_fit(self, X, y=None, sample_weight=None):\n \n         sample_weight : array-like of shape (n_samples,), default=None\n             The weights for each observation in X. If None, all observations\n-            are assigned equal weight.\n+            are assigned equal weight. `sample_weight` is not used during\n+            initialization if `init` is a callable or a user provided array.\n \n         Returns\n         -------\n@@ -2220,6 +2270,7 @@ def partial_fit(self, X, y=None, sample_weight=None):\n                 init=init,\n                 random_state=self._random_state,\n                 init_size=self._init_size,\n+                sample_weight=sample_weight,\n             )\n \n             # Initialize counts\n",
  "test_patch": "diff --git a/sklearn/cluster/tests/test_bisect_k_means.py b/sklearn/cluster/tests/test_bisect_k_means.py\n--- a/sklearn/cluster/tests/test_bisect_k_means.py\n+++ b/sklearn/cluster/tests/test_bisect_k_means.py\n@@ -4,34 +4,33 @@\n \n from sklearn.utils._testing import assert_array_equal, assert_allclose\n from sklearn.cluster import BisectingKMeans\n+from sklearn.metrics import v_measure_score\n \n \n @pytest.mark.parametrize(\"bisecting_strategy\", [\"biggest_inertia\", \"largest_cluster\"])\n-def test_three_clusters(bisecting_strategy):\n+@pytest.mark.parametrize(\"init\", [\"k-means++\", \"random\"])\n+def test_three_clusters(bisecting_strategy, init):\n     \"\"\"Tries to perform bisect k-means for three clusters to check\n     if splitting data is performed correctly.\n     \"\"\"\n-\n-    # X = np.array([[1, 2], [1, 4], [1, 0],\n-    #               [10, 2], [10, 4], [10, 0],\n-    #               [10, 6], [10, 8], [10, 10]])\n-\n-    # X[0][1] swapped with X[1][1] intentionally for checking labeling\n     X = np.array(\n-        [[1, 2], [10, 4], [1, 0], [10, 2], [1, 4], [10, 0], [10, 6], [10, 8], [10, 10]]\n+        [[1, 1], [10, 1], [3, 1], [10, 0], [2, 1], [10, 2], [10, 8], [10, 9], [10, 10]]\n     )\n     bisect_means = BisectingKMeans(\n-        n_clusters=3, random_state=0, bisecting_strategy=bisecting_strategy\n+        n_clusters=3,\n+        random_state=0,\n+        bisecting_strategy=bisecting_strategy,\n+        init=init,\n     )\n     bisect_means.fit(X)\n \n-    expected_centers = [[10, 2], [10, 8], [1, 2]]\n-    expected_predict = [2, 0]\n-    expected_labels = [2, 0, 2, 0, 2, 0, 1, 1, 1]\n+    expected_centers = [[2, 1], [10, 1], [10, 9]]\n+    expected_labels = [0, 1, 0, 1, 0, 1, 2, 2, 2]\n \n-    assert_allclose(expected_centers, bisect_means.cluster_centers_)\n-    assert_array_equal(expected_predict, bisect_means.predict([[0, 0], [12, 3]]))\n-    assert_array_equal(expected_labels, bisect_means.labels_)\n+    assert_allclose(\n+        sorted(expected_centers), sorted(bisect_means.cluster_centers_.tolist())\n+    )\n+    assert_allclose(v_measure_score(expected_labels, bisect_means.labels_), 1.0)\n \n \n def test_sparse():\ndiff --git a/sklearn/cluster/tests/test_k_means.py b/sklearn/cluster/tests/test_k_means.py\n--- a/sklearn/cluster/tests/test_k_means.py\n+++ b/sklearn/cluster/tests/test_k_means.py\n@@ -17,6 +17,7 @@\n from sklearn.utils.extmath import row_norms\n from sklearn.metrics import pairwise_distances\n from sklearn.metrics import pairwise_distances_argmin\n+from sklearn.metrics.pairwise import euclidean_distances\n from sklearn.metrics.cluster import v_measure_score\n from sklearn.cluster import KMeans, k_means, kmeans_plusplus\n from sklearn.cluster import MiniBatchKMeans\n@@ -1276,3 +1277,67 @@ def test_predict_does_not_change_cluster_centers(is_sparse):\n \n     y_pred2 = kmeans.predict(X)\n     assert_array_equal(y_pred1, y_pred2)\n+\n+\n+@pytest.mark.parametrize(\"init\", [\"k-means++\", \"random\"])\n+def test_sample_weight_init(init, global_random_seed):\n+    \"\"\"Check that sample weight is used during init.\n+\n+    `_init_centroids` is shared across all classes inheriting from _BaseKMeans so\n+    it's enough to check for KMeans.\n+    \"\"\"\n+    rng = np.random.RandomState(global_random_seed)\n+    X, _ = make_blobs(\n+        n_samples=200, n_features=10, centers=10, random_state=global_random_seed\n+    )\n+    x_squared_norms = row_norms(X, squared=True)\n+\n+    kmeans = KMeans()\n+    clusters_weighted = kmeans._init_centroids(\n+        X=X,\n+        x_squared_norms=x_squared_norms,\n+        init=init,\n+        sample_weight=rng.uniform(size=X.shape[0]),\n+        n_centroids=5,\n+        random_state=np.random.RandomState(global_random_seed),\n+    )\n+    clusters = kmeans._init_centroids(\n+        X=X,\n+        x_squared_norms=x_squared_norms,\n+        init=init,\n+        sample_weight=np.ones(X.shape[0]),\n+        n_centroids=5,\n+        random_state=np.random.RandomState(global_random_seed),\n+    )\n+    with pytest.raises(AssertionError):\n+        assert_allclose(clusters_weighted, clusters)\n+\n+\n+@pytest.mark.parametrize(\"init\", [\"k-means++\", \"random\"])\n+def test_sample_weight_zero(init, global_random_seed):\n+    \"\"\"Check that if sample weight is 0, this sample won't be chosen.\n+\n+    `_init_centroids` is shared across all classes inheriting from _BaseKMeans so\n+    it's enough to check for KMeans.\n+    \"\"\"\n+    rng = np.random.RandomState(global_random_seed)\n+    X, _ = make_blobs(\n+        n_samples=100, n_features=5, centers=5, random_state=global_random_seed\n+    )\n+    sample_weight = rng.uniform(size=X.shape[0])\n+    sample_weight[::2] = 0\n+    x_squared_norms = row_norms(X, squared=True)\n+\n+    kmeans = KMeans()\n+    clusters_weighted = kmeans._init_centroids(\n+        X=X,\n+        x_squared_norms=x_squared_norms,\n+        init=init,\n+        sample_weight=sample_weight,\n+        n_centroids=10,\n+        random_state=np.random.RandomState(global_random_seed),\n+    )\n+    # No center should be one of the 0 sample weight point\n+    # (i.e. be at a distance=0 from it)\n+    d = euclidean_distances(X[::2], clusters_weighted)\n+    assert not np.any(np.isclose(d, 0))\ndiff --git a/sklearn/manifold/tests/test_spectral_embedding.py b/sklearn/manifold/tests/test_spectral_embedding.py\n--- a/sklearn/manifold/tests/test_spectral_embedding.py\n+++ b/sklearn/manifold/tests/test_spectral_embedding.py\n@@ -336,7 +336,7 @@ def test_pipeline_spectral_clustering(seed=36):\n         random_state=random_state,\n     )\n     for se in [se_rbf, se_knn]:\n-        km = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=\"auto\")\n+        km = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n         km.fit(se.fit_transform(S))\n         assert_array_almost_equal(\n             normalized_mutual_info_score(km.labels_, true_labels), 1.0, 2\n",
  "problem_statement": "KMeans initialization does not use sample weights\n### Describe the bug\r\n\r\nClustering by KMeans does not weight the input data.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```py\r\nimport numpy as np\r\nfrom sklearn.cluster import KMeans\r\nx = np.array([1, 1, 5, 5, 100, 100])\r\nw = 10**np.array([8.,8,8,8,-8,-8]) # large weights for 1 and 5, small weights for 100\r\nx=x.reshape(-1,1)# reshape to a 2-dimensional array requested for KMeans\r\ncenters_with_weight = KMeans(n_clusters=2, random_state=0,n_init=10).fit(x,sample_weight=w).cluster_centers_\r\ncenters_no_weight = KMeans(n_clusters=2, random_state=0,n_init=10).fit(x).cluster_centers_\r\n```\r\n\r\n### Expected Results\r\n\r\ncenters_with_weight=[[1.],[5.]]\r\ncenters_no_weight=[[100.],[3.]]\r\n\r\n### Actual Results\r\n\r\ncenters_with_weight=[[100.],[3.]]\r\ncenters_no_weight=[[100.],[3.]]\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]\r\nexecutable: E:\\WPy64-31040\\python-3.10.4.amd64\\python.exe\r\n   machine: Windows-10-10.0.19045-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.1\r\n          pip: 22.3.1\r\n   setuptools: 62.1.0\r\n        numpy: 1.23.3\r\n        scipy: 1.8.1\r\n       Cython: 0.29.28\r\n       pandas: 1.4.2\r\n   matplotlib: 3.5.1\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: E:\\WPy64-31040\\python-3.10.4.amd64\\Lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: E:\\WPy64-31040\\python-3.10.4.amd64\\Lib\\site-packages\\scipy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\r\n        version: 0.3.17\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: vcomp\r\n       filepath: E:\\WPy64-31040\\python-3.10.4.amd64\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\r\n        version: None\r\n    num_threads: 12\r\n```\r\n\n",
  "hints_text": "Thanks for the reproducible example.\r\n\r\n`KMeans` **does** weight the data, but your example is an extreme case. Because `Kmeans` is a non-convex problem, the algorithm can get stuck in a local minimum, and not find the true minimum  of the optimization landscape. This is the reason why the code proposes to use multiple initializations, hoping that some initializations will not get stuck in poor local minima.\r\n\r\nImportantly, **the initialization does not use sample weights**. So when using `init=\"k-means++\"` (default), it gets a centroid on the outliers, and `Kmeans` cannot escape this local minimum, even with strong sample weights.\r\n\r\nInstead, the optimization does not get stuck if we do one of the following changes:\r\n- the outliers are less extremes (e.g. 10 instead of 100), because it allows the sample weights to remove the local minimum\r\n- the low samples weights are exactly equal to zero, because it completely discard the outliers\r\n- using `init=\"random\"`, because some of the different init are able to avoid the local minimum\nIf we have less extreme weighting then ```init='random'``` does not work at all as shown in this example. The centers are always [[3.],[40.]] instead of circa [[1.],[5.]] for the weighted case.\r\n```py\r\nimport numpy as np\r\nfrom sklearn.cluster import KMeans\r\nx = np.array([1, 1, 5, 5, 40, 40])\r\nw = np.array([100,100,100,100,1,1]) # large weights for 1 and 5, small weights for 40\r\nx=x.reshape(-1,1)# reshape to a 2-dimensional array requested for KMeans\r\ncenters_with_weight = KMeans(n_clusters=2, random_state=0,n_init=100,init='random').fit(x,sample_weight=w).cluster_centers_\r\ncenters_no_weight = KMeans(n_clusters=2, random_state=0,n_init=100).fit(x).cluster_centers_\r\n```\nSure, you can find examples where `init=\"random\"` also fails.\r\nMy point is that `KMeans` is non-convex, so it is never guaranteed to find the global minimum.\r\n\r\nThat being said, **it would be nice to have an initialization that uses the sample weights**. I don't know about `init=\"k-means++\"`, but for `init=\"random\"` we could use a non-uniform sampling relative to the sample weights, instead of the current uniform sampling.\r\n\r\nDo you want to submit a pull-request?\nyes\nI want to work on this issue\nWas this bug already resolved?\nNot resolved yet, you are welcome to submit a pull-request if you'd like.\nI already answered this question by \"yes\" 3 weeks ago. ",
  "created_at": "2023-03-03T09:07:31Z",
  "version": "1.3",
  "FAIL_TO_PASS": "[\"sklearn/cluster/tests/test_k_means.py::test_sample_weight_init[42-k-means++]\", \"sklearn/cluster/tests/test_k_means.py::test_sample_weight_init[42-random]\", \"sklearn/cluster/tests/test_k_means.py::test_sample_weight_zero[42-k-means++]\", \"sklearn/cluster/tests/test_k_means.py::test_sample_weight_zero[42-random]\"]",
  "PASS_TO_PASS": "[\"sklearn/cluster/tests/test_bisect_k_means.py::test_three_clusters[k-means++-biggest_inertia]\", \"sklearn/cluster/tests/test_bisect_k_means.py::test_three_clusters[k-means++-largest_cluster]\", \"sklearn/cluster/tests/test_bisect_k_means.py::test_three_clusters[random-biggest_inertia]\", \"sklearn/cluster/tests/test_bisect_k_means.py::test_three_clusters[random-largest_cluster]\", \"sklearn/cluster/tests/test_bisect_k_means.py::test_sparse\", \"sklearn/cluster/tests/test_bisect_k_means.py::test_n_clusters[4]\", \"sklearn/cluster/tests/test_bisect_k_means.py::test_n_clusters[5]\", \"sklearn/cluster/tests/test_bisect_k_means.py::test_one_cluster\", \"sklearn/cluster/tests/test_bisect_k_means.py::test_fit_predict[True]\", \"sklearn/cluster/tests/test_bisect_k_means.py::test_fit_predict[False]\", \"sklearn/cluster/tests/test_bisect_k_means.py::test_dtype_preserved[float64-True]\", \"sklearn/cluster/tests/test_bisect_k_means.py::test_dtype_preserved[float64-False]\", \"sklearn/cluster/tests/test_bisect_k_means.py::test_float32_float64_equivalence[True]\", \"sklearn/cluster/tests/test_bisect_k_means.py::test_float32_float64_equivalence[False]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_results[float32-lloyd-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_results[float32-lloyd-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_results[float32-elkan-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_results[float32-elkan-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_results[float64-lloyd-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_results[float64-lloyd-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_results[float64-elkan-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_results[float64-elkan-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_relocated_clusters[lloyd-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_relocated_clusters[lloyd-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_relocated_clusters[elkan-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_relocated_clusters[elkan-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_relocate_empty_clusters[dense]\", \"sklearn/cluster/tests/test_k_means.py::test_relocate_empty_clusters[sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_elkan_results[42-0.01-dense-normal]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_elkan_results[42-0.01-dense-blobs]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_elkan_results[42-0.01-sparse-normal]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_elkan_results[42-0.01-sparse-blobs]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_elkan_results[42-1e-08-dense-normal]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_elkan_results[42-1e-08-dense-blobs]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_elkan_results[42-1e-08-sparse-normal]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_elkan_results[42-1e-08-sparse-blobs]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_elkan_results[42-1e-100-dense-normal]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_elkan_results[42-1e-100-dense-blobs]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_elkan_results[42-1e-100-sparse-normal]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_elkan_results[42-1e-100-sparse-blobs]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_elkan_results[42-0-dense-normal]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_elkan_results[42-0-dense-blobs]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_elkan_results[42-0-sparse-normal]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_elkan_results[42-0-sparse-blobs]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_convergence[42-lloyd]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_convergence[42-elkan]\", \"sklearn/cluster/tests/test_k_means.py::test_algorithm_auto_full_deprecation_warning[auto]\", \"sklearn/cluster/tests/test_k_means.py::test_algorithm_auto_full_deprecation_warning[full]\", \"sklearn/cluster/tests/test_k_means.py::test_predict_sample_weight_deprecation_warning[KMeans]\", \"sklearn/cluster/tests/test_k_means.py::test_predict_sample_weight_deprecation_warning[MiniBatchKMeans]\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_update_consistency[42]\", \"sklearn/cluster/tests/test_k_means.py::test_all_init[KMeans-random-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_all_init[KMeans-random-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_all_init[KMeans-k-means++-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_all_init[KMeans-k-means++-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_all_init[KMeans-ndarray-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_all_init[KMeans-ndarray-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_all_init[KMeans-callable-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_all_init[KMeans-callable-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_all_init[MiniBatchKMeans-random-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_all_init[MiniBatchKMeans-random-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_all_init[MiniBatchKMeans-k-means++-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_all_init[MiniBatchKMeans-k-means++-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_all_init[MiniBatchKMeans-ndarray-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_all_init[MiniBatchKMeans-ndarray-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_all_init[MiniBatchKMeans-callable-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_all_init[MiniBatchKMeans-callable-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_kmeans_partial_fit_init[random]\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_kmeans_partial_fit_init[k-means++]\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_kmeans_partial_fit_init[ndarray]\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_kmeans_partial_fit_init[callable]\", \"sklearn/cluster/tests/test_k_means.py::test_fortran_aligned_data[42-KMeans]\", \"sklearn/cluster/tests/test_k_means.py::test_fortran_aligned_data[42-MiniBatchKMeans]\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_kmeans_verbose\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_verbose[0.01-lloyd]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_verbose[0.01-elkan]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_verbose[0-lloyd]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_verbose[0-elkan]\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_kmeans_warning_init_size\", \"sklearn/cluster/tests/test_k_means.py::test_warning_n_init_precomputed_centers[KMeans]\", \"sklearn/cluster/tests/test_k_means.py::test_warning_n_init_precomputed_centers[MiniBatchKMeans]\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_sensible_reassign[42]\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_reassign[42-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_reassign[42-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_with_many_reassignments\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_kmeans_init_size\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_declared_convergence[0.0001-None]\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_declared_convergence[0-10]\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_iter_steps\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_copyx\", \"sklearn/cluster/tests/test_k_means.py::test_score_max_iter[42-KMeans]\", \"sklearn/cluster/tests/test_k_means.py::test_score_max_iter[42-MiniBatchKMeans]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_predict[float64-42-2-KMeans-lloyd-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_predict[float64-42-2-KMeans-lloyd-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_predict[float64-42-2-KMeans-elkan-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_predict[float64-42-2-KMeans-elkan-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_predict[float64-42-2-MiniBatchKMeans-None-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_predict[float64-42-2-MiniBatchKMeans-None-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_predict[float64-42-100-KMeans-lloyd-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_predict[float64-42-100-KMeans-lloyd-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_predict[float64-42-100-KMeans-elkan-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_predict[float64-42-100-KMeans-elkan-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_predict[float64-42-100-MiniBatchKMeans-None-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_predict[float64-42-100-MiniBatchKMeans-None-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_dense_sparse[42-KMeans]\", \"sklearn/cluster/tests/test_k_means.py::test_dense_sparse[42-MiniBatchKMeans]\", \"sklearn/cluster/tests/test_k_means.py::test_predict_dense_sparse[KMeans-random]\", \"sklearn/cluster/tests/test_k_means.py::test_predict_dense_sparse[KMeans-k-means++]\", \"sklearn/cluster/tests/test_k_means.py::test_predict_dense_sparse[KMeans-ndarray]\", \"sklearn/cluster/tests/test_k_means.py::test_predict_dense_sparse[MiniBatchKMeans-random]\", \"sklearn/cluster/tests/test_k_means.py::test_predict_dense_sparse[MiniBatchKMeans-k-means++]\", \"sklearn/cluster/tests/test_k_means.py::test_predict_dense_sparse[MiniBatchKMeans-ndarray]\", \"sklearn/cluster/tests/test_k_means.py::test_integer_input[42-KMeans-k-means++-int32-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_integer_input[42-KMeans-k-means++-int32-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_integer_input[42-KMeans-k-means++-int64-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_integer_input[42-KMeans-k-means++-int64-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_integer_input[42-KMeans-ndarray-int32-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_integer_input[42-KMeans-ndarray-int32-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_integer_input[42-KMeans-ndarray-int64-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_integer_input[42-KMeans-ndarray-int64-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_integer_input[42-MiniBatchKMeans-k-means++-int32-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_integer_input[42-MiniBatchKMeans-k-means++-int32-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_integer_input[42-MiniBatchKMeans-k-means++-int64-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_integer_input[42-MiniBatchKMeans-k-means++-int64-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_integer_input[42-MiniBatchKMeans-ndarray-int32-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_integer_input[42-MiniBatchKMeans-ndarray-int32-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_integer_input[42-MiniBatchKMeans-ndarray-int64-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_integer_input[42-MiniBatchKMeans-ndarray-int64-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_transform[42-KMeans]\", \"sklearn/cluster/tests/test_k_means.py::test_transform[42-MiniBatchKMeans]\", \"sklearn/cluster/tests/test_k_means.py::test_fit_transform[42-KMeans]\", \"sklearn/cluster/tests/test_k_means.py::test_fit_transform[42-MiniBatchKMeans]\", \"sklearn/cluster/tests/test_k_means.py::test_n_init[42]\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_function[42]\", \"sklearn/cluster/tests/test_k_means.py::test_float_precision[42-KMeans-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_float_precision[42-KMeans-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_float_precision[42-MiniBatchKMeans-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_float_precision[42-MiniBatchKMeans-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_centers_not_mutated[KMeans-int32]\", \"sklearn/cluster/tests/test_k_means.py::test_centers_not_mutated[KMeans-int64]\", \"sklearn/cluster/tests/test_k_means.py::test_centers_not_mutated[KMeans-float32]\", \"sklearn/cluster/tests/test_k_means.py::test_centers_not_mutated[KMeans-float64]\", \"sklearn/cluster/tests/test_k_means.py::test_centers_not_mutated[MiniBatchKMeans-int32]\", \"sklearn/cluster/tests/test_k_means.py::test_centers_not_mutated[MiniBatchKMeans-int64]\", \"sklearn/cluster/tests/test_k_means.py::test_centers_not_mutated[MiniBatchKMeans-float32]\", \"sklearn/cluster/tests/test_k_means.py::test_centers_not_mutated[MiniBatchKMeans-float64]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_init_fitted_centers[dense]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_init_fitted_centers[sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_warns_less_centers_than_unique_points[42]\", \"sklearn/cluster/tests/test_k_means.py::test_weighted_vs_repeated[42]\", \"sklearn/cluster/tests/test_k_means.py::test_unit_weights_vs_no_weights[42-KMeans-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_unit_weights_vs_no_weights[42-KMeans-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_unit_weights_vs_no_weights[42-MiniBatchKMeans-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_unit_weights_vs_no_weights[42-MiniBatchKMeans-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_scaled_weights[42-KMeans-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_scaled_weights[42-KMeans-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_scaled_weights[42-MiniBatchKMeans-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_scaled_weights[42-MiniBatchKMeans-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_elkan_iter_attribute\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_empty_cluster_relocated[dense]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_empty_cluster_relocated[sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_result_equal_in_diff_n_threads[42-KMeans]\", \"sklearn/cluster/tests/test_k_means.py::test_result_equal_in_diff_n_threads[42-MiniBatchKMeans]\", \"sklearn/cluster/tests/test_k_means.py::test_warning_elkan_1_cluster\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_1_iteration[42-lloyd-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_1_iteration[42-lloyd-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_1_iteration[42-elkan-dense]\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_1_iteration[42-elkan-sparse]\", \"sklearn/cluster/tests/test_k_means.py::test_euclidean_distance[42-True-float32]\", \"sklearn/cluster/tests/test_k_means.py::test_euclidean_distance[42-True-float64]\", \"sklearn/cluster/tests/test_k_means.py::test_euclidean_distance[42-False-float32]\", \"sklearn/cluster/tests/test_k_means.py::test_euclidean_distance[42-False-float64]\", \"sklearn/cluster/tests/test_k_means.py::test_inertia[42-float32]\", \"sklearn/cluster/tests/test_k_means.py::test_inertia[42-float64]\", \"sklearn/cluster/tests/test_k_means.py::test_change_n_init_future_warning[KMeans-10]\", \"sklearn/cluster/tests/test_k_means.py::test_change_n_init_future_warning[MiniBatchKMeans-3]\", \"sklearn/cluster/tests/test_k_means.py::test_n_init_auto[KMeans-10]\", \"sklearn/cluster/tests/test_k_means.py::test_n_init_auto[MiniBatchKMeans-3]\", \"sklearn/cluster/tests/test_k_means.py::test_sample_weight_unchanged[KMeans]\", \"sklearn/cluster/tests/test_k_means.py::test_sample_weight_unchanged[MiniBatchKMeans]\", \"sklearn/cluster/tests/test_k_means.py::test_wrong_params[param0-n_samples.*\", \"sklearn/cluster/tests/test_k_means.py::test_wrong_params[param1-The\", \"sklearn/cluster/tests/test_k_means.py::test_wrong_params[param2-The\", \"sklearn/cluster/tests/test_k_means.py::test_wrong_params[param3-The\", \"sklearn/cluster/tests/test_k_means.py::test_wrong_params[param4-The\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_plusplus_wrong_params[param0-The\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_plusplus_output[42-float64-data0]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_plusplus_output[42-float64-data1]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_plusplus_output[42-float32-data0]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_plusplus_output[42-float32-data1]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_plusplus_norms[x_squared_norms0]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_plusplus_norms[None]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_plusplus_dataorder[42]\", \"sklearn/cluster/tests/test_k_means.py::test_is_same_clustering\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_with_array_like_or_np_scalar_init[kwargs0]\", \"sklearn/cluster/tests/test_k_means.py::test_kmeans_with_array_like_or_np_scalar_init[kwargs1]\", \"sklearn/cluster/tests/test_k_means.py::test_feature_names_out[KMeans-fit]\", \"sklearn/cluster/tests/test_k_means.py::test_feature_names_out[MiniBatchKMeans-fit]\", \"sklearn/cluster/tests/test_k_means.py::test_feature_names_out[MiniBatchKMeans-partial_fit]\", \"sklearn/cluster/tests/test_k_means.py::test_predict_does_not_change_cluster_centers[True]\", \"sklearn/cluster/tests/test_k_means.py::test_predict_does_not_change_cluster_centers[False]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_sparse_graph_connected_component\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_two_components[float32-arpack]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_two_components[float32-lobpcg]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_two_components[float64-arpack]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_two_components[float64-lobpcg]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_precomputed_affinity[float32-arpack-dense]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_precomputed_affinity[float32-arpack-sparse]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_precomputed_affinity[float32-lobpcg-dense]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_precomputed_affinity[float32-lobpcg-sparse]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_precomputed_affinity[float64-arpack-dense]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_precomputed_affinity[float64-arpack-sparse]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_precomputed_affinity[float64-lobpcg-dense]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_precomputed_affinity[float64-lobpcg-sparse]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_precomputed_nearest_neighbors_filtering\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_callable_affinity[dense]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_callable_affinity[sparse]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_pipeline_spectral_clustering\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_connectivity\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_deterministic\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_unnormalized\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_first_eigen_vector\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_preserves_dtype[float32-arpack]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_preserves_dtype[float32-lobpcg]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_preserves_dtype[float64-arpack]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_preserves_dtype[float64-lobpcg]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_error_pyamg_not_available\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_eigen_tol_auto[arpack]\", \"sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_eigen_tol_auto[lobpcg]\"]",
  "environment_setup_commit": "1e8a5b833d1b58f3ab84099c4582239af854b23a",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:31.022598",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}