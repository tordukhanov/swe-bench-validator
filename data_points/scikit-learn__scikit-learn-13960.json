{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-13960",
  "base_commit": "4a6264db68b28a2e65efdecc459233911c9aee95",
  "patch": "diff --git a/sklearn/decomposition/incremental_pca.py b/sklearn/decomposition/incremental_pca.py\n--- a/sklearn/decomposition/incremental_pca.py\n+++ b/sklearn/decomposition/incremental_pca.py\n@@ -5,7 +5,7 @@\n # License: BSD 3 clause\n \n import numpy as np\n-from scipy import linalg\n+from scipy import linalg, sparse\n \n from .base import _BasePCA\n from ..utils import check_array, gen_batches\n@@ -21,11 +21,13 @@ class IncrementalPCA(_BasePCA):\n     but not scaled for each feature before applying the SVD.\n \n     Depending on the size of the input data, this algorithm can be much more\n-    memory efficient than a PCA.\n+    memory efficient than a PCA, and allows sparse input.\n \n     This algorithm has constant memory complexity, on the order\n-    of ``batch_size``, enabling use of np.memmap files without loading the\n-    entire file into memory.\n+    of ``batch_size * n_features``, enabling use of np.memmap files without\n+    loading the entire file into memory. For sparse matrices, the input\n+    is converted to dense in batches (in order to be able to subtract the\n+    mean) which avoids storing the entire dense matrix at any one time.\n \n     The computational overhead of each SVD is\n     ``O(batch_size * n_features ** 2)``, but only 2 * batch_size samples\n@@ -104,13 +106,15 @@ class IncrementalPCA(_BasePCA):\n     --------\n     >>> from sklearn.datasets import load_digits\n     >>> from sklearn.decomposition import IncrementalPCA\n+    >>> from scipy import sparse\n     >>> X, _ = load_digits(return_X_y=True)\n     >>> transformer = IncrementalPCA(n_components=7, batch_size=200)\n     >>> # either partially fit on smaller batches of data\n     >>> transformer.partial_fit(X[:100, :])\n     IncrementalPCA(batch_size=200, n_components=7)\n     >>> # or let the fit function itself divide the data into batches\n-    >>> X_transformed = transformer.fit_transform(X)\n+    >>> X_sparse = sparse.csr_matrix(X)\n+    >>> X_transformed = transformer.fit_transform(X_sparse)\n     >>> X_transformed.shape\n     (1797, 7)\n \n@@ -167,7 +171,7 @@ def fit(self, X, y=None):\n \n         Parameters\n         ----------\n-        X : array-like, shape (n_samples, n_features)\n+        X : array-like or sparse matrix, shape (n_samples, n_features)\n             Training data, where n_samples is the number of samples and\n             n_features is the number of features.\n \n@@ -188,7 +192,8 @@ def fit(self, X, y=None):\n         self.singular_values_ = None\n         self.noise_variance_ = None\n \n-        X = check_array(X, copy=self.copy, dtype=[np.float64, np.float32])\n+        X = check_array(X, accept_sparse=['csr', 'csc', 'lil'],\n+                        copy=self.copy, dtype=[np.float64, np.float32])\n         n_samples, n_features = X.shape\n \n         if self.batch_size is None:\n@@ -198,7 +203,10 @@ def fit(self, X, y=None):\n \n         for batch in gen_batches(n_samples, self.batch_size_,\n                                  min_batch_size=self.n_components or 0):\n-            self.partial_fit(X[batch], check_input=False)\n+            X_batch = X[batch]\n+            if sparse.issparse(X_batch):\n+                X_batch = X_batch.toarray()\n+            self.partial_fit(X_batch, check_input=False)\n \n         return self\n \n@@ -221,6 +229,11 @@ def partial_fit(self, X, y=None, check_input=True):\n             Returns the instance itself.\n         \"\"\"\n         if check_input:\n+            if sparse.issparse(X):\n+                raise TypeError(\n+                    \"IncrementalPCA.partial_fit does not support \"\n+                    \"sparse input. Either convert data to dense \"\n+                    \"or use IncrementalPCA.fit to do so in batches.\")\n             X = check_array(X, copy=self.copy, dtype=[np.float64, np.float32])\n         n_samples, n_features = X.shape\n         if not hasattr(self, 'components_'):\n@@ -274,7 +287,7 @@ def partial_fit(self, X, y=None, check_input=True):\n                 np.sqrt((self.n_samples_seen_ * n_samples) /\n                         n_total_samples) * (self.mean_ - col_batch_mean)\n             X = np.vstack((self.singular_values_.reshape((-1, 1)) *\n-                          self.components_, X, mean_correction))\n+                           self.components_, X, mean_correction))\n \n         U, S, V = linalg.svd(X, full_matrices=False)\n         U, V = svd_flip(U, V, u_based_decision=False)\n@@ -295,3 +308,42 @@ def partial_fit(self, X, y=None, check_input=True):\n         else:\n             self.noise_variance_ = 0.\n         return self\n+\n+    def transform(self, X):\n+        \"\"\"Apply dimensionality reduction to X.\n+\n+        X is projected on the first principal components previously extracted\n+        from a training set, using minibatches of size batch_size if X is\n+        sparse.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            New data, where n_samples is the number of samples\n+            and n_features is the number of features.\n+\n+        Returns\n+        -------\n+        X_new : array-like, shape (n_samples, n_components)\n+\n+        Examples\n+        --------\n+\n+        >>> import numpy as np\n+        >>> from sklearn.decomposition import IncrementalPCA\n+        >>> X = np.array([[-1, -1], [-2, -1], [-3, -2],\n+        ...               [1, 1], [2, 1], [3, 2]])\n+        >>> ipca = IncrementalPCA(n_components=2, batch_size=3)\n+        >>> ipca.fit(X)\n+        IncrementalPCA(batch_size=3, n_components=2)\n+        >>> ipca.transform(X) # doctest: +SKIP\n+        \"\"\"\n+        if sparse.issparse(X):\n+            n_samples = X.shape[0]\n+            output = []\n+            for batch in gen_batches(n_samples, self.batch_size_,\n+                                     min_batch_size=self.n_components or 0):\n+                output.append(super().transform(X[batch].toarray()))\n+            return np.vstack(output)\n+        else:\n+            return super().transform(X)\n",
  "test_patch": "diff --git a/sklearn/decomposition/tests/test_incremental_pca.py b/sklearn/decomposition/tests/test_incremental_pca.py\n--- a/sklearn/decomposition/tests/test_incremental_pca.py\n+++ b/sklearn/decomposition/tests/test_incremental_pca.py\n@@ -1,5 +1,6 @@\n \"\"\"Tests for Incremental PCA.\"\"\"\n import numpy as np\n+import pytest\n \n from sklearn.utils.testing import assert_almost_equal\n from sklearn.utils.testing import assert_array_almost_equal\n@@ -10,6 +11,8 @@\n from sklearn import datasets\n from sklearn.decomposition import PCA, IncrementalPCA\n \n+from scipy import sparse\n+\n iris = datasets.load_iris()\n \n \n@@ -23,17 +26,51 @@ def test_incremental_pca():\n \n     X_transformed = ipca.fit_transform(X)\n \n-    np.testing.assert_equal(X_transformed.shape, (X.shape[0], 2))\n-    assert_almost_equal(ipca.explained_variance_ratio_.sum(),\n-                        pca.explained_variance_ratio_.sum(), 1)\n+    assert X_transformed.shape == (X.shape[0], 2)\n+    np.testing.assert_allclose(ipca.explained_variance_ratio_.sum(),\n+                               pca.explained_variance_ratio_.sum(), rtol=1e-3)\n \n     for n_components in [1, 2, X.shape[1]]:\n         ipca = IncrementalPCA(n_components, batch_size=batch_size)\n         ipca.fit(X)\n         cov = ipca.get_covariance()\n         precision = ipca.get_precision()\n-        assert_array_almost_equal(np.dot(cov, precision),\n-                                  np.eye(X.shape[1]))\n+        np.testing.assert_allclose(np.dot(cov, precision),\n+                                   np.eye(X.shape[1]), atol=1e-13)\n+\n+\n+@pytest.mark.parametrize(\n+    \"matrix_class\",\n+    [sparse.csc_matrix, sparse.csr_matrix, sparse.lil_matrix])\n+def test_incremental_pca_sparse(matrix_class):\n+    # Incremental PCA on sparse arrays.\n+    X = iris.data\n+    pca = PCA(n_components=2)\n+    pca.fit_transform(X)\n+    X_sparse = matrix_class(X)\n+    batch_size = X_sparse.shape[0] // 3\n+    ipca = IncrementalPCA(n_components=2, batch_size=batch_size)\n+\n+    X_transformed = ipca.fit_transform(X_sparse)\n+\n+    assert X_transformed.shape == (X_sparse.shape[0], 2)\n+    np.testing.assert_allclose(ipca.explained_variance_ratio_.sum(),\n+                               pca.explained_variance_ratio_.sum(), rtol=1e-3)\n+\n+    for n_components in [1, 2, X.shape[1]]:\n+        ipca = IncrementalPCA(n_components, batch_size=batch_size)\n+        ipca.fit(X_sparse)\n+        cov = ipca.get_covariance()\n+        precision = ipca.get_precision()\n+        np.testing.assert_allclose(np.dot(cov, precision),\n+                                   np.eye(X_sparse.shape[1]), atol=1e-13)\n+\n+    with pytest.raises(\n+            TypeError,\n+            match=\"IncrementalPCA.partial_fit does not support \"\n+            \"sparse input. Either convert data to dense \"\n+            \"or use IncrementalPCA.fit to do so in batches.\"):\n+        ipca.partial_fit(X_sparse)\n \n \n def test_incremental_pca_check_projection():\n",
  "problem_statement": "IncrementalPCA should accept sparse input\n<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn\r\n- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn\r\nFor more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n`IncrementalPCA` is by design suited to application to sparse data in a way that most PCA classes are not. However, it is not written to accept this by default.\r\n\r\n#### Steps/Code to Reproduce\r\n```\r\nimport numpy as np\r\nfrom sklearn.decomposition import IncrementalPCA\r\nfrom scipy import sparse\r\n\r\npca_op = IncrementalPCA(batch_size=10)\r\nX = np.random.poisson(0.2, [100, 100])\r\nfor m in [sparse.csc_matrix, sparse.csr_matrix, sparse.dok_matrix, sparse.lil_matrix]:\r\n    pca_op.fit_transform(m(X))\r\n```\r\n\r\n#### Expected Results\r\nNo error should be thrown.\r\n\r\n#### Actual Results\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/home/scottgigante/.local/lib/python3.5/site-packages/sklearn/base.py\", line 464, in fit_transform\r\n    return self.fit(X, **fit_params).transform(X)\r\n  File \"/home/scottgigante/.local/lib/python3.5/site-packages/sklearn/decomposition/incremental_pca.py\", line 191, in fit\r\n    X = check_array(X, copy=self.copy, dtype=[np.float64, np.float32])\r\n  File \"/home/scottgigante/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\", line 517, in check_array\r\n    accept_large_sparse=accept_large_sparse)\r\n  File \"/home/scottgigante/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\", line 318, in _ensure_sparse_format\r\n    raise TypeError('A sparse matrix was passed, but dense '\r\nTypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\r\n```\r\n\r\n#### Suggested fix\r\n```\r\nimport numpy as np\r\nfrom sklearn.decomposition import IncrementalPCA\r\nfrom sklearn.utils import check_array, gen_batches\r\nfrom scipy import sparse\r\n\r\n\r\nclass IncrementalPCA(IncrementalPCA):\r\n\r\n    def fit(self, X, y=None):\r\n        self.components_ = None\r\n        self.n_samples_seen_ = 0\r\n        self.mean_ = .0\r\n        self.var_ = .0\r\n        self.singular_values_ = None\r\n        self.explained_variance_ = None\r\n        self.explained_variance_ratio_ = None\r\n        self.singular_values_ = None\r\n        self.noise_variance_ = None\r\n        X = check_array(X, accept_sparse=['csr', 'csc', 'dok', 'lil'], copy=self.copy, dtype=[np.float64, np.float32])\r\n        n_samples, n_features = X.shape\r\n        \r\n        if self.batch_size is None:\r\n            self.batch_size_ = 5 * n_features\r\n        else:\r\n            self.batch_size_ = self.batch_size\r\n        for batch in gen_batches(n_samples, self.batch_size_,\r\n                                 min_batch_size=self.n_components or 0):\r\n            self.partial_fit(X[batch], check_input=True)\r\n        return self\r\n\r\n    def partial_fit(self, X, y=None, check_input=True):\r\n        if check_input and sparse.issparse(X):\r\n                X = X.toarray()\r\n        super().partial_fit(X, y=y, check_input=check_input)\r\n\r\n    def transform(self, X):\r\n        n_samples = X.shape[0]\r\n        output = []\r\n        for batch in gen_batches(n_samples, self.batch_size_,\r\n                                 min_batch_size=self.n_components or 0):\r\n            X_batch = X[batch]\r\n            if sparse.issparse(X_batch):\r\n                X_batch = X_batch.toarray()\r\n            output.append(super().transform(X_batch))\r\n        return np.vstack(output)\r\n\r\n\r\npca_op = IncrementalPCA(batch_size=10)\r\nX = np.random.poisson(0.2, [100, 100])\r\nfor m in [sparse.csc_matrix, sparse.csr_matrix, sparse.dok_matrix, sparse.lil_matrix]:\r\n    pca_op.fit_transform(m(X))\r\n```\r\n\r\nI'd be happy to submit this as a PR if it's desirable.\r\n\r\n#### Versions\r\n\r\n<details>\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/home/scottgigante/.local/lib/python3.5/site-packages/sklearn/base.py\", line 464, in fit_transform\r\n    return self.fit(X, **fit_params).transform(X)\r\n  File \"/home/scottgigante/.local/lib/python3.5/site-packages/sklearn/decomposition/incremental_pca.py\", line 191, in fit\r\n    X = check_array(X, copy=self.copy, dtype=[np.float64, np.float32])\r\n  File \"/home/scottgigante/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\", line 517, in check_array\r\n    accept_large_sparse=accept_large_sparse)\r\n  File \"/home/scottgigante/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\", line 318, in _ensure_sparse_format\r\n    raise TypeError('A sparse matrix was passed, but dense '\r\nTypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\r\n>>> import sklearn; sklearn.show_versions()\r\n/home/scottgigante/.local/lib/python3.5/site-packages/numpy/distutils/system_info.py:638: UserWarning:\r\n    Atlas (http://math-atlas.sourceforge.net/) libraries not found.\r\n    Directories to search for the libraries can be specified in the\r\n    numpy/distutils/site.cfg file (section [atlas]) or by setting\r\n    the ATLAS environment variable.\r\n  self.calc_info()\r\n/usr/bin/ld: cannot find -lcblas\r\ncollect2: error: ld returned 1 exit status\r\n\r\nSystem:\r\nexecutable: /usr/bin/python3\r\n    python: 3.5.2 (default, Nov 23 2017, 16:37:01)  [GCC 5.4.0 20160609]\r\n   machine: Linux-4.4.0-17134-Microsoft-x86_64-with-Ubuntu-16.04-xenial\r\n\r\nBLAS:\r\ncblas_libs: cblas\r\n  lib_dirs: /usr/lib\r\n    macros: NO_ATLAS_INFO=1, HAVE_CBLAS=None\r\n\r\nPython deps:\r\n     scipy: 1.2.1\r\n    pandas: 0.23.4\r\n       pip: 19.0.3\r\n     numpy: 1.16.2\r\n    Cython: None\r\n   sklearn: 0.20.3\r\nsetuptools: 40.8.0\r\n```\r\n\r\n</details>\r\n<!-- Thanks for contributing! -->\r\n\n",
  "hints_text": "Yeah feel free to open a PR.",
  "created_at": "2019-05-27T23:17:57Z",
  "version": "0.22",
  "FAIL_TO_PASS": "[\"sklearn/decomposition/tests/test_incremental_pca.py::test_incremental_pca_sparse[csc_matrix]\", \"sklearn/decomposition/tests/test_incremental_pca.py::test_incremental_pca_sparse[csr_matrix]\", \"sklearn/decomposition/tests/test_incremental_pca.py::test_incremental_pca_sparse[lil_matrix]\"]",
  "PASS_TO_PASS": "[\"sklearn/decomposition/tests/test_incremental_pca.py::test_incremental_pca\", \"sklearn/decomposition/tests/test_incremental_pca.py::test_incremental_pca_check_projection\", \"sklearn/decomposition/tests/test_incremental_pca.py::test_incremental_pca_inverse\", \"sklearn/decomposition/tests/test_incremental_pca.py::test_incremental_pca_validation\", \"sklearn/decomposition/tests/test_incremental_pca.py::test_n_components_none\", \"sklearn/decomposition/tests/test_incremental_pca.py::test_incremental_pca_set_params\", \"sklearn/decomposition/tests/test_incremental_pca.py::test_incremental_pca_num_features_change\", \"sklearn/decomposition/tests/test_incremental_pca.py::test_incremental_pca_batch_signs\", \"sklearn/decomposition/tests/test_incremental_pca.py::test_incremental_pca_batch_values\", \"sklearn/decomposition/tests/test_incremental_pca.py::test_incremental_pca_batch_rank\", \"sklearn/decomposition/tests/test_incremental_pca.py::test_incremental_pca_partial_fit\", \"sklearn/decomposition/tests/test_incremental_pca.py::test_incremental_pca_against_pca_iris\", \"sklearn/decomposition/tests/test_incremental_pca.py::test_incremental_pca_against_pca_random_data\", \"sklearn/decomposition/tests/test_incremental_pca.py::test_explained_variances\", \"sklearn/decomposition/tests/test_incremental_pca.py::test_singular_values\", \"sklearn/decomposition/tests/test_incremental_pca.py::test_whitening\", \"sklearn/decomposition/tests/test_incremental_pca.py::test_incremental_pca_partial_fit_float_division\"]",
  "environment_setup_commit": "7e85a6d1f038bbb932b36f18d75df6be937ed00d",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:31.000530",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}