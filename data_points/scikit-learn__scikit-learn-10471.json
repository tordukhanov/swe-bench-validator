{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-10471",
  "base_commit": "52aaf8269235d4965022b8ec970243bdcb59c9a7",
  "patch": "diff --git a/sklearn/cluster/k_means_.py b/sklearn/cluster/k_means_.py\n--- a/sklearn/cluster/k_means_.py\n+++ b/sklearn/cluster/k_means_.py\n@@ -22,6 +22,7 @@\n from ..utils.extmath import row_norms, squared_norm, stable_cumsum\n from ..utils.sparsefuncs_fast import assign_rows_csr\n from ..utils.sparsefuncs import mean_variance_axis\n+from ..utils.validation import _num_samples\n from ..utils import check_array\n from ..utils import check_random_state\n from ..utils import as_float_array\n@@ -175,7 +176,9 @@ def k_means(X, n_clusters, init='k-means++', precompute_distances='auto',\n     Parameters\n     ----------\n     X : array-like or sparse matrix, shape (n_samples, n_features)\n-        The observations to cluster.\n+        The observations to cluster. It must be noted that the data\n+        will be converted to C ordering, which will cause a memory copy\n+        if the given data is not C-contiguous.\n \n     n_clusters : int\n         The number of clusters to form as well as the number of\n@@ -230,10 +233,12 @@ def k_means(X, n_clusters, init='k-means++', precompute_distances='auto',\n \n     copy_x : boolean, optional\n         When pre-computing distances it is more numerically accurate to center\n-        the data first.  If copy_x is True, then the original data is not\n-        modified.  If False, the original data is modified, and put back before\n-        the function returns, but small numerical differences may be introduced\n-        by subtracting and then adding the data mean.\n+        the data first.  If copy_x is True (default), then the original data is\n+        not modified, ensuring X is C-contiguous.  If False, the original data\n+        is modified, and put back before the function returns, but small\n+        numerical differences may be introduced by subtracting and then adding\n+        the data mean, in this case it will also not ensure that data is\n+        C-contiguous which may cause a significant slowdown.\n \n     n_jobs : int\n         The number of jobs to use for the computation. This works by computing\n@@ -280,7 +285,14 @@ def k_means(X, n_clusters, init='k-means++', precompute_distances='auto',\n         raise ValueError('Number of iterations should be a positive number,'\n                          ' got %d instead' % max_iter)\n \n-    X = as_float_array(X, copy=copy_x)\n+    # avoid forcing order when copy_x=False\n+    order = \"C\" if copy_x else None\n+    X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n+                    order=order, copy=copy_x)\n+    # verify that the number of samples given is larger than k\n+    if _num_samples(X) < n_clusters:\n+        raise ValueError(\"n_samples=%d should be >= n_clusters=%d\" % (\n+            _num_samples(X), n_clusters))\n     tol = _tolerance(X, tol)\n \n     # If the distances are precomputed every job will create a matrix of shape\n@@ -392,8 +404,7 @@ def _kmeans_single_elkan(X, n_clusters, max_iter=300, init='k-means++',\n                          random_state=None, tol=1e-4,\n                          precompute_distances=True):\n     if sp.issparse(X):\n-        raise ValueError(\"algorithm='elkan' not supported for sparse input X\")\n-    X = check_array(X, order=\"C\")\n+        raise TypeError(\"algorithm='elkan' not supported for sparse input X\")\n     random_state = check_random_state(random_state)\n     if x_squared_norms is None:\n         x_squared_norms = row_norms(X, squared=True)\n@@ -768,12 +779,14 @@ class KMeans(BaseEstimator, ClusterMixin, TransformerMixin):\n         If None, the random number generator is the RandomState instance used\n         by `np.random`.\n \n-    copy_x : boolean, default True\n+    copy_x : boolean, optional\n         When pre-computing distances it is more numerically accurate to center\n-        the data first.  If copy_x is True, then the original data is not\n-        modified.  If False, the original data is modified, and put back before\n-        the function returns, but small numerical differences may be introduced\n-        by subtracting and then adding the data mean.\n+        the data first.  If copy_x is True (default), then the original data is\n+        not modified, ensuring X is C-contiguous.  If False, the original data\n+        is modified, and put back before the function returns, but small\n+        numerical differences may be introduced by subtracting and then adding\n+        the data mean, in this case it will also not ensure that data is\n+        C-contiguous which may cause a significant slowdown.\n \n     n_jobs : int\n         The number of jobs to use for the computation. This works by computing\n@@ -860,14 +873,6 @@ def __init__(self, n_clusters=8, init='k-means++', n_init=10,\n         self.n_jobs = n_jobs\n         self.algorithm = algorithm\n \n-    def _check_fit_data(self, X):\n-        \"\"\"Verify that the number of samples given is larger than k\"\"\"\n-        X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32])\n-        if X.shape[0] < self.n_clusters:\n-            raise ValueError(\"n_samples=%d should be >= n_clusters=%d\" % (\n-                X.shape[0], self.n_clusters))\n-        return X\n-\n     def _check_test_data(self, X):\n         X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES)\n         n_samples, n_features = X.shape\n@@ -885,13 +890,14 @@ def fit(self, X, y=None):\n         Parameters\n         ----------\n         X : array-like or sparse matrix, shape=(n_samples, n_features)\n-            Training instances to cluster.\n+            Training instances to cluster. It must be noted that the data\n+            will be converted to C ordering, which will cause a memory\n+            copy if the given data is not C-contiguous.\n \n         y : Ignored\n \n         \"\"\"\n         random_state = check_random_state(self.random_state)\n-        X = self._check_fit_data(X)\n \n         self.cluster_centers_, self.labels_, self.inertia_, self.n_iter_ = \\\n             k_means(\n@@ -944,7 +950,6 @@ def fit_transform(self, X, y=None):\n         # np.array or CSR format already.\n         # XXX This skips _check_test_data, which may change the dtype;\n         # we should refactor the input validation.\n-        X = self._check_fit_data(X)\n         return self.fit(X)._transform(X)\n \n     def transform(self, X):\n@@ -1351,7 +1356,9 @@ def fit(self, X, y=None):\n         Parameters\n         ----------\n         X : array-like or sparse matrix, shape=(n_samples, n_features)\n-            Training instances to cluster.\n+            Training instances to cluster. It must be noted that the data\n+            will be converted to C ordering, which will cause a memory copy\n+            if the given data is not C-contiguous.\n \n         y : Ignored\n \n@@ -1361,8 +1368,8 @@ def fit(self, X, y=None):\n                         dtype=[np.float64, np.float32])\n         n_samples, n_features = X.shape\n         if n_samples < self.n_clusters:\n-            raise ValueError(\"Number of samples smaller than number \"\n-                             \"of clusters.\")\n+            raise ValueError(\"n_samples=%d should be >= n_clusters=%d\"\n+                             % (n_samples, self.n_clusters))\n \n         n_init = self.n_init\n         if hasattr(self.init, '__array__'):\n@@ -1516,13 +1523,14 @@ def partial_fit(self, X, y=None):\n         Parameters\n         ----------\n         X : array-like, shape = [n_samples, n_features]\n-            Coordinates of the data points to cluster.\n+            Coordinates of the data points to cluster. It must be noted that\n+            X will be copied if it is not C-contiguous.\n \n         y : Ignored\n \n         \"\"\"\n \n-        X = check_array(X, accept_sparse=\"csr\")\n+        X = check_array(X, accept_sparse=\"csr\", order=\"C\")\n         n_samples, n_features = X.shape\n         if hasattr(self.init, '__array__'):\n             self.init = np.ascontiguousarray(self.init, dtype=X.dtype)\n",
  "test_patch": "diff --git a/sklearn/cluster/tests/test_k_means.py b/sklearn/cluster/tests/test_k_means.py\n--- a/sklearn/cluster/tests/test_k_means.py\n+++ b/sklearn/cluster/tests/test_k_means.py\n@@ -169,7 +169,8 @@ def _check_fitted_model(km):\n     assert_greater(km.inertia_, 0.0)\n \n     # check error on dataset being too small\n-    assert_raises(ValueError, km.fit, [[0., 1.]])\n+    assert_raise_message(ValueError, \"n_samples=1 should be >= n_clusters=%d\"\n+                         % km.n_clusters, km.fit, [[0., 1.]])\n \n \n def test_k_means_plus_plus_init():\n@@ -750,6 +751,11 @@ def test_k_means_function():\n     # to many clusters desired\n     assert_raises(ValueError, k_means, X, n_clusters=X.shape[0] + 1)\n \n+    # kmeans for algorithm='elkan' raises TypeError on sparse matrix\n+    assert_raise_message(TypeError, \"algorithm='elkan' not supported for \"\n+                         \"sparse input X\", k_means, X=X_csr, n_clusters=2,\n+                         algorithm=\"elkan\")\n+\n \n def test_x_squared_norms_init_centroids():\n     \"\"\"Test that x_squared_norms can be None in _init_centroids\"\"\"\n",
  "problem_statement": "KMeans optimisation for array C/F contiguity (was: Make sure that the output of PCA.fit_transform is C contiguous)\notherwise, I would rather use:\n\n``` Python\npca.fit(X)\nX_new = pca.transform(X)\n```\n\nBecause of FORTRAN data for inner product is very slow. (for example, in the KMeans)\n\n",
  "hints_text": "Can you give an example?\nMaybe we should then rather change the behavior in KMeans.\nI wouldn't change the output format, since we don't know what the user wants to do next.\n\nThey should be the same that the output and input format of PCA.fit_transform, isn't it?\nBecause PCA.transform is such.\n\nIn _k_means._assign_labels_array, the ddot will be very slow, when X is Fortran data.\n\n``` Python\n    for sample_idx in range(n_samples):\n        min_dist = -1\n        for center_idx in range(n_clusters):\n            dist = 0.0\n            # hardcoded: minimize euclidean distance to cluster center:\n            # ||a - b||^2 = ||a||^2 + ||b||^2 -2 <a, b>\n            dist += ddot(n_features, &X[sample_idx, 0], x_stride,\n                         &centers[center_idx, 0], center_stride)\n```\n\nI have a large sample set, before the dimension reduction, each iteration only need a little more than a minute, but need to be 7 minute after dimension reduction.\n\n2 important questions:\n- How general is the requirement to be C contiguous? It speeds up for\n  k-means, but will it not slow down for other algorithms?\n- Can the change be made in k-means rather than in PCA? If you are not\n  using copy_X=False, I don't see any argument for not making the change\n  in k-means, and I would much prefer it.\n\n- I just think it is a natural idea that the features of a sample are stored in a row.\n- I don't know how to speeds up for k-means, in the case of using Fortran data. :(\n- copy_X=False. This reduces the memory usage, but without speeds up for k-means. Anyway I like it too.\n\n>   • I just think it is a natural idea that the features of a sample are stored in a row.\n\nI think that you are assuming too much.\n\n>   • I don't know how to speeds up for k-means, in the case of using Fortran\n>     data. :(\n\nIt's just a question of calling \"np.ascontiguousarray\" on the array when\nmaking the copy.\n\n>   • copy_X=False. This reduces the memory usage,\n\nOK. We would have to do a copy here.\n\nI am opposed to changing the behavior of PCA without a more exhausive\nreview of what it implies speedwise in the codebase: what algorithms\nperform best with C or Fortran ordered data.\n\nOk, I agree with you, and thank you for your explanation.\n\n@zhaipro I think it would be appreciated if you could benchmark k-means with different memory layouts and see how it performs. We could change the memory layout there, which would solve your problem. Btw, you might be interested in this PR #2008 which will make k-means quite a bit faster.\n\nbenchmark:\nC order time: 1.370000 inertia: 422652.759578\nF order time: 6.536000 inertia: 422652.759578\n\nI have a large sample set, so that the `precompute_distances = False`, \nby the following code in k_means:\n\n``` Python\nif precompute_distances == 'auto':\n    n_samples = X.shape[0]\n    precompute_distances = (n_clusters * n_samples) < 12e6\n```\n\nHere, To show my problem, I set directly `precompute_distances = False`.\n\n``` Python\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom time import time\n\ndef bench_kmeans(name, data):\n    start = time()\n    km = KMeans(n_clusters=200, init='random', n_init=1, max_iter=1,\n                copy_x=False, random_state=42, precompute_distances=False).fit(data)\n    print(\"%s time: %f inertia: %f\" % (name, time() - start, km.inertia_))\n\n\nnp.random.seed(0)\ndata = np.random.random(3000*1000).reshape((3000, 1000))\n# for C order\nbench_kmeans(name='C order', data=data)\n# for F order\ndata = np.asfortranarray(data)\nbench_kmeans(name='F order', data=data)\n```\n\nIs that true for slim and fat data and different number of clusters? Also, have you tried the elkan branch?\n\nCool, for elkan alg.\n@amueller However, it requires more memory?\n\n```\n('shape:', (3000, 1000))\nn_clusters  alg name    time    inertia:\n50  lloyd   C order 0.426000    453724.729456\n50  lloyd   F order 1.782000    453724.729456\n50  elkan   C order 0.588000    244227.752545\n50  elkan   F order 0.652000    244227.752545\n100 lloyd   C order 0.744000    442351.514140\n100 lloyd   F order 3.488000    442351.514140\n100 elkan   C order 0.892000    239220.149899\n100 elkan   F order 0.942000    239220.149899\n200 lloyd   C order 1.401000    422652.759578\n200 lloyd   F order 6.694000    422652.759578\n200 elkan   C order 1.608000    229660.875075\n200 elkan   F order 1.632000    229660.875075\n('shape:', (1000, 3000))\nn_clusters  alg name    time    inertia:\n50  lloyd   C order 0.510000    453021.642152\n50  lloyd   F order 1.416000    453021.642152\n50  elkan   C order 0.630000    236247.135296\n50  elkan   F order 0.692000    236247.135296\n100 lloyd   C order 1.042000    427321.624518\n100 lloyd   F order 2.580000    427321.624518\n100 elkan   C order 1.152000    222766.273428\n100 elkan   F order 1.212000    222766.273428\n200 lloyd   C order 1.908000    378199.959299\n200 lloyd   F order 5.046000    378199.959299\n200 elkan   C order 1.964000    196655.300444\n200 elkan   F order 2.069000    196655.300444\n```\n\n``` Python\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom time import time\nfrom itertools import product\n\ndef bench_kmeans(name, data, alg, n_clusters):\n    start = time()\n    km = KMeans(algorithm=alg, n_clusters=n_clusters, init='random', n_init=1, max_iter=1,\n                copy_x=False, random_state=42, precompute_distances=False).fit(data)\n    print(\"%d\\t%s\\t%s\\t%f\\t%f\" % (n_clusters, alg, name, time() - start, km.inertia_))\n\n\ndef test_kmeans(data):\n    c_data = data\n    f_data = np.asfortranarray(data)\n    print('n_clusters\\talg\\tname\\ttime\\tinertia:')\n    for n_clusters, alg, (name, data) in product((50, 100, 200),\n                                    ('lloyd', 'elkan'),\n                                     zip(('C order', 'F order'), (c_data, f_data))):\n        bench_kmeans(name=name, data=data, alg=alg, n_clusters=n_clusters)\n\nnp.random.seed(0)\ndata = np.random.random(3000*1000).reshape((3000, 1000))\nprint('shape:', data.shape)\ntest_kmeans(data)\ndata = data.reshape((1000, 3000))\nprint('shape:', data.shape)\ntest_kmeans(data)\n```\n\nThanks a lot for the benchmark.\nI'm quite surprised at the difference in inertia between the two implementations... maybe I messed up the stopping criterion again?\nThe memory requirement for elkan shouldn't be substantial.\n\nIt looks to me like for lloyd (the current implementation) we should `copy(\"C\")` by default (or rather do that in `check_array`\nI'm not sure if we should make this optional or add a parameter. I think I'd actually not add a parameter but just add to the docs \"the data will be converted to C ordering, which might cause a memory copy if the data is given in fortran order\" something like that.\n\n> I'm quite surprised at the difference in inertia between the two implementations...\n\n@amueller I think maybe the reason is that I have only one iteration.\n\nmakes sense.\n\nCan someone give me a little explanation of what needs to be done for this issue? Since it seems like the issue description (of title was later updated), perhaps the issue description was not updated.\r\n\r\nIn particular, there is term ['C contiguous'](https://www.ibm.com/support/knowledgecenter/SSAT4T_14.1.0/com.ibm.xlf141.linux.doc/language_ref/contiguous.html) (probably right link) what does that refer, is that fortran term? machine learning term? or [numpy.ascountiguous](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.ascontiguousarray.html).\r\n\r\nAlso if a summary of issue discussion can be given it had be great (since following the issue discussion is a little confusing for me). I'm assuming that this issue needs work, even though the PR https://github.com/scikit-learn/scikit-learn/pull/5414 was merged, which seems to be for accelerating K means algorithm.\nSummary: KMeans, when using check_array, should declare order='C'.\r\n\r\nSee https://docs.scipy.org/doc/numpy-1.12.0/glossary.html#term-row-major",
  "created_at": "2018-01-14T11:20:46Z",
  "version": "0.20",
  "FAIL_TO_PASS": "[\"sklearn/cluster/tests/test_k_means.py::test_mb_k_means_plus_plus_init_dense_array\", \"sklearn/cluster/tests/test_k_means.py::test_mb_k_means_plus_plus_init_sparse_matrix\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_k_means_random_init_dense_array\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_k_means_random_init_sparse_csr\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_k_means_perfect_init_dense_array\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_k_means_perfect_init_sparse_csr\", \"sklearn/cluster/tests/test_k_means.py::test_sparse_mb_k_means_callable_init\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_default_init_size\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_tol\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_set_init_size\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_function\"]",
  "PASS_TO_PASS": "[\"sklearn/cluster/tests/test_k_means.py::test_elkan_results\", \"sklearn/cluster/tests/test_k_means.py::test_labels_assignment_and_inertia\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_update_consistency\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_plus_plus_init\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_new_centers\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_plus_plus_init_2_jobs\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_precompute_distances_flag\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_plus_plus_init_sparse\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_random_init\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_random_init_sparse\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_plus_plus_init_not_precomputed\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_random_init_not_precomputed\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_perfect_init\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_n_init\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_explicit_init_shape\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_fortran_aligned_data\", \"sklearn/cluster/tests/test_k_means.py::test_mb_kmeans_verbose\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_init_with_large_k\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_k_means_init_multiple_runs_with_explicit_centers\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_sensible_reassign_fit\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_sensible_reassign_partial_fit\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_reassign\", \"sklearn/cluster/tests/test_k_means.py::test_minibatch_with_many_reassignments\", \"sklearn/cluster/tests/test_k_means.py::test_mini_batch_k_means_random_init_partial_fit\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_invalid_init\", \"sklearn/cluster/tests/test_k_means.py::test_mini_match_k_means_invalid_init\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_copyx\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_non_collapsed\", \"sklearn/cluster/tests/test_k_means.py::test_predict\", \"sklearn/cluster/tests/test_k_means.py::test_score\", \"sklearn/cluster/tests/test_k_means.py::test_predict_minibatch_dense_input\", \"sklearn/cluster/tests/test_k_means.py::test_predict_minibatch_kmeanspp_init_sparse_input\", \"sklearn/cluster/tests/test_k_means.py::test_predict_minibatch_random_init_sparse_input\", \"sklearn/cluster/tests/test_k_means.py::test_int_input\", \"sklearn/cluster/tests/test_k_means.py::test_transform\", \"sklearn/cluster/tests/test_k_means.py::test_fit_transform\", \"sklearn/cluster/tests/test_k_means.py::test_predict_equal_labels\", \"sklearn/cluster/tests/test_k_means.py::test_full_vs_elkan\", \"sklearn/cluster/tests/test_k_means.py::test_n_init\", \"sklearn/cluster/tests/test_k_means.py::test_x_squared_norms_init_centroids\", \"sklearn/cluster/tests/test_k_means.py::test_max_iter_error\", \"sklearn/cluster/tests/test_k_means.py::test_float_precision\", \"sklearn/cluster/tests/test_k_means.py::test_k_means_init_centers\", \"sklearn/cluster/tests/test_k_means.py::test_sparse_k_means_init_centers\", \"sklearn/cluster/tests/test_k_means.py::test_sparse_validate_centers\", \"sklearn/cluster/tests/test_k_means.py::test_less_centers_than_unique_points\"]",
  "environment_setup_commit": "55bf5d93e5674f13a1134d93a11fd0cd11aabcd1",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.951827",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}