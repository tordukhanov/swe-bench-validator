{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-9274",
  "base_commit": "faa940608befaeca99db501609c6db796739f30f",
  "patch": "diff --git a/sklearn/neural_network/multilayer_perceptron.py b/sklearn/neural_network/multilayer_perceptron.py\n--- a/sklearn/neural_network/multilayer_perceptron.py\n+++ b/sklearn/neural_network/multilayer_perceptron.py\n@@ -51,7 +51,7 @@ def __init__(self, hidden_layer_sizes, activation, solver,\n                  max_iter, loss, shuffle, random_state, tol, verbose,\n                  warm_start, momentum, nesterovs_momentum, early_stopping,\n                  validation_fraction, beta_1, beta_2, epsilon,\n-                 n_iter_no_change):\n+                 n_iter_no_change, max_fun):\n         self.activation = activation\n         self.solver = solver\n         self.alpha = alpha\n@@ -75,6 +75,7 @@ def __init__(self, hidden_layer_sizes, activation, solver,\n         self.beta_2 = beta_2\n         self.epsilon = epsilon\n         self.n_iter_no_change = n_iter_no_change\n+        self.max_fun = max_fun\n \n     def _unpack(self, packed_parameters):\n         \"\"\"Extract the coefficients and intercepts from packed_parameters.\"\"\"\n@@ -172,7 +173,6 @@ def _loss_grad_lbfgs(self, packed_coef_inter, X, y, activations, deltas,\n         self._unpack(packed_coef_inter)\n         loss, coef_grads, intercept_grads = self._backprop(\n             X, y, activations, deltas, coef_grads, intercept_grads)\n-        self.n_iter_ += 1\n         grad = _pack(coef_grads, intercept_grads)\n         return loss, grad\n \n@@ -381,6 +381,8 @@ def _validate_hyperparameters(self):\n                              self.shuffle)\n         if self.max_iter <= 0:\n             raise ValueError(\"max_iter must be > 0, got %s.\" % self.max_iter)\n+        if self.max_fun <= 0:\n+            raise ValueError(\"max_fun must be > 0, got %s.\" % self.max_fun)\n         if self.alpha < 0.0:\n             raise ValueError(\"alpha must be >= 0, got %s.\" % self.alpha)\n         if (self.learning_rate in [\"constant\", \"invscaling\", \"adaptive\"] and\n@@ -459,10 +461,29 @@ def _fit_lbfgs(self, X, y, activations, deltas, coef_grads,\n         optimal_parameters, self.loss_, d = fmin_l_bfgs_b(\n             x0=packed_coef_inter,\n             func=self._loss_grad_lbfgs,\n-            maxfun=self.max_iter,\n+            maxfun=self.max_fun,\n+            maxiter=self.max_iter,\n             iprint=iprint,\n             pgtol=self.tol,\n             args=(X, y, activations, deltas, coef_grads, intercept_grads))\n+        self.n_iter_ = d['nit']\n+        if d['warnflag'] == 1:\n+            if d['nit'] >= self.max_iter:\n+                warnings.warn(\n+                    \"LBFGS Optimizer: Maximum iterations (%d) \"\n+                    \"reached and the optimization hasn't converged yet.\"\n+                    % self.max_iter, ConvergenceWarning)\n+            if d['funcalls'] >= self.max_fun:\n+                warnings.warn(\n+                    \"LBFGS Optimizer: Maximum function evaluations (%d) \"\n+                    \"reached and the optimization hasn't converged yet.\"\n+                    % self.max_fun, ConvergenceWarning)\n+        elif d['warnflag'] == 2:\n+            warnings.warn(\n+                \"LBFGS Optimizer: Optimization hasn't converged yet, \"\n+                \"cause of LBFGS stopping: %s.\"\n+                % d['task'], ConvergenceWarning)\n+\n \n         self._unpack(optimal_parameters)\n \n@@ -833,6 +854,15 @@ class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):\n \n         .. versionadded:: 0.20\n \n+    max_fun : int, optional, default 15000\n+        Only used when solver='lbfgs'. Maximum number of loss function calls.\n+        The solver iterates until convergence (determined by 'tol'), number\n+        of iterations reaches max_iter, or this number of loss function calls.\n+        Note that number of loss function calls will be greater than or equal\n+        to the number of iterations for the `MLPClassifier`.\n+\n+        .. versionadded:: 0.22\n+\n     Attributes\n     ----------\n     classes_ : array or list of array of shape (n_classes,)\n@@ -898,8 +928,7 @@ def __init__(self, hidden_layer_sizes=(100,), activation=\"relu\",\n                  verbose=False, warm_start=False, momentum=0.9,\n                  nesterovs_momentum=True, early_stopping=False,\n                  validation_fraction=0.1, beta_1=0.9, beta_2=0.999,\n-                 epsilon=1e-8, n_iter_no_change=10):\n-\n+                 epsilon=1e-8, n_iter_no_change=10, max_fun=15000):\n         super().__init__(\n             hidden_layer_sizes=hidden_layer_sizes,\n             activation=activation, solver=solver, alpha=alpha,\n@@ -912,7 +941,7 @@ def __init__(self, hidden_layer_sizes=(100,), activation=\"relu\",\n             early_stopping=early_stopping,\n             validation_fraction=validation_fraction,\n             beta_1=beta_1, beta_2=beta_2, epsilon=epsilon,\n-            n_iter_no_change=n_iter_no_change)\n+            n_iter_no_change=n_iter_no_change, max_fun=max_fun)\n \n     def _validate_input(self, X, y, incremental):\n         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n@@ -1216,6 +1245,15 @@ class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):\n \n         .. versionadded:: 0.20\n \n+    max_fun : int, optional, default 15000\n+        Only used when solver='lbfgs'. Maximum number of function calls.\n+        The solver iterates until convergence (determined by 'tol'), number\n+        of iterations reaches max_iter, or this number of function calls.\n+        Note that number of function calls will be greater than or equal to\n+        the number of iterations for the MLPRegressor.\n+\n+        .. versionadded:: 0.22\n+\n     Attributes\n     ----------\n     loss_ : float\n@@ -1279,8 +1317,7 @@ def __init__(self, hidden_layer_sizes=(100,), activation=\"relu\",\n                  verbose=False, warm_start=False, momentum=0.9,\n                  nesterovs_momentum=True, early_stopping=False,\n                  validation_fraction=0.1, beta_1=0.9, beta_2=0.999,\n-                 epsilon=1e-8, n_iter_no_change=10):\n-\n+                 epsilon=1e-8, n_iter_no_change=10, max_fun=15000):\n         super().__init__(\n             hidden_layer_sizes=hidden_layer_sizes,\n             activation=activation, solver=solver, alpha=alpha,\n@@ -1293,7 +1330,7 @@ def __init__(self, hidden_layer_sizes=(100,), activation=\"relu\",\n             early_stopping=early_stopping,\n             validation_fraction=validation_fraction,\n             beta_1=beta_1, beta_2=beta_2, epsilon=epsilon,\n-            n_iter_no_change=n_iter_no_change)\n+            n_iter_no_change=n_iter_no_change, max_fun=max_fun)\n \n     def predict(self, X):\n         \"\"\"Predict using the multi-layer perceptron model.\n",
  "test_patch": "diff --git a/sklearn/neural_network/tests/test_mlp.py b/sklearn/neural_network/tests/test_mlp.py\n--- a/sklearn/neural_network/tests/test_mlp.py\n+++ b/sklearn/neural_network/tests/test_mlp.py\n@@ -48,6 +48,8 @@\n Xboston = StandardScaler().fit_transform(boston.data)[: 200]\n yboston = boston.target[:200]\n \n+regression_datasets = [(Xboston, yboston)]\n+\n iris = load_iris()\n \n X_iris = iris.data\n@@ -228,32 +230,30 @@ def loss_grad_fun(t):\n             assert_almost_equal(numgrad, grad)\n \n \n-def test_lbfgs_classification():\n+@pytest.mark.parametrize('X,y', classification_datasets)\n+def test_lbfgs_classification(X, y):\n     # Test lbfgs on classification.\n     # It should achieve a score higher than 0.95 for the binary and multi-class\n     # versions of the digits dataset.\n-    for X, y in classification_datasets:\n-        X_train = X[:150]\n-        y_train = y[:150]\n-        X_test = X[150:]\n-\n-        expected_shape_dtype = (X_test.shape[0], y_train.dtype.kind)\n-\n-        for activation in ACTIVATION_TYPES:\n-            mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=50,\n-                                max_iter=150, shuffle=True, random_state=1,\n-                                activation=activation)\n-            mlp.fit(X_train, y_train)\n-            y_predict = mlp.predict(X_test)\n-            assert mlp.score(X_train, y_train) > 0.95\n-            assert ((y_predict.shape[0], y_predict.dtype.kind) ==\n-                         expected_shape_dtype)\n+    X_train = X[:150]\n+    y_train = y[:150]\n+    X_test = X[150:]\n+    expected_shape_dtype = (X_test.shape[0], y_train.dtype.kind)\n \n-\n-def test_lbfgs_regression():\n+    for activation in ACTIVATION_TYPES:\n+        mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=50,\n+                            max_iter=150, shuffle=True, random_state=1,\n+                            activation=activation)\n+        mlp.fit(X_train, y_train)\n+        y_predict = mlp.predict(X_test)\n+        assert mlp.score(X_train, y_train) > 0.95\n+        assert ((y_predict.shape[0], y_predict.dtype.kind) ==\n+                expected_shape_dtype)\n+\n+\n+@pytest.mark.parametrize('X,y', regression_datasets)\n+def test_lbfgs_regression(X, y):\n     # Test lbfgs on the boston dataset, a regression problems.\n-    X = Xboston\n-    y = yboston\n     for activation in ACTIVATION_TYPES:\n         mlp = MLPRegressor(solver='lbfgs', hidden_layer_sizes=50,\n                            max_iter=150, shuffle=True, random_state=1,\n@@ -266,6 +266,39 @@ def test_lbfgs_regression():\n             assert mlp.score(X, y) > 0.95\n \n \n+@pytest.mark.parametrize('X,y', classification_datasets)\n+def test_lbfgs_classification_maxfun(X, y):\n+    # Test lbfgs parameter max_fun.\n+    # It should independently limit the number of iterations for lbfgs.\n+    max_fun = 10\n+    # classification tests\n+    for activation in ACTIVATION_TYPES:\n+        mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=50,\n+                            max_iter=150, max_fun=max_fun, shuffle=True,\n+                            random_state=1, activation=activation)\n+        with pytest.warns(ConvergenceWarning):\n+            mlp.fit(X, y)\n+            assert max_fun >= mlp.n_iter_\n+\n+\n+@pytest.mark.parametrize('X,y', regression_datasets)\n+def test_lbfgs_regression_maxfun(X, y):\n+    # Test lbfgs parameter max_fun.\n+    # It should independently limit the number of iterations for lbfgs.\n+    max_fun = 10\n+    # regression tests\n+    for activation in ACTIVATION_TYPES:\n+        mlp = MLPRegressor(solver='lbfgs', hidden_layer_sizes=50,\n+                           max_iter=150, max_fun=max_fun, shuffle=True,\n+                           random_state=1, activation=activation)\n+        with pytest.warns(ConvergenceWarning):\n+            mlp.fit(X, y)\n+            assert max_fun >= mlp.n_iter_\n+\n+    mlp.max_fun = -1\n+    assert_raises(ValueError, mlp.fit, X, y)\n+\n+\n def test_learning_rate_warmstart():\n     # Tests that warm_start reuse past solutions.\n     X = [[3, 2], [1, 6], [5, 6], [-2, -4]]\n",
  "problem_statement": "Training MLP using l-bfgs limited to default l-bfgs maxiter value\n#### Description\r\n\r\nTraining an MLP regressor (or classifier) using l-bfgs currently cannot run for more than (approx) 15000 iterations.\r\nThis artificial limit is caused by the call site to l-bfgs passing the MLP argument value \"max_iters\" to the argument for \"maxfun\" (maximum number of function calls), but not for \"maxiter\" (maximum number of iterations), so that no matter how large a number you pass as \"max_iters\" to train for MLP, the iterations are capped by the default value for maxiter (15000).\r\n\r\n#### Steps/Code to Reproduce\r\nFit an MLP for a problem that requires > 15000 iterations\r\n\r\nHere is an example (tested in python 2.7):\r\nhttps://gist.github.com/daniel-perry/d9e356a03936673e58e0ce47d5fc70ef\r\n\r\n(you will need data.npy from the gist linked to above)\r\n\r\n````\r\nfrom __future__ import print_function\r\nimport numpy as np\r\nfrom sklearn.neural_network import MLPRegressor\r\n\r\ntrain = np.load(\"data.npy\").tolist()\r\n\r\nmax_iter = 18000\r\nclf = MLPRegressor(max_iter=max_iter, activation='relu', solver='lbfgs', verbose=True)\r\n\r\nclf.fit(train[\"train_x\"],train[\"train_y\"])\r\n\r\nprint(\"score: \", clf.score(train[\"train_x\"],train[\"train_y\"]))\r\nprint(\"iters: \", clf.n_iter_, \" / \", max_iter)\r\n````\r\n\r\n#### Expected Results\r\n\r\nThe training should run for 18000 iterations.\r\n\r\n#### Actual Results\r\n\r\nThe training runs for 15000 iterations.\r\n\r\n#### Versions\r\n\r\nHere are my local version details, though the problem appears to exist on the current head, and so should exist for any python/sklearn versions.\r\n\r\n'Python', '2.7.12 (default, Jul  1 2016, 15:12:24) \\n[GCC 5.4.0 20160609]'\r\n'NumPy', '1.13.0'\r\n'SciPy', '0.19.1'\r\n'Scikit-Learn', '0.18'\r\n\r\n\r\n\n[WIP] FIX: use maxiter rather than maxfun in MultiLayerPerceptron with solver='lbfgs'\nIn my limited experience with LBFGS, the number of function calls is greater than the number of iterations.\r\n\r\nThe impact of this bug is that with solver='lbfgs' is probably not doing as many iterations as it should in master although I am not sure it matters that much in practice.\r\n\r\nTo get an idea how much funtion calls differ from iterations, I tweaked `examples/neural_networks/plot_mnist_filters.py` to be able to run for a few hundred iterations:\r\n\r\n```py\r\nmlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, alpha=1e-4,\r\n                    solver='lbfgs', verbose=10, tol=1e-16, random_state=1,\r\n                    learning_rate_init=.1)\r\n```\r\n\r\nThe result: 393 iterations and 414 function calls.\r\n\r\nNot sure whether we nest to test this, and how to test it, suggestions more than welcome!\r\n\r\n- [ ] add a whats_new entry once there is agreement\n",
  "hints_text": "\n",
  "created_at": "2017-07-03T22:39:22Z",
  "version": "0.22",
  "FAIL_TO_PASS": "[\"sklearn/neural_network/tests/test_mlp.py::test_lbfgs_classification_maxfun[X0-y0]\", \"sklearn/neural_network/tests/test_mlp.py::test_lbfgs_classification_maxfun[X1-y1]\", \"sklearn/neural_network/tests/test_mlp.py::test_lbfgs_regression_maxfun[X0-y0]\"]",
  "PASS_TO_PASS": "[\"sklearn/neural_network/tests/test_mlp.py::test_alpha\", \"sklearn/neural_network/tests/test_mlp.py::test_fit\", \"sklearn/neural_network/tests/test_mlp.py::test_gradient\", \"sklearn/neural_network/tests/test_mlp.py::test_lbfgs_classification[X0-y0]\", \"sklearn/neural_network/tests/test_mlp.py::test_lbfgs_classification[X1-y1]\", \"sklearn/neural_network/tests/test_mlp.py::test_lbfgs_regression[X0-y0]\", \"sklearn/neural_network/tests/test_mlp.py::test_learning_rate_warmstart\", \"sklearn/neural_network/tests/test_mlp.py::test_multilabel_classification\", \"sklearn/neural_network/tests/test_mlp.py::test_multioutput_regression\", \"sklearn/neural_network/tests/test_mlp.py::test_partial_fit_classes_error\", \"sklearn/neural_network/tests/test_mlp.py::test_partial_fit_classification\", \"sklearn/neural_network/tests/test_mlp.py::test_partial_fit_unseen_classes\", \"sklearn/neural_network/tests/test_mlp.py::test_partial_fit_regression\", \"sklearn/neural_network/tests/test_mlp.py::test_partial_fit_errors\", \"sklearn/neural_network/tests/test_mlp.py::test_params_errors\", \"sklearn/neural_network/tests/test_mlp.py::test_predict_proba_binary\", \"sklearn/neural_network/tests/test_mlp.py::test_predict_proba_multiclass\", \"sklearn/neural_network/tests/test_mlp.py::test_predict_proba_multilabel\", \"sklearn/neural_network/tests/test_mlp.py::test_shuffle\", \"sklearn/neural_network/tests/test_mlp.py::test_sparse_matrices\", \"sklearn/neural_network/tests/test_mlp.py::test_tolerance\", \"sklearn/neural_network/tests/test_mlp.py::test_verbose_sgd\", \"sklearn/neural_network/tests/test_mlp.py::test_early_stopping\", \"sklearn/neural_network/tests/test_mlp.py::test_adaptive_learning_rate\", \"sklearn/neural_network/tests/test_mlp.py::test_warm_start\", \"sklearn/neural_network/tests/test_mlp.py::test_n_iter_no_change\", \"sklearn/neural_network/tests/test_mlp.py::test_n_iter_no_change_inf\", \"sklearn/neural_network/tests/test_mlp.py::test_early_stopping_stratified\"]",
  "environment_setup_commit": "7e85a6d1f038bbb932b36f18d75df6be937ed00d",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:31.028343",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}