{
  "repo": "scikit-learn/scikit-learn",
  "instance_id": "scikit-learn__scikit-learn-10483",
  "base_commit": "98eb09e9206cb714da85c5a0616deff3cc85a1d5",
  "patch": "diff --git a/examples/plot_missing_values.py b/examples/plot_missing_values.py\n--- a/examples/plot_missing_values.py\n+++ b/examples/plot_missing_values.py\n@@ -28,7 +28,7 @@\n from sklearn.datasets import load_boston\n from sklearn.ensemble import RandomForestRegressor\n from sklearn.pipeline import Pipeline\n-from sklearn.preprocessing import Imputer\n+from sklearn.impute import SimpleImputer\n from sklearn.model_selection import cross_val_score\n \n rng = np.random.RandomState(0)\n@@ -64,9 +64,9 @@\n X_missing = X_full.copy()\n X_missing[np.where(missing_samples)[0], missing_features] = 0\n y_missing = y_full.copy()\n-estimator = Pipeline([(\"imputer\", Imputer(missing_values=0,\n-                                          strategy=\"mean\",\n-                                          axis=0)),\n+estimator = Pipeline([(\"imputer\", SimpleImputer(missing_values=0,\n+                                                strategy=\"mean\",\n+                                                axis=0)),\n                       (\"forest\", RandomForestRegressor(random_state=0,\n                                                        n_estimators=100))])\n score = cross_val_score(estimator, X_missing, y_missing).mean()\ndiff --git a/sklearn/__init__.py b/sklearn/__init__.py\n--- a/sklearn/__init__.py\n+++ b/sklearn/__init__.py\n@@ -73,7 +73,7 @@\n                'mixture', 'model_selection', 'multiclass', 'multioutput',\n                'naive_bayes', 'neighbors', 'neural_network', 'pipeline',\n                'preprocessing', 'random_projection', 'semi_supervised',\n-               'svm', 'tree', 'discriminant_analysis',\n+               'svm', 'tree', 'discriminant_analysis', 'impute',\n                # Non-modules:\n                'clone', 'get_config', 'set_config', 'config_context']\n \ndiff --git a/sklearn/impute.py b/sklearn/impute.py\nnew file mode 100644\n--- /dev/null\n+++ b/sklearn/impute.py\n@@ -0,0 +1,376 @@\n+\"\"\"Transformers for missing value imputation\"\"\"\n+# Authors: Nicolas Tresegnie <nicolas.tresegnie@gmail.com>\n+# License: BSD 3 clause\n+\n+import warnings\n+\n+import numpy as np\n+import numpy.ma as ma\n+from scipy import sparse\n+from scipy import stats\n+\n+from .base import BaseEstimator, TransformerMixin\n+from .utils import check_array\n+from .utils.sparsefuncs import _get_median\n+from .utils.validation import check_is_fitted\n+from .utils.validation import FLOAT_DTYPES\n+\n+from .externals import six\n+\n+zip = six.moves.zip\n+map = six.moves.map\n+\n+__all__ = [\n+    'SimpleImputer',\n+]\n+\n+\n+def _get_mask(X, value_to_mask):\n+    \"\"\"Compute the boolean mask X == missing_values.\"\"\"\n+    if value_to_mask == \"NaN\" or np.isnan(value_to_mask):\n+        return np.isnan(X)\n+    else:\n+        return X == value_to_mask\n+\n+\n+def _most_frequent(array, extra_value, n_repeat):\n+    \"\"\"Compute the most frequent value in a 1d array extended with\n+       [extra_value] * n_repeat, where extra_value is assumed to be not part\n+       of the array.\"\"\"\n+    # Compute the most frequent value in array only\n+    if array.size > 0:\n+        mode = stats.mode(array)\n+        most_frequent_value = mode[0][0]\n+        most_frequent_count = mode[1][0]\n+    else:\n+        most_frequent_value = 0\n+        most_frequent_count = 0\n+\n+    # Compare to array + [extra_value] * n_repeat\n+    if most_frequent_count == 0 and n_repeat == 0:\n+        return np.nan\n+    elif most_frequent_count < n_repeat:\n+        return extra_value\n+    elif most_frequent_count > n_repeat:\n+        return most_frequent_value\n+    elif most_frequent_count == n_repeat:\n+        # Ties the breaks. Copy the behaviour of scipy.stats.mode\n+        if most_frequent_value < extra_value:\n+            return most_frequent_value\n+        else:\n+            return extra_value\n+\n+\n+class SimpleImputer(BaseEstimator, TransformerMixin):\n+    \"\"\"Imputation transformer for completing missing values.\n+\n+    Read more in the :ref:`User Guide <impute>`.\n+\n+    Parameters\n+    ----------\n+    missing_values : integer or \"NaN\", optional (default=\"NaN\")\n+        The placeholder for the missing values. All occurrences of\n+        `missing_values` will be imputed. For missing values encoded as np.nan,\n+        use the string value \"NaN\".\n+\n+    strategy : string, optional (default=\"mean\")\n+        The imputation strategy.\n+\n+        - If \"mean\", then replace missing values using the mean along\n+          the axis.\n+        - If \"median\", then replace missing values using the median along\n+          the axis.\n+        - If \"most_frequent\", then replace missing using the most frequent\n+          value along the axis.\n+\n+    axis : integer, optional (default=0)\n+        The axis along which to impute.\n+\n+        - If `axis=0`, then impute along columns.\n+        - If `axis=1`, then impute along rows.\n+\n+    verbose : integer, optional (default=0)\n+        Controls the verbosity of the imputer.\n+\n+    copy : boolean, optional (default=True)\n+        If True, a copy of X will be created. If False, imputation will\n+        be done in-place whenever possible. Note that, in the following cases,\n+        a new copy will always be made, even if `copy=False`:\n+\n+        - If X is not an array of floating values;\n+        - If X is sparse and `missing_values=0`;\n+        - If `axis=0` and X is encoded as a CSR matrix;\n+        - If `axis=1` and X is encoded as a CSC matrix.\n+\n+    Attributes\n+    ----------\n+    statistics_ : array of shape (n_features,)\n+        The imputation fill value for each feature if axis == 0.\n+\n+    Notes\n+    -----\n+    - When ``axis=0``, columns which only contained missing values at `fit`\n+      are discarded upon `transform`.\n+    - When ``axis=1``, an exception is raised if there are rows for which it is\n+      not possible to fill in the missing values (e.g., because they only\n+      contain missing values).\n+    \"\"\"\n+    def __init__(self, missing_values=\"NaN\", strategy=\"mean\",\n+                 axis=0, verbose=0, copy=True):\n+        self.missing_values = missing_values\n+        self.strategy = strategy\n+        self.axis = axis\n+        self.verbose = verbose\n+        self.copy = copy\n+\n+    def fit(self, X, y=None):\n+        \"\"\"Fit the imputer on X.\n+\n+        Parameters\n+        ----------\n+        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n+            Input data, where ``n_samples`` is the number of samples and\n+            ``n_features`` is the number of features.\n+\n+        Returns\n+        -------\n+        self : SimpleImputer\n+        \"\"\"\n+        # Check parameters\n+        allowed_strategies = [\"mean\", \"median\", \"most_frequent\"]\n+        if self.strategy not in allowed_strategies:\n+            raise ValueError(\"Can only use these strategies: {0} \"\n+                             \" got strategy={1}\".format(allowed_strategies,\n+                                                        self.strategy))\n+\n+        if self.axis not in [0, 1]:\n+            raise ValueError(\"Can only impute missing values on axis 0 and 1, \"\n+                             \" got axis={0}\".format(self.axis))\n+\n+        # Since two different arrays can be provided in fit(X) and\n+        # transform(X), the imputation data will be computed in transform()\n+        # when the imputation is done per sample (i.e., when axis=1).\n+        if self.axis == 0:\n+            X = check_array(X, accept_sparse='csc', dtype=np.float64,\n+                            force_all_finite=False)\n+\n+            if sparse.issparse(X):\n+                self.statistics_ = self._sparse_fit(X,\n+                                                    self.strategy,\n+                                                    self.missing_values,\n+                                                    self.axis)\n+            else:\n+                self.statistics_ = self._dense_fit(X,\n+                                                   self.strategy,\n+                                                   self.missing_values,\n+                                                   self.axis)\n+\n+        return self\n+\n+    def _sparse_fit(self, X, strategy, missing_values, axis):\n+        \"\"\"Fit the transformer on sparse data.\"\"\"\n+        # Imputation is done \"by column\", so if we want to do it\n+        # by row we only need to convert the matrix to csr format.\n+        if axis == 1:\n+            X = X.tocsr()\n+        else:\n+            X = X.tocsc()\n+\n+        # Count the zeros\n+        if missing_values == 0:\n+            n_zeros_axis = np.zeros(X.shape[not axis], dtype=int)\n+        else:\n+            n_zeros_axis = X.shape[axis] - np.diff(X.indptr)\n+\n+        # Mean\n+        if strategy == \"mean\":\n+            if missing_values != 0:\n+                n_non_missing = n_zeros_axis\n+\n+                # Mask the missing elements\n+                mask_missing_values = _get_mask(X.data, missing_values)\n+                mask_valids = np.logical_not(mask_missing_values)\n+\n+                # Sum only the valid elements\n+                new_data = X.data.copy()\n+                new_data[mask_missing_values] = 0\n+                X = sparse.csc_matrix((new_data, X.indices, X.indptr),\n+                                      copy=False)\n+                sums = X.sum(axis=0)\n+\n+                # Count the elements != 0\n+                mask_non_zeros = sparse.csc_matrix(\n+                    (mask_valids.astype(np.float64),\n+                     X.indices,\n+                     X.indptr), copy=False)\n+                s = mask_non_zeros.sum(axis=0)\n+                n_non_missing = np.add(n_non_missing, s)\n+\n+            else:\n+                sums = X.sum(axis=axis)\n+                n_non_missing = np.diff(X.indptr)\n+\n+            # Ignore the error, columns with a np.nan statistics_\n+            # are not an error at this point. These columns will\n+            # be removed in transform\n+            with np.errstate(all=\"ignore\"):\n+                return np.ravel(sums) / np.ravel(n_non_missing)\n+\n+        # Median + Most frequent\n+        else:\n+            # Remove the missing values, for each column\n+            columns_all = np.hsplit(X.data, X.indptr[1:-1])\n+            mask_missing_values = _get_mask(X.data, missing_values)\n+            mask_valids = np.hsplit(np.logical_not(mask_missing_values),\n+                                    X.indptr[1:-1])\n+\n+            # astype necessary for bug in numpy.hsplit before v1.9\n+            columns = [col[mask.astype(bool, copy=False)]\n+                       for col, mask in zip(columns_all, mask_valids)]\n+\n+            # Median\n+            if strategy == \"median\":\n+                median = np.empty(len(columns))\n+                for i, column in enumerate(columns):\n+                    median[i] = _get_median(column, n_zeros_axis[i])\n+\n+                return median\n+\n+            # Most frequent\n+            elif strategy == \"most_frequent\":\n+                most_frequent = np.empty(len(columns))\n+\n+                for i, column in enumerate(columns):\n+                    most_frequent[i] = _most_frequent(column,\n+                                                      0,\n+                                                      n_zeros_axis[i])\n+\n+                return most_frequent\n+\n+    def _dense_fit(self, X, strategy, missing_values, axis):\n+        \"\"\"Fit the transformer on dense data.\"\"\"\n+        X = check_array(X, force_all_finite=False)\n+        mask = _get_mask(X, missing_values)\n+        masked_X = ma.masked_array(X, mask=mask)\n+\n+        # Mean\n+        if strategy == \"mean\":\n+            mean_masked = np.ma.mean(masked_X, axis=axis)\n+            # Avoid the warning \"Warning: converting a masked element to nan.\"\n+            mean = np.ma.getdata(mean_masked)\n+            mean[np.ma.getmask(mean_masked)] = np.nan\n+\n+            return mean\n+\n+        # Median\n+        elif strategy == \"median\":\n+            if tuple(int(v) for v in np.__version__.split('.')[:2]) < (1, 5):\n+                # In old versions of numpy, calling a median on an array\n+                # containing nans returns nan. This is different is\n+                # recent versions of numpy, which we want to mimic\n+                masked_X.mask = np.logical_or(masked_X.mask,\n+                                              np.isnan(X))\n+            median_masked = np.ma.median(masked_X, axis=axis)\n+            # Avoid the warning \"Warning: converting a masked element to nan.\"\n+            median = np.ma.getdata(median_masked)\n+            median[np.ma.getmaskarray(median_masked)] = np.nan\n+\n+            return median\n+\n+        # Most frequent\n+        elif strategy == \"most_frequent\":\n+            # scipy.stats.mstats.mode cannot be used because it will no work\n+            # properly if the first element is masked and if its frequency\n+            # is equal to the frequency of the most frequent valid element\n+            # See https://github.com/scipy/scipy/issues/2636\n+\n+            # To be able access the elements by columns\n+            if axis == 0:\n+                X = X.transpose()\n+                mask = mask.transpose()\n+\n+            most_frequent = np.empty(X.shape[0])\n+\n+            for i, (row, row_mask) in enumerate(zip(X[:], mask[:])):\n+                row_mask = np.logical_not(row_mask).astype(np.bool)\n+                row = row[row_mask]\n+                most_frequent[i] = _most_frequent(row, np.nan, 0)\n+\n+            return most_frequent\n+\n+    def transform(self, X):\n+        \"\"\"Impute all missing values in X.\n+\n+        Parameters\n+        ----------\n+        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n+            The input data to complete.\n+        \"\"\"\n+        if self.axis == 0:\n+            check_is_fitted(self, 'statistics_')\n+            X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES,\n+                            force_all_finite=False, copy=self.copy)\n+            statistics = self.statistics_\n+            if X.shape[1] != statistics.shape[0]:\n+                raise ValueError(\"X has %d features per sample, expected %d\"\n+                                 % (X.shape[1], self.statistics_.shape[0]))\n+\n+        # Since two different arrays can be provided in fit(X) and\n+        # transform(X), the imputation data need to be recomputed\n+        # when the imputation is done per sample\n+        else:\n+            X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES,\n+                            force_all_finite=False, copy=self.copy)\n+\n+            if sparse.issparse(X):\n+                statistics = self._sparse_fit(X,\n+                                              self.strategy,\n+                                              self.missing_values,\n+                                              self.axis)\n+\n+            else:\n+                statistics = self._dense_fit(X,\n+                                             self.strategy,\n+                                             self.missing_values,\n+                                             self.axis)\n+\n+        # Delete the invalid rows/columns\n+        invalid_mask = np.isnan(statistics)\n+        valid_mask = np.logical_not(invalid_mask)\n+        valid_statistics = statistics[valid_mask]\n+        valid_statistics_indexes = np.where(valid_mask)[0]\n+        missing = np.arange(X.shape[not self.axis])[invalid_mask]\n+\n+        if self.axis == 0 and invalid_mask.any():\n+            if self.verbose:\n+                warnings.warn(\"Deleting features without \"\n+                              \"observed values: %s\" % missing)\n+            X = X[:, valid_statistics_indexes]\n+        elif self.axis == 1 and invalid_mask.any():\n+            raise ValueError(\"Some rows only contain \"\n+                             \"missing values: %s\" % missing)\n+\n+        # Do actual imputation\n+        if sparse.issparse(X) and self.missing_values != 0:\n+            mask = _get_mask(X.data, self.missing_values)\n+            indexes = np.repeat(np.arange(len(X.indptr) - 1, dtype=np.int),\n+                                np.diff(X.indptr))[mask]\n+\n+            X.data[mask] = valid_statistics[indexes].astype(X.dtype,\n+                                                            copy=False)\n+        else:\n+            if sparse.issparse(X):\n+                X = X.toarray()\n+\n+            mask = _get_mask(X, self.missing_values)\n+            n_missing = np.sum(mask, axis=self.axis)\n+            values = np.repeat(valid_statistics, n_missing)\n+\n+            if self.axis == 0:\n+                coordinates = np.where(mask.transpose())[::-1]\n+            else:\n+                coordinates = mask\n+\n+            X[coordinates] = values\n+\n+        return X\ndiff --git a/sklearn/preprocessing/imputation.py b/sklearn/preprocessing/imputation.py\n--- a/sklearn/preprocessing/imputation.py\n+++ b/sklearn/preprocessing/imputation.py\n@@ -10,6 +10,7 @@\n \n from ..base import BaseEstimator, TransformerMixin\n from ..utils import check_array\n+from ..utils import deprecated\n from ..utils.sparsefuncs import _get_median\n from ..utils.validation import check_is_fitted\n from ..utils.validation import FLOAT_DTYPES\n@@ -60,6 +61,9 @@ def _most_frequent(array, extra_value, n_repeat):\n             return extra_value\n \n \n+@deprecated(\"Imputer was deprecated in version 0.20 and will be \"\n+            \"removed in 0.22. Import impute.SimpleImputer from \"\n+            \"sklearn instead.\")\n class Imputer(BaseEstimator, TransformerMixin):\n     \"\"\"Imputation transformer for completing missing values.\n \ndiff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py\n--- a/sklearn/utils/estimator_checks.py\n+++ b/sklearn/utils/estimator_checks.py\n@@ -91,7 +91,7 @@ def _yield_non_meta_checks(name, estimator):\n         # cross-decomposition's \"transform\" returns X and Y\n         yield check_pipeline_consistency\n \n-    if name not in ['Imputer']:\n+    if name not in ['SimpleImputer', 'Imputer']:\n         # Test that all estimators check their input for NaN's and infs\n         yield check_estimators_nan_inf\n \n",
  "test_patch": "diff --git a/sklearn/model_selection/tests/test_search.py b/sklearn/model_selection/tests/test_search.py\n--- a/sklearn/model_selection/tests/test_search.py\n+++ b/sklearn/model_selection/tests/test_search.py\n@@ -63,7 +63,7 @@\n from sklearn.metrics import accuracy_score\n from sklearn.metrics import make_scorer\n from sklearn.metrics import roc_auc_score\n-from sklearn.preprocessing import Imputer\n+from sklearn.impute import SimpleImputer\n from sklearn.pipeline import Pipeline\n from sklearn.linear_model import Ridge, SGDClassifier\n \n@@ -1288,12 +1288,12 @@ def test_predict_proba_disabled():\n \n \n def test_grid_search_allows_nans():\n-    # Test GridSearchCV with Imputer\n+    # Test GridSearchCV with SimpleImputer\n     X = np.arange(20, dtype=np.float64).reshape(5, -1)\n     X[2, :] = np.nan\n     y = [0, 0, 1, 1, 1]\n     p = Pipeline([\n-        ('imputer', Imputer(strategy='mean', missing_values='NaN')),\n+        ('imputer', SimpleImputer(strategy='mean', missing_values='NaN')),\n         ('classifier', MockClassifier()),\n     ])\n     GridSearchCV(p, {'classifier__foo_param': [1, 2, 3]}, cv=2).fit(X, y)\ndiff --git a/sklearn/model_selection/tests/test_validation.py b/sklearn/model_selection/tests/test_validation.py\n--- a/sklearn/model_selection/tests/test_validation.py\n+++ b/sklearn/model_selection/tests/test_validation.py\n@@ -64,7 +64,8 @@\n from sklearn.svm import SVC\n from sklearn.cluster import KMeans\n \n-from sklearn.preprocessing import Imputer\n+from sklearn.impute import SimpleImputer\n+\n from sklearn.preprocessing import LabelEncoder\n from sklearn.pipeline import Pipeline\n \n@@ -731,7 +732,7 @@ def test_permutation_test_score_allow_nans():\n     X[2, :] = np.nan\n     y = np.repeat([0, 1], X.shape[0] / 2)\n     p = Pipeline([\n-        ('imputer', Imputer(strategy='mean', missing_values='NaN')),\n+        ('imputer', SimpleImputer(strategy='mean', missing_values='NaN')),\n         ('classifier', MockClassifier()),\n     ])\n     permutation_test_score(p, X, y, cv=5)\n@@ -743,7 +744,7 @@ def test_cross_val_score_allow_nans():\n     X[2, :] = np.nan\n     y = np.repeat([0, 1], X.shape[0] / 2)\n     p = Pipeline([\n-        ('imputer', Imputer(strategy='mean', missing_values='NaN')),\n+        ('imputer', SimpleImputer(strategy='mean', missing_values='NaN')),\n         ('classifier', MockClassifier()),\n     ])\n     cross_val_score(p, X, y, cv=5)\ndiff --git a/sklearn/preprocessing/tests/test_imputation.py b/sklearn/preprocessing/tests/test_imputation.py\n--- a/sklearn/preprocessing/tests/test_imputation.py\n+++ b/sklearn/preprocessing/tests/test_imputation.py\n@@ -7,6 +7,7 @@\n from sklearn.utils.testing import assert_array_almost_equal\n from sklearn.utils.testing import assert_raises\n from sklearn.utils.testing import assert_false\n+from sklearn.utils.testing import ignore_warnings\n \n from sklearn.preprocessing.imputation import Imputer\n from sklearn.pipeline import Pipeline\n@@ -15,6 +16,7 @@\n from sklearn.random_projection import sparse_random_matrix\n \n \n+@ignore_warnings\n def _check_statistics(X, X_true,\n                       strategy, statistics, missing_values):\n     \"\"\"Utility function for testing imputation for a given strategy.\n@@ -79,6 +81,7 @@ def _check_statistics(X, X_true,\n                   err_msg=err_msg.format(1, True))\n \n \n+@ignore_warnings\n def test_imputation_shape():\n     # Verify the shapes of the imputed matrix for different strategies.\n     X = np.random.randn(10, 2)\n@@ -92,6 +95,7 @@ def test_imputation_shape():\n         assert_equal(X_imputed.shape, (10, 2))\n \n \n+@ignore_warnings\n def test_imputation_mean_median_only_zero():\n     # Test imputation using the mean and median strategies, when\n     # missing_values == 0.\n@@ -138,6 +142,7 @@ def safe_mean(arr, *args, **kwargs):\n     return np.nan if length == 0 else np.mean(arr, *args, **kwargs)\n \n \n+@ignore_warnings\n def test_imputation_mean_median():\n     # Test imputation using the mean and median strategies, when\n     # missing_values != 0.\n@@ -208,6 +213,7 @@ def test_imputation_mean_median():\n                           true_statistics, test_missing_values)\n \n \n+@ignore_warnings\n def test_imputation_median_special_cases():\n     # Test median imputation with sparse boundary cases\n     X = np.array([\n@@ -237,6 +243,7 @@ def test_imputation_median_special_cases():\n                       statistics_median, 'NaN')\n \n \n+@ignore_warnings\n def test_imputation_most_frequent():\n     # Test imputation using the most-frequent strategy.\n     X = np.array([\n@@ -260,6 +267,7 @@ def test_imputation_most_frequent():\n     _check_statistics(X, X_true, \"most_frequent\", [np.nan, 2, 3, 3], -1)\n \n \n+@ignore_warnings\n def test_imputation_pipeline_grid_search():\n     # Test imputation within a pipeline + gridsearch.\n     pipeline = Pipeline([('imputer', Imputer(missing_values=0)),\n@@ -277,6 +285,7 @@ def test_imputation_pipeline_grid_search():\n     gs.fit(X, Y)\n \n \n+@ignore_warnings\n def test_imputation_pickle():\n     # Test for pickling imputers.\n     import pickle\n@@ -298,6 +307,7 @@ def test_imputation_pickle():\n         )\n \n \n+@ignore_warnings\n def test_imputation_copy():\n     # Test imputation with copy\n     X_orig = sparse_random_matrix(5, 5, density=0.75, random_state=0)\ndiff --git a/sklearn/tests/test_calibration.py b/sklearn/tests/test_calibration.py\n--- a/sklearn/tests/test_calibration.py\n+++ b/sklearn/tests/test_calibration.py\n@@ -17,7 +17,7 @@\n from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n from sklearn.svm import LinearSVC\n from sklearn.pipeline import Pipeline\n-from sklearn.preprocessing import Imputer\n+from sklearn.impute import SimpleImputer\n from sklearn.metrics import brier_score_loss, log_loss\n from sklearn.calibration import CalibratedClassifierCV\n from sklearn.calibration import _sigmoid_calibration, _SigmoidCalibration\n@@ -266,7 +266,7 @@ def test_calibration_nan_imputer():\n                                random_state=42)\n     X[0, 0] = np.nan\n     clf = Pipeline(\n-        [('imputer', Imputer()),\n+        [('imputer', SimpleImputer()),\n          ('rf', RandomForestClassifier(n_estimators=1))])\n     clf_c = CalibratedClassifierCV(clf, cv=2, method='isotonic')\n     clf_c.fit(X, y)\ndiff --git a/sklearn/tests/test_impute.py b/sklearn/tests/test_impute.py\nnew file mode 100644\n--- /dev/null\n+++ b/sklearn/tests/test_impute.py\n@@ -0,0 +1,365 @@\n+\n+import numpy as np\n+from scipy import sparse\n+\n+from sklearn.utils.testing import assert_equal\n+from sklearn.utils.testing import assert_array_equal\n+from sklearn.utils.testing import assert_array_almost_equal\n+from sklearn.utils.testing import assert_raises\n+from sklearn.utils.testing import assert_false\n+\n+from sklearn.impute import SimpleImputer\n+from sklearn.pipeline import Pipeline\n+from sklearn.model_selection import GridSearchCV\n+from sklearn import tree\n+from sklearn.random_projection import sparse_random_matrix\n+\n+\n+def _check_statistics(X, X_true,\n+                      strategy, statistics, missing_values):\n+    \"\"\"Utility function for testing imputation for a given strategy.\n+\n+    Test:\n+        - along the two axes\n+        - with dense and sparse arrays\n+\n+    Check that:\n+        - the statistics (mean, median, mode) are correct\n+        - the missing values are imputed correctly\"\"\"\n+\n+    err_msg = \"Parameters: strategy = %s, missing_values = %s, \" \\\n+              \"axis = {0}, sparse = {1}\" % (strategy, missing_values)\n+\n+    assert_ae = assert_array_equal\n+    if X.dtype.kind == 'f' or X_true.dtype.kind == 'f':\n+        assert_ae = assert_array_almost_equal\n+\n+    # Normal matrix, axis = 0\n+    imputer = SimpleImputer(missing_values, strategy=strategy, axis=0)\n+    X_trans = imputer.fit(X).transform(X.copy())\n+    assert_ae(imputer.statistics_, statistics,\n+              err_msg=err_msg.format(0, False))\n+    assert_ae(X_trans, X_true, err_msg=err_msg.format(0, False))\n+\n+    # Normal matrix, axis = 1\n+    imputer = SimpleImputer(missing_values, strategy=strategy, axis=1)\n+    imputer.fit(X.transpose())\n+    if np.isnan(statistics).any():\n+        assert_raises(ValueError, imputer.transform, X.copy().transpose())\n+    else:\n+        X_trans = imputer.transform(X.copy().transpose())\n+        assert_ae(X_trans, X_true.transpose(),\n+                  err_msg=err_msg.format(1, False))\n+\n+    # Sparse matrix, axis = 0\n+    imputer = SimpleImputer(missing_values, strategy=strategy, axis=0)\n+    imputer.fit(sparse.csc_matrix(X))\n+    X_trans = imputer.transform(sparse.csc_matrix(X.copy()))\n+\n+    if sparse.issparse(X_trans):\n+        X_trans = X_trans.toarray()\n+\n+    assert_ae(imputer.statistics_, statistics,\n+              err_msg=err_msg.format(0, True))\n+    assert_ae(X_trans, X_true, err_msg=err_msg.format(0, True))\n+\n+    # Sparse matrix, axis = 1\n+    imputer = SimpleImputer(missing_values, strategy=strategy, axis=1)\n+    imputer.fit(sparse.csc_matrix(X.transpose()))\n+    if np.isnan(statistics).any():\n+        assert_raises(ValueError, imputer.transform,\n+                      sparse.csc_matrix(X.copy().transpose()))\n+    else:\n+        X_trans = imputer.transform(sparse.csc_matrix(X.copy().transpose()))\n+\n+        if sparse.issparse(X_trans):\n+            X_trans = X_trans.toarray()\n+\n+        assert_ae(X_trans, X_true.transpose(),\n+                  err_msg=err_msg.format(1, True))\n+\n+\n+def test_imputation_shape():\n+    # Verify the shapes of the imputed matrix for different strategies.\n+    X = np.random.randn(10, 2)\n+    X[::2] = np.nan\n+\n+    for strategy in ['mean', 'median', 'most_frequent']:\n+        imputer = SimpleImputer(strategy=strategy)\n+        X_imputed = imputer.fit_transform(X)\n+        assert_equal(X_imputed.shape, (10, 2))\n+        X_imputed = imputer.fit_transform(sparse.csr_matrix(X))\n+        assert_equal(X_imputed.shape, (10, 2))\n+\n+\n+def test_imputation_mean_median_only_zero():\n+    # Test imputation using the mean and median strategies, when\n+    # missing_values == 0.\n+    X = np.array([\n+        [np.nan, 0, 0, 0, 5],\n+        [np.nan, 1, 0, np.nan, 3],\n+        [np.nan, 2, 0, 0, 0],\n+        [np.nan, 6, 0, 5, 13],\n+    ])\n+\n+    X_imputed_mean = np.array([\n+        [3, 5],\n+        [1, 3],\n+        [2, 7],\n+        [6, 13],\n+    ])\n+    statistics_mean = [np.nan, 3, np.nan, np.nan, 7]\n+\n+    # Behaviour of median with NaN is undefined, e.g. different results in\n+    # np.median and np.ma.median\n+    X_for_median = X[:, [0, 1, 2, 4]]\n+    X_imputed_median = np.array([\n+        [2, 5],\n+        [1, 3],\n+        [2, 5],\n+        [6, 13],\n+    ])\n+    statistics_median = [np.nan, 2, np.nan, 5]\n+\n+    _check_statistics(X, X_imputed_mean, \"mean\", statistics_mean, 0)\n+    _check_statistics(X_for_median, X_imputed_median, \"median\",\n+                      statistics_median, 0)\n+\n+\n+def safe_median(arr, *args, **kwargs):\n+    # np.median([]) raises a TypeError for numpy >= 1.10.1\n+    length = arr.size if hasattr(arr, 'size') else len(arr)\n+    return np.nan if length == 0 else np.median(arr, *args, **kwargs)\n+\n+\n+def safe_mean(arr, *args, **kwargs):\n+    # np.mean([]) raises a RuntimeWarning for numpy >= 1.10.1\n+    length = arr.size if hasattr(arr, 'size') else len(arr)\n+    return np.nan if length == 0 else np.mean(arr, *args, **kwargs)\n+\n+\n+def test_imputation_mean_median():\n+    # Test imputation using the mean and median strategies, when\n+    # missing_values != 0.\n+    rng = np.random.RandomState(0)\n+\n+    dim = 10\n+    dec = 10\n+    shape = (dim * dim, dim + dec)\n+\n+    zeros = np.zeros(shape[0])\n+    values = np.arange(1, shape[0] + 1)\n+    values[4::2] = - values[4::2]\n+\n+    tests = [(\"mean\", \"NaN\", lambda z, v, p: safe_mean(np.hstack((z, v)))),\n+             (\"mean\", 0, lambda z, v, p: np.mean(v)),\n+             (\"median\", \"NaN\", lambda z, v, p: safe_median(np.hstack((z, v)))),\n+             (\"median\", 0, lambda z, v, p: np.median(v))]\n+\n+    for strategy, test_missing_values, true_value_fun in tests:\n+        X = np.empty(shape)\n+        X_true = np.empty(shape)\n+        true_statistics = np.empty(shape[1])\n+\n+        # Create a matrix X with columns\n+        #    - with only zeros,\n+        #    - with only missing values\n+        #    - with zeros, missing values and values\n+        # And a matrix X_true containing all true values\n+        for j in range(shape[1]):\n+            nb_zeros = (j - dec + 1 > 0) * (j - dec + 1) * (j - dec + 1)\n+            nb_missing_values = max(shape[0] + dec * dec\n+                                    - (j + dec) * (j + dec), 0)\n+            nb_values = shape[0] - nb_zeros - nb_missing_values\n+\n+            z = zeros[:nb_zeros]\n+            p = np.repeat(test_missing_values, nb_missing_values)\n+            v = values[rng.permutation(len(values))[:nb_values]]\n+\n+            true_statistics[j] = true_value_fun(z, v, p)\n+\n+            # Create the columns\n+            X[:, j] = np.hstack((v, z, p))\n+\n+            if 0 == test_missing_values:\n+                X_true[:, j] = np.hstack((v,\n+                                          np.repeat(\n+                                              true_statistics[j],\n+                                              nb_missing_values + nb_zeros)))\n+            else:\n+                X_true[:, j] = np.hstack((v,\n+                                          z,\n+                                          np.repeat(true_statistics[j],\n+                                                    nb_missing_values)))\n+\n+            # Shuffle them the same way\n+            np.random.RandomState(j).shuffle(X[:, j])\n+            np.random.RandomState(j).shuffle(X_true[:, j])\n+\n+        # Mean doesn't support columns containing NaNs, median does\n+        if strategy == \"median\":\n+            cols_to_keep = ~np.isnan(X_true).any(axis=0)\n+        else:\n+            cols_to_keep = ~np.isnan(X_true).all(axis=0)\n+\n+        X_true = X_true[:, cols_to_keep]\n+\n+        _check_statistics(X, X_true, strategy,\n+                          true_statistics, test_missing_values)\n+\n+\n+def test_imputation_median_special_cases():\n+    # Test median imputation with sparse boundary cases\n+    X = np.array([\n+        [0, np.nan, np.nan],  # odd: implicit zero\n+        [5, np.nan, np.nan],  # odd: explicit nonzero\n+        [0, 0, np.nan],    # even: average two zeros\n+        [-5, 0, np.nan],   # even: avg zero and neg\n+        [0, 5, np.nan],    # even: avg zero and pos\n+        [4, 5, np.nan],    # even: avg nonzeros\n+        [-4, -5, np.nan],  # even: avg negatives\n+        [-1, 2, np.nan],   # even: crossing neg and pos\n+    ]).transpose()\n+\n+    X_imputed_median = np.array([\n+        [0, 0, 0],\n+        [5, 5, 5],\n+        [0, 0, 0],\n+        [-5, 0, -2.5],\n+        [0, 5, 2.5],\n+        [4, 5, 4.5],\n+        [-4, -5, -4.5],\n+        [-1, 2, .5],\n+    ]).transpose()\n+    statistics_median = [0, 5, 0, -2.5, 2.5, 4.5, -4.5, .5]\n+\n+    _check_statistics(X, X_imputed_median, \"median\",\n+                      statistics_median, 'NaN')\n+\n+\n+def test_imputation_most_frequent():\n+    # Test imputation using the most-frequent strategy.\n+    X = np.array([\n+        [-1, -1, 0, 5],\n+        [-1, 2, -1, 3],\n+        [-1, 1, 3, -1],\n+        [-1, 2, 3, 7],\n+    ])\n+\n+    X_true = np.array([\n+        [2, 0, 5],\n+        [2, 3, 3],\n+        [1, 3, 3],\n+        [2, 3, 7],\n+    ])\n+\n+    # scipy.stats.mode, used in SimpleImputer, doesn't return the first most\n+    # frequent as promised in the doc but the lowest most frequent. When this\n+    # test will fail after an update of scipy, SimpleImputer will need to be\n+    # updated to be consistent with the new (correct) behaviour\n+    _check_statistics(X, X_true, \"most_frequent\", [np.nan, 2, 3, 3], -1)\n+\n+\n+def test_imputation_pipeline_grid_search():\n+    # Test imputation within a pipeline + gridsearch.\n+    pipeline = Pipeline([('imputer', SimpleImputer(missing_values=0)),\n+                         ('tree', tree.DecisionTreeRegressor(random_state=0))])\n+\n+    parameters = {\n+        'imputer__strategy': [\"mean\", \"median\", \"most_frequent\"],\n+        'imputer__axis': [0, 1]\n+    }\n+\n+    X = sparse_random_matrix(100, 100, density=0.10)\n+    Y = sparse_random_matrix(100, 1, density=0.10).toarray()\n+    gs = GridSearchCV(pipeline, parameters)\n+    gs.fit(X, Y)\n+\n+\n+def test_imputation_pickle():\n+    # Test for pickling imputers.\n+    import pickle\n+\n+    X = sparse_random_matrix(100, 100, density=0.10)\n+\n+    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n+        imputer = SimpleImputer(missing_values=0, strategy=strategy)\n+        imputer.fit(X)\n+\n+        imputer_pickled = pickle.loads(pickle.dumps(imputer))\n+\n+        assert_array_almost_equal(\n+            imputer.transform(X.copy()),\n+            imputer_pickled.transform(X.copy()),\n+            err_msg=\"Fail to transform the data after pickling \"\n+            \"(strategy = %s)\" % (strategy)\n+        )\n+\n+\n+def test_imputation_copy():\n+    # Test imputation with copy\n+    X_orig = sparse_random_matrix(5, 5, density=0.75, random_state=0)\n+\n+    # copy=True, dense => copy\n+    X = X_orig.copy().toarray()\n+    imputer = SimpleImputer(missing_values=0, strategy=\"mean\", copy=True)\n+    Xt = imputer.fit(X).transform(X)\n+    Xt[0, 0] = -1\n+    assert_false(np.all(X == Xt))\n+\n+    # copy=True, sparse csr => copy\n+    X = X_orig.copy()\n+    imputer = SimpleImputer(missing_values=X.data[0], strategy=\"mean\",\n+                            copy=True)\n+    Xt = imputer.fit(X).transform(X)\n+    Xt.data[0] = -1\n+    assert_false(np.all(X.data == Xt.data))\n+\n+    # copy=False, dense => no copy\n+    X = X_orig.copy().toarray()\n+    imputer = SimpleImputer(missing_values=0, strategy=\"mean\", copy=False)\n+    Xt = imputer.fit(X).transform(X)\n+    Xt[0, 0] = -1\n+    assert_array_almost_equal(X, Xt)\n+\n+    # copy=False, sparse csr, axis=1 => no copy\n+    X = X_orig.copy()\n+    imputer = SimpleImputer(missing_values=X.data[0], strategy=\"mean\",\n+                            copy=False, axis=1)\n+    Xt = imputer.fit(X).transform(X)\n+    Xt.data[0] = -1\n+    assert_array_almost_equal(X.data, Xt.data)\n+\n+    # copy=False, sparse csc, axis=0 => no copy\n+    X = X_orig.copy().tocsc()\n+    imputer = SimpleImputer(missing_values=X.data[0], strategy=\"mean\",\n+                            copy=False, axis=0)\n+    Xt = imputer.fit(X).transform(X)\n+    Xt.data[0] = -1\n+    assert_array_almost_equal(X.data, Xt.data)\n+\n+    # copy=False, sparse csr, axis=0 => copy\n+    X = X_orig.copy()\n+    imputer = SimpleImputer(missing_values=X.data[0], strategy=\"mean\",\n+                            copy=False, axis=0)\n+    Xt = imputer.fit(X).transform(X)\n+    Xt.data[0] = -1\n+    assert_false(np.all(X.data == Xt.data))\n+\n+    # copy=False, sparse csc, axis=1 => copy\n+    X = X_orig.copy().tocsc()\n+    imputer = SimpleImputer(missing_values=X.data[0], strategy=\"mean\",\n+                            copy=False, axis=1)\n+    Xt = imputer.fit(X).transform(X)\n+    Xt.data[0] = -1\n+    assert_false(np.all(X.data == Xt.data))\n+\n+    # copy=False, sparse csr, axis=1, missing_values=0 => copy\n+    X = X_orig.copy()\n+    imputer = SimpleImputer(missing_values=0, strategy=\"mean\",\n+                            copy=False, axis=1)\n+    Xt = imputer.fit(X).transform(X)\n+    assert_false(sparse.issparse(Xt))\n+\n+    # Note: If X is sparse and if missing_values=0, then a (dense) copy of X is\n+    # made, even if copy=False.\n",
  "problem_statement": "Move imputation out of preprocessing\nWhile we're considering additional imputers, I've wondered whether preprocessing is the right place for it. Yes, it is a preprocessing step before other learning, but it often makes use of other supervised and unsupervised learners and hence is a learning task of its own. And preprocessing is getting a bit cramped.\r\n\r\nWe could also do as with other models and have imputers appear in modules on the basis of how they work rather than function: `KNNImputer` could appear in neighbors for instance. `MICE` could appear where..? And the basic `Imputer` in dummy? probably not.\r\n\r\nIn practice I think it is more useful for users to `import sklearn.impute`, akin to our clusterers and decomposition, and unlike our predictors and outlier detectors that are grouped by algorithm.\n",
  "hints_text": "> modules on the basis of how they work rather than function\r\n\r\nI don't like this (even though we've done that in the past - inconsistently. Why is LDA and LinearSVC not in linear models?)\r\n\r\nI'm +.5 for ``sklearn.impute``, possibly moving when when we add the next class (KNNImputer I guess?).\nActually, given that MICE is not that far away, should be +1\nMICE is not far away at all. `sklearn.impute` would be useful, but importing it would conceivably import `neighbors`, `linear_model`, `ensemble` and `tree` if we had implementations of MICE, KNN and forest-based imputation there. We have decidedly scattered anomaly detection around the place. I am uncomfortable about putting MICE and KNNImputer in preprocessing, but I'm not *entirely* certain that sklearn.impute is the right solution.\r\n\r\nIf we make sklearn.impute, do we rename Imputer to `sklearn.impute.BasicImputer` or `FeaturewiseImputer` or some such?\nOf course we could just make KNNImputer live under neighbors and MICE live under ?ensemble.\r\n\r\nI've wondered whether in some ways it would make sense to have a pseudo-module sklearn.classifiers, sklearn.regressors, sklearn.imputers, etc, that import from the relevant implementation locations...\nYeah... I would prefer a semantic organization, but it's not how we have done things in the past. I guess you suggest having that in parallel to the current structure? I wouldn't be opposed, but it's a big change. Is the goal to keep two places to import from long-term? That seems slightly confusing....\nit's not entirely true that we haven't done semantic organisation in the\npast, sklearn.cluster,decomposition,manifold...\n\nOn 9 Dec 2017 5:59 am, \"Andreas Mueller\" <notifications@github.com> wrote:\n\n> Yeah... I would prefer a semantic organization, but it's not how we have\n> done things in the past. I guess you suggest having that in parallel to the\n> current structure? I wouldn't be opposed, but it's a big change. Is the\n> goal to keep two places to import from long-term? That seems slightly\n> confusing....\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/9726#issuecomment-350343951>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz6yLh_frd5DWue1XE_rdsyqIfjCXoks5s-YbzgaJpZM4PSgEV>\n> .\n>\n\nTrue, we have done a really weird mix. The fact that we have ``SGDClassifier`` which implements many losses, and ``LogisticRegression`` which implements many solvers shows that we're not the best with the consistency ;)\nbut for the users it's moot, as long as they can find stuff.\n\nThe MissingnessIndicator (#8075) should also live in this module, which may help users.\nI'd like another opinion, but I think this should happen. Make sklearn.impute and move Imputer and all the open imputation-related PRs to this module.\nI've opened this to contributors. Please\r\n* copy `sklearn/preprocessing/imputation.py` to `sklearn/impute.py`, as well as the corresponding tests,\r\n* deprecate `Imputer` in  `sklearn/preprocessing/imputation.py` to be removed in v0.22\r\n* create a `sklearn.impute` section in `doc/modules/classes.rst`\r\n* update the deprecated section at the bottom of `doc/modules/classes.rst`\r\n* update `sklearn/__init__.py`'s `__all__`\r\n* move imputation documentation from `doc/modules/preprocessing.rst` to `doc/modules/impute.rst`\r\n* we might also want to rename Imputer in the new module to `SimpleImputer`, `DummyImputer` or something (ideas??)\r\n* update any references to Imputer (in sklearn/, doc/ or examples/) to refer to the new location\r\n\r\nand after merge, please advise contributors at #8075, #8478, #9212 to move their work to the new module.\nRe: naming, I like `ConstantImputer` because it fills in all missing values in a feature with a constant, but it might not be clear from the name that is what it's doing. Maybe `BasicImputer` or `NaiveImputer`.\nit's comparable to DummyRegressor, but calling it DummyImputer seems\nunreasonably disparaging :p\n\nI agree because DummyRegressor is not actually useful for doing work, but the Imputer is quite useful.\n@jnothman I can work on this. Thanks for the detailed steps.",
  "created_at": "2018-01-16T20:27:35Z",
  "version": "0.20",
  "FAIL_TO_PASS": "[\"sklearn/model_selection/tests/test_search.py::test_parameter_grid\", \"sklearn/model_selection/tests/test_search.py::test_grid_search\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_with_fit_params\", \"sklearn/model_selection/tests/test_search.py::test_random_search_with_fit_params\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_fit_params_deprecation\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_fit_params_two_places\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_no_score\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_score_method\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_groups\", \"sklearn/model_selection/tests/test_search.py::test_return_train_score_warn\", \"sklearn/model_selection/tests/test_search.py::test_classes__property\", \"sklearn/model_selection/tests/test_search.py::test_trivial_cv_results_attr\", \"sklearn/model_selection/tests/test_search.py::test_no_refit\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_error\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_one_grid_point\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_when_param_grid_includes_range\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_bad_param_grid\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_sparse\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_sparse_scoring\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_precomputed_kernel\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_precomputed_kernel_error_nonsquare\", \"sklearn/model_selection/tests/test_search.py::test_refit\", \"sklearn/model_selection/tests/test_search.py::test_gridsearch_nd\", \"sklearn/model_selection/tests/test_search.py::test_X_as_list\", \"sklearn/model_selection/tests/test_search.py::test_y_as_list\", \"sklearn/model_selection/tests/test_search.py::test_pandas_input\", \"sklearn/model_selection/tests/test_search.py::test_unsupervised_grid_search\", \"sklearn/model_selection/tests/test_search.py::test_gridsearch_no_predict\", \"sklearn/model_selection/tests/test_search.py::test_param_sampler\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_cv_results\", \"sklearn/model_selection/tests/test_search.py::test_random_search_cv_results\", \"sklearn/model_selection/tests/test_search.py::test_search_iid_param\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_cv_results_multimetric\", \"sklearn/model_selection/tests/test_search.py::test_random_search_cv_results_multimetric\", \"sklearn/model_selection/tests/test_search.py::test_search_cv_results_rank_tie_breaking\", \"sklearn/model_selection/tests/test_search.py::test_search_cv_results_none_param\", \"sklearn/model_selection/tests/test_search.py::test_search_cv_timing\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_correct_score_results\", \"sklearn/model_selection/tests/test_search.py::test_fit_grid_point\", \"sklearn/model_selection/tests/test_search.py::test_pickle\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_with_multioutput_data\", \"sklearn/model_selection/tests/test_search.py::test_predict_proba_disabled\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_allows_nans\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_failing_classifier\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_failing_classifier_raise\", \"sklearn/model_selection/tests/test_search.py::test_parameters_sampler_replacement\", \"sklearn/model_selection/tests/test_search.py::test_stochastic_gradient_loss_param\", \"sklearn/model_selection/tests/test_search.py::test_search_train_scores_set_to_false\", \"sklearn/model_selection/tests/test_search.py::test_grid_search_cv_splits_consistency\", \"sklearn/model_selection/tests/test_search.py::test_transform_inverse_transform_round_trip\", \"sklearn/model_selection/tests/test_search.py::test_deprecated_grid_search_iid\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_score\", \"sklearn/model_selection/tests/test_validation.py::test_cross_validate_invalid_scoring_param\", \"sklearn/model_selection/tests/test_validation.py::test_cross_validate_return_train_score_warn\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_score_predict_groups\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_score_pandas\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_score_mask\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_score_precomputed\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_score_fit_params\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_score_score_func\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_score_errors\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_score_with_score_func_classification\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_score_with_score_func_regression\", \"sklearn/model_selection/tests/test_validation.py::test_permutation_score\", \"sklearn/model_selection/tests/test_validation.py::test_permutation_test_score_allow_nans\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_score_allow_nans\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_score_multilabel\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_predict\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_predict_decision_function_shape\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_predict_predict_proba_shape\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_predict_predict_log_proba_shape\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_predict_input_types\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_predict_pandas\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_score_sparse_fit_params\", \"sklearn/model_selection/tests/test_validation.py::test_learning_curve\", \"sklearn/model_selection/tests/test_validation.py::test_learning_curve_unsupervised\", \"sklearn/model_selection/tests/test_validation.py::test_learning_curve_verbose\", \"sklearn/model_selection/tests/test_validation.py::test_learning_curve_incremental_learning_not_possible\", \"sklearn/model_selection/tests/test_validation.py::test_learning_curve_incremental_learning\", \"sklearn/model_selection/tests/test_validation.py::test_learning_curve_incremental_learning_unsupervised\", \"sklearn/model_selection/tests/test_validation.py::test_learning_curve_batch_and_incremental_learning_are_equal\", \"sklearn/model_selection/tests/test_validation.py::test_learning_curve_n_sample_range_out_of_bounds\", \"sklearn/model_selection/tests/test_validation.py::test_learning_curve_remove_duplicate_sample_sizes\", \"sklearn/model_selection/tests/test_validation.py::test_learning_curve_with_boolean_indices\", \"sklearn/model_selection/tests/test_validation.py::test_learning_curve_with_shuffle\", \"sklearn/model_selection/tests/test_validation.py::test_validation_curve\", \"sklearn/model_selection/tests/test_validation.py::test_validation_curve_clone_estimator\", \"sklearn/model_selection/tests/test_validation.py::test_validation_curve_cv_splits_consistency\", \"sklearn/model_selection/tests/test_validation.py::test_check_is_permutation\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_predict_sparse_prediction\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_predict_with_method\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_predict_method_checking\", \"sklearn/model_selection/tests/test_validation.py::test_gridsearchcv_cross_val_predict_with_method\", \"sklearn/model_selection/tests/test_validation.py::test_cross_val_predict_class_subset\", \"sklearn/model_selection/tests/test_validation.py::test_score_memmap\", \"sklearn/model_selection/tests/test_validation.py::test_permutation_test_score_pandas\", \"sklearn/model_selection/tests/test_validation.py::test_fit_and_score\", \"sklearn/preprocessing/tests/test_imputation.py::test_imputation_shape\", \"sklearn/preprocessing/tests/test_imputation.py::test_imputation_mean_median_only_zero\", \"sklearn/preprocessing/tests/test_imputation.py::test_imputation_mean_median\", \"sklearn/preprocessing/tests/test_imputation.py::test_imputation_median_special_cases\", \"sklearn/preprocessing/tests/test_imputation.py::test_imputation_most_frequent\", \"sklearn/preprocessing/tests/test_imputation.py::test_imputation_pipeline_grid_search\", \"sklearn/preprocessing/tests/test_imputation.py::test_imputation_pickle\", \"sklearn/preprocessing/tests/test_imputation.py::test_imputation_copy\", \"sklearn/tests/test_calibration.py::test_calibration\", \"sklearn/tests/test_calibration.py::test_sample_weight\", \"sklearn/tests/test_calibration.py::test_calibration_multiclass\", \"sklearn/tests/test_calibration.py::test_calibration_prefit\", \"sklearn/tests/test_calibration.py::test_sigmoid_calibration\", \"sklearn/tests/test_calibration.py::test_calibration_curve\", \"sklearn/tests/test_calibration.py::test_calibration_nan_imputer\", \"sklearn/tests/test_calibration.py::test_calibration_prob_sum\", \"sklearn/tests/test_calibration.py::test_calibration_less_classes\", \"sklearn/tests/test_impute.py::test_imputation_shape\", \"sklearn/tests/test_impute.py::test_imputation_mean_median_only_zero\", \"sklearn/tests/test_impute.py::test_imputation_mean_median\", \"sklearn/tests/test_impute.py::test_imputation_median_special_cases\", \"sklearn/tests/test_impute.py::test_imputation_most_frequent\", \"sklearn/tests/test_impute.py::test_imputation_pipeline_grid_search\", \"sklearn/tests/test_impute.py::test_imputation_pickle\", \"sklearn/tests/test_impute.py::test_imputation_copy\"]",
  "PASS_TO_PASS": "[]",
  "environment_setup_commit": "55bf5d93e5674f13a1134d93a11fd0cd11aabcd1",
  "_download_metadata": {
    "downloaded_at": "2025-10-07T21:23:30.952103",
    "dataset_name": "swe-bench",
    "split": "test",
    "downloader_version": "0.1.0"
  }
}